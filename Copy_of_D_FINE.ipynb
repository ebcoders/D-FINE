{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ebcoders/D-FINE/blob/master/Copy_of_D_FINE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtNitv7zb01K",
        "outputId": "1a75a9e9-82c5-4693-baeb-ec40d831f205"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'D-FINE'...\n",
            "remote: Enumerating objects: 844, done.\u001b[K\n",
            "remote: Counting objects: 100% (553/553), done.\u001b[K\n",
            "remote: Compressing objects: 100% (290/290), done.\u001b[K\n",
            "remote: Total 844 (delta 321), reused 445 (delta 255), pack-reused 291 (from 1)\u001b[K\n",
            "Receiving objects: 100% (844/844), 380.57 KiB | 1.50 MiB/s, done.\n",
            "Resolving deltas: 100% (447/447), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Peterande/D-FINE/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BR8_CcJecHk7",
        "outputId": "b455d5ba-a7b5-46a9-81af-084461e31a69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/D-FINE\n"
          ]
        }
      ],
      "source": [
        "cd D-FINE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86o5EiwfcKBo",
        "outputId": "dcf448d2-bfdb-4551-8187-1ac065ba9a4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.15.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (0.20.1+cu121)\n",
            "Collecting faster-coco-eval>=1.6.5 (from -r requirements.txt (line 3))\n",
            "  Downloading faster_coco_eval-1.6.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (6.0.2)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (2.17.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (1.13.1)\n",
            "Collecting calflops (from -r requirements.txt (line 7))\n",
            "  Downloading calflops-0.3.2-py3-none-any.whl.metadata (28 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (4.46.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->-r requirements.txt (line 1)) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->-r requirements.txt (line 1)) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->-r requirements.txt (line 1)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->-r requirements.txt (line 1)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->-r requirements.txt (line 1)) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->-r requirements.txt (line 1)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.0.1->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.15.2->-r requirements.txt (line 2)) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.15.2->-r requirements.txt (line 2)) (11.0.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from faster-coco-eval>=1.6.5->-r requirements.txt (line 3)) (5.24.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from faster-coco-eval>=1.6.5->-r requirements.txt (line 3)) (2.2.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 5)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 5)) (1.68.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 5)) (3.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 5)) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 5)) (4.25.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 5)) (75.1.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 5)) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 5)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 5)) (3.1.3)\n",
            "Requirement already satisfied: accelerate>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from calflops->-r requirements.txt (line 7)) (1.1.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from calflops->-r requirements.txt (line 7)) (0.26.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 8)) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 8)) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 8)) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 8)) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 8)) (4.66.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.22.0->calflops->-r requirements.txt (line 7)) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 5)) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->faster-coco-eval>=1.6.5->-r requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->faster-coco-eval>=1.6.5->-r requirements.txt (line 3)) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->faster-coco-eval>=1.6.5->-r requirements.txt (line 3)) (2024.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->faster-coco-eval>=1.6.5->-r requirements.txt (line 3)) (9.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 8)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 8)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 8)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 8)) (2024.8.30)\n",
            "Downloading faster_coco_eval-1.6.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (470 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m470.9/470.9 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading calflops-0.3.2-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: faster-coco-eval, calflops\n",
            "Successfully installed calflops-0.3.2 faster-coco-eval-1.6.5\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXAe4Q5R7SFs",
        "outputId": "e9d15e26-f05b-488a-c97b-a66c021bd734"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "8NllWQk3cNSy",
        "outputId": "14a3c19d-ae82-47dd-c267-47956e93a897"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-fa216fd7-6b2a-4a4e-abc0-e4faf7cd8f1a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-fa216fd7-6b2a-4a4e-abc0-e4faf7cd8f1a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-fde57312c69d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Upload the new kaggle.json file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(target_dir)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \"\"\"\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'complete'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m     result = _output.eval_js(\n\u001b[0m\u001b[1;32m    173\u001b[0m         'google.colab._files._uploadFilesContinue(\"{output_id}\")'.format(\n\u001b[1;32m    174\u001b[0m             \u001b[0moutput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()  # Upload the new kaggle.json file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOMm1nswcnW5"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImrTB79Cc1IC",
        "outputId": "cb85ca51-0ef8-4527-8974-94da95ef2b51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ref                                                            title                                              size  lastUpdated          downloadCount  voteCount  usabilityRating  \n",
            "-------------------------------------------------------------  ------------------------------------------------  -----  -------------------  -------------  ---------  ---------------  \n",
            "uom190346a/mental-health-diagnosis-and-treatment-monitoring    Mental Health Diagnosis and Treatment Monitoring   10KB  2024-11-07 05:49:42            955         28  1.0              \n",
            "valakhorasani/mobile-device-usage-and-user-behavior-dataset    Mobile Device Usage and User Behavior Dataset      11KB  2024-09-28 20:21:12          23014        467  1.0              \n",
            "muhammadroshaanriaz/students-performance-dataset-cleaned       Students Performance | Clean Dataset               10KB  2024-10-29 19:32:26           2973         60  1.0              \n",
            "valakhorasani/gym-members-exercise-dataset                     Gym Members Exercise Dataset                       22KB  2024-10-06 11:27:38          13186        195  1.0              \n",
            "computingvictor/transactions-fraud-datasets                    üí≥ Financial Transactions Dataset: Analytics       348MB  2024-10-31 21:29:56           1506         31  1.0              \n",
            "valakhorasani/bank-transaction-dataset-for-fraud-detection     Bank Transaction Dataset for Fraud Detection      102KB  2024-11-04 09:23:49           1344         34  1.0              \n",
            "taweilo/loan-approval-classification-data                      Loan Approval Classification Dataset              751KB  2024-10-29 04:07:34           2666         47  1.0              \n",
            "bhadramohit/mental-health-dataset                              Mental Health Dataset                              13KB  2024-10-22 12:08:16           1446         24  1.0              \n",
            "arpitsinghaiml/most-dangerous-countries-for-women-2024         Most Dangerous Countries for Women 2024             3KB  2024-11-04 08:08:04            868         28  1.0              \n",
            "yusufdelikkaya/online-sales-dataset                            Online Sales Dataset                                1MB  2024-10-29 10:08:40           2432         41  0.88235295       \n",
            "jacopoferretti/wages-and-education-of-young-males-dataset      Wages and Education of Young Males Dataset         72KB  2024-10-31 21:07:44            897         28  1.0              \n",
            "bhadramohit/social-media-usage-datasetapplications             Social Media Usage Dataset(Applications)            9KB  2024-10-23 05:33:28           1807         30  1.0              \n",
            "yusufdelikkaya/imdb-movie-dataset                              IMDB Movie Dataset                                134KB  2024-10-30 16:52:53           1924         35  0.88235295       \n",
            "yusufdelikkaya/google-play-store-apps-dataset                  Google Play Store Apps Dataset                    312KB  2024-10-30 17:02:58            626         23  0.88235295       \n",
            "whisperingkahuna/premier-league-2324-team-and-player-insights  Premier League 23/24 ‚öΩ: Team & Player Stats üìä     196KB  2024-10-30 08:59:41           1253         27  1.0              \n",
            "brsahan/data-science-job                                       Data Science Job                                   80KB  2024-11-03 10:13:11           1015         38  1.0              \n",
            "refiaozturk/spotify-songs-dataset                              50K Songs Dataset - Generated by AI                 3MB  2024-11-06 11:50:25            678         30  1.0              \n",
            "willianoliveiragibin/phone-information-2024                    Phone Information 2024                             74KB  2024-10-20 19:04:06           1167         23  1.0              \n",
            "fatihyavuzz/smartphone-sales                                   Smartphone_sales                                   27KB  2024-11-07 14:39:31            704         34  0.8235294        \n",
            "bhadramohit/smartphone-usage-and-behavioral-dataset            Smartphone Usage and Behavioral Dataset            17KB  2024-10-23 03:16:02           2563         48  1.0              \n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bxfo91JseK6h",
        "outputId": "b29266ea-721b-4497-cac1-66ea7f6a5208"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.10/dist-packages (0.3.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from kagglehub) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kagglehub) (4.66.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install kagglehub\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O70kRiQheNH0",
        "outputId": "aafa0f57-e6ca-4d72-f7b3-9a37de203b1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/awsaf49/coco-2017-dataset?dataset_version_number=2...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25.0G/25.0G [19:45<00:00, 22.7MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/awsaf49/coco-2017-dataset/versions/2\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download the dataset using KaggleHub\n",
        "path = kagglehub.dataset_download(\"awsaf49/coco-2017-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReAQ4irnhhPh",
        "outputId": "f9fe8b22-939d-42d8-b8df-408830a0c7b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset is located at: /root/.cache/kagglehub/datasets/awsaf49/coco-2017-dataset/versions/2\n",
            "Files in the dataset directory:\n",
            "['coco2017']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Path to check for the dataset files\n",
        "dataset_path = '/root/.cache/kagglehub/datasets/awsaf49/coco-2017-dataset/versions/2'\n",
        "\n",
        "# List files in the dataset directory to verify\n",
        "if os.path.exists(dataset_path):\n",
        "    print(f\"Dataset is located at: {dataset_path}\")\n",
        "    print(\"Files in the dataset directory:\")\n",
        "    print(os.listdir(dataset_path))  # List files in the directory\n",
        "else:\n",
        "    print(f\"The specified path does not exist: {dataset_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNejiOich-qy",
        "outputId": "3f7f576d-bc97-47ab-c402-e97a03981ca4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/root/.cache/kagglehub/datasets/awsaf49/coco-2017-dataset/versions/2\n"
          ]
        }
      ],
      "source": [
        "cd /root/.cache/kagglehub/datasets/awsaf49/coco-2017-dataset/versions/2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouJM76dAiC0K",
        "outputId": "d77d6a9e-3343-4fa5-d6b7-c826d96f9bf4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ".  ..  datasets\n",
            ".  ..  awsaf49\n",
            ".  ..  coco-2017-dataset\n",
            ".  ..  2.complete  versions\n"
          ]
        }
      ],
      "source": [
        "!ls -a /root/.cache/kagglehub\n",
        "!ls -a /root/.cache/kagglehub/datasets\n",
        "!ls -a /root/.cache/kagglehub/datasets/awsaf49\n",
        "!ls -a /root/.cache/kagglehub/datasets/awsaf49/coco-2017-dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "Vb7MSWrRkOiJ",
        "outputId": "81ad1830-d2f5-432a-e95b-80d5d7ca2f6a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/root/.cache/kagglehub/datasets/awsaf49/coco-2017-dataset/versions/2/coco2017/train2017/'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-4e38ee5c3462>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Get all files in the train2017 directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mall_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Select 400 random images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/root/.cache/kagglehub/datasets/awsaf49/coco-2017-dataset/versions/2/coco2017/train2017/'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "# Set paths\n",
        "train_dir = '/root/.cache/kagglehub/datasets/awsaf49/coco-2017-dataset/versions/2/coco2017/train2017/'  # Correct path to train2017 folder\n",
        "output_dir = '/content/coco/train_subset/'  # Path to save the subset\n",
        "\n",
        "# Make output directory if it doesn't exist\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Get all files in the train2017 directory\n",
        "all_images = os.listdir(train_dir)\n",
        "\n",
        "# Select 400 random images\n",
        "selected_images = random.sample(all_images, 400)\n",
        "\n",
        "# Copy selected images to output directory\n",
        "for image in selected_images:\n",
        "    shutil.copy(os.path.join(train_dir, image), os.path.join(output_dir, image))\n",
        "\n",
        "print(f\"Subset of 400 images copied to {output_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "x4YQIEAvlGGi",
        "outputId": "2c67c217-89d8-4a7b-8c6d-ad9130529023"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/root/.cache/kagglehub/datasets/awsaf49/coco-2017-dataset/versions/2/coco2017/annotations/instances_train2017.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-983441a27ded>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load the annotations file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotations_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mannotations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/root/.cache/kagglehub/datasets/awsaf49/coco-2017-dataset/versions/2/coco2017/annotations/instances_train2017.json'"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "# Correct path to the annotations file\n",
        "annotations_file = '/root/.cache/kagglehub/datasets/awsaf49/coco-2017-dataset/versions/2/coco2017/annotations/instances_train2017.json'\n",
        "\n",
        "# Load the annotations file\n",
        "with open(annotations_file) as f:\n",
        "    annotations = json.load(f)\n",
        "\n",
        "# Filter annotations\n",
        "image_ids = [image.split('.')[0] for image in selected_images]  # Extract image IDs (removing file extensions)\n",
        "filtered_annotations = {\n",
        "    'images': [image for image in annotations['images'] if image['id'] in image_ids],\n",
        "    'annotations': [ann for ann in annotations['annotations'] if ann['image_id'] in image_ids],\n",
        "    'categories': annotations['categories']\n",
        "}\n",
        "\n",
        "# Correct output path for the filtered annotations file\n",
        "output_annotations_file = '/content/coco/annotations/instances_train_subset.json'\n",
        "\n",
        "# Save the filtered annotations\n",
        "os.makedirs(os.path.dirname(output_annotations_file), exist_ok=True)  # Create the directory if it doesn't exist\n",
        "with open(output_annotations_file, 'w') as f:\n",
        "    json.dump(filtered_annotations, f)\n",
        "\n",
        "print(\"Filtered annotations saved.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qs080_al8yU",
        "outputId": "47f24ad9-3e86-4e5a-f251-ec0c8201f0a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Subset of 400 validation images copied to /content/coco/val_subset/\n"
          ]
        }
      ],
      "source": [
        "# Path for validation images (val2017)\n",
        "val_dir = '/root/.cache/kagglehub/datasets/awsaf49/coco-2017-dataset/versions/2/coco2017/val2017/'\n",
        "\n",
        "# Make output directory for val_subset if it doesn't exist\n",
        "val_output_dir = '/content/coco/val_subset/'\n",
        "os.makedirs(val_output_dir, exist_ok=True)\n",
        "\n",
        "# Get all files in the val2017 directory\n",
        "all_val_images = os.listdir(val_dir)\n",
        "\n",
        "# Select 400 random images for validation\n",
        "selected_val_images = random.sample(all_val_images, 400)\n",
        "\n",
        "# Copy selected validation images to the new output directory\n",
        "for image in selected_val_images:\n",
        "    shutil.copy(os.path.join(val_dir, image), os.path.join(val_output_dir, image))\n",
        "\n",
        "print(f\"Subset of 400 validation images copied to {val_output_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUGhvKP5l_eA",
        "outputId": "3f18097e-2ede-4556-8bba-840782d009a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtered validation annotations saved.\n"
          ]
        }
      ],
      "source": [
        "# Correct path to the validation annotations file\n",
        "val_annotations_file = '/root/.cache/kagglehub/datasets/awsaf49/coco-2017-dataset/versions/2/coco2017/annotations/instances_val2017.json'\n",
        "\n",
        "# Load the validation annotations file\n",
        "with open(val_annotations_file) as f:\n",
        "    val_annotations = json.load(f)\n",
        "\n",
        "# Filter validation annotations for the selected 400 images\n",
        "val_image_ids = [image.split('.')[0] for image in selected_val_images]  # Extract image IDs (removing file extensions)\n",
        "filtered_val_annotations = {\n",
        "    'images': [image for image in val_annotations['images'] if image['id'] in val_image_ids],\n",
        "    'annotations': [ann for ann in val_annotations['annotations'] if ann['image_id'] in val_image_ids],\n",
        "    'categories': val_annotations['categories']\n",
        "}\n",
        "\n",
        "# Save the filtered validation annotations\n",
        "val_output_annotations_file = '/content/coco/annotations/instances_val_subset.json'\n",
        "os.makedirs(os.path.dirname(val_output_annotations_file), exist_ok=True)  # Create the directory if it doesn't exist\n",
        "with open(val_output_annotations_file, 'w') as f:\n",
        "    json.dump(filtered_val_annotations, f)\n",
        "\n",
        "print(\"Filtered validation annotations saved.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRrISUUTmHnp",
        "outputId": "59fa01df-b41c-4cd1-bb6b-5b8217d8ac5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W1116 06:00:52.598000 5801 torch/distributed/run.py:793] \n",
            "W1116 06:00:52.598000 5801 torch/distributed/run.py:793] *****************************************\n",
            "W1116 06:00:52.598000 5801 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
            "W1116 06:00:52.598000 5801 torch/distributed/run.py:793] *****************************************\n",
            "2024-11-16 06:01:01.072878: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-16 06:01:01.136034: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-16 06:01:01.137010: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-16 06:01:01.144425: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-16 06:01:01.159930: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-16 06:01:01.161260: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-16 06:01:01.196992: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-16 06:01:01.198635: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-16 06:01:01.208043: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-16 06:01:01.218503: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-16 06:01:01.219120: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-16 06:01:01.229510: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-16 06:01:01.245379: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-16 06:01:01.261325: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-16 06:01:01.262004: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-16 06:01:01.282925: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-16 06:01:04.684919: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-11-16 06:01:04.753801: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-11-16 06:01:04.905219: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-11-16 06:01:04.917575: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Not init distributed mode.\n",
            "Not init distributed mode.\n",
            "Initialized distributed mode...\n",
            "Not init distributed mode.\n",
            "cfg:  {'task': 'detection', '_model': None, '_postprocessor': None, '_criterion': None, '_optimizer': None, '_lr_scheduler': None, '_lr_warmup_scheduler': None, '_train_dataloader': None, '_val_dataloader': None, '_ema': None, '_scaler': None, '_train_dataset': None, '_val_dataset': None, '_collate_fn': None, '_evaluator': None, '_writer': None, 'num_workers': 0, 'batch_size': None, '_train_batch_size': None, '_val_batch_size': None, '_train_shuffle': None, '_val_shuffle': None, 'resume': None, 'tuning': None, 'epoches': 80, 'last_epoch': -1, 'use_amp': True, 'use_ema': True, 'ema_decay': 0.9999, 'ema_warmups': 2000, 'sync_bn': True, 'clip_max_norm': 0.1, 'find_unused_parameters': False, 'seed': 0, 'print_freq': 100, 'checkpoint_freq': 12, 'output_dir': './output/dfine_hgnetv2_l_coco', 'summary_dir': None, 'device': '', 'yaml_cfg': {'task': 'detection', 'evaluator': {'type': 'CocoEvaluator', 'iou_types': ['bbox']}, 'num_classes': 80, 'remap_mscoco_category': True, 'train_dataloader': {'type': 'DataLoader', 'dataset': {'type': 'CocoDetection', 'img_folder': '/data/COCO2017/train2017/', 'ann_file': '/data/COCO2017/annotations/instances_train2017.json', 'return_masks': False, 'transforms': {'type': 'Compose', 'ops': [{'type': 'RandomPhotometricDistort', 'p': 0.5}, {'type': 'RandomZoomOut', 'fill': 0}, {'type': 'RandomIoUCrop', 'p': 0.8}, {'type': 'SanitizeBoundingBoxes', 'min_size': 1}, {'type': 'RandomHorizontalFlip'}, {'type': 'Resize', 'size': [640, 640]}, {'type': 'SanitizeBoundingBoxes', 'min_size': 1}, {'type': 'ConvertPILImage', 'dtype': 'float32', 'scale': True}, {'type': 'ConvertBoxes', 'fmt': 'cxcywh', 'normalize': True}], 'policy': {'name': 'stop_epoch', 'epoch': 72, 'ops': ['RandomPhotometricDistort', 'RandomZoomOut', 'RandomIoUCrop']}}}, 'shuffle': True, 'num_workers': 4, 'drop_last': True, 'collate_fn': {'type': 'BatchImageCollateFunction', 'base_size': 640, 'base_size_repeat': 4, 'stop_epoch': 72, 'ema_restart_decay': 0.9999}, 'total_batch_size': 32}, 'val_dataloader': {'type': 'DataLoader', 'dataset': {'type': 'CocoDetection', 'img_folder': '/data/COCO2017/val2017/', 'ann_file': '/data/COCO2017/annotations/instances_val2017.json', 'return_masks': False, 'transforms': {'type': 'Compose', 'ops': [{'type': 'Resize', 'size': [640, 640]}, {'type': 'ConvertPILImage', 'dtype': 'float32', 'scale': True}]}}, 'shuffle': False, 'num_workers': 4, 'drop_last': False, 'collate_fn': {'type': 'BatchImageCollateFunction'}, 'total_batch_size': 64}, 'print_freq': 100, 'output_dir': './output/dfine_hgnetv2_l_coco', 'checkpoint_freq': 12, 'sync_bn': True, 'find_unused_parameters': False, 'use_amp': True, 'scaler': {'type': 'GradScaler', 'enabled': True}, 'use_ema': True, 'ema': {'type': 'ModelEMA', 'decay': 0.9999, 'warmups': 1000, 'start': 0}, 'epoches': 80, 'clip_max_norm': 0.1, 'optimizer': {'type': 'AdamW', 'params': [{'params': '^(?=.*backbone)(?!.*norm|bn).*$', 'lr': 1.25e-05}, {'params': '^(?=.*(?:encoder|decoder))(?=.*(?:norm|bn)).*$', 'weight_decay': 0.0}], 'lr': 0.00025, 'betas': [0.9, 0.999], 'weight_decay': 0.000125}, 'lr_scheduler': {'type': 'MultiStepLR', 'milestones': [500], 'gamma': 0.1}, 'lr_warmup_scheduler': {'type': 'LinearWarmup', 'warmup_duration': 500}, 'model': 'DFINE', 'criterion': 'DFINECriterion', 'postprocessor': 'DFINEPostProcessor', 'use_focal_loss': True, 'eval_spatial_size': [640, 640], 'DFINE': {'backbone': 'HGNetv2', 'encoder': 'HybridEncoder', 'decoder': 'DFINETransformer'}, 'HGNetv2': {'pretrained': True, 'local_model_dir': 'weight/hgnetv2/', 'name': 'B4', 'return_idx': [1, 2, 3], 'freeze_stem_only': True, 'freeze_at': 0, 'freeze_norm': True}, 'HybridEncoder': {'in_channels': [512, 1024, 2048], 'feat_strides': [8, 16, 32], 'hidden_dim': 256, 'use_encoder_idx': [2], 'num_encoder_layers': 1, 'nhead': 8, 'dim_feedforward': 1024, 'dropout': 0.0, 'enc_act': 'gelu', 'expansion': 1.0, 'depth_mult': 1, 'act': 'silu'}, 'DFINETransformer': {'feat_channels': [256, 256, 256], 'feat_strides': [8, 16, 32], 'hidden_dim': 256, 'num_levels': 3, 'num_layers': 6, 'eval_idx': -1, 'num_queries': 300, 'num_denoising': 100, 'label_noise_ratio': 0.5, 'box_noise_scale': 1.0, 'reg_max': 32, 'reg_scale': 4, 'layer_scale': 1, 'num_points': [3, 6, 3], 'cross_attn_method': 'default', 'query_select_method': 'default'}, 'DFINEPostProcessor': {'num_top_queries': 300}, 'DFINECriterion': {'weight_dict': {'loss_vfl': 1, 'loss_bbox': 5, 'loss_giou': 2, 'loss_fgl': 0.15, 'loss_ddf': 1.5}, 'losses': ['vfl', 'boxes', 'local'], 'alpha': 0.75, 'gamma': 2.0, 'reg_max': 32, 'matcher': {'type': 'HungarianMatcher', 'weight_dict': {'cost_class': 2, 'cost_bbox': 5, 'cost_giou': 2}, 'alpha': 0.25, 'gamma': 2.0}}, '__include__': ['../dataset/coco_detection.yml', '../runtime.yml', './include/dataloader.yml', './include/optimizer.yml', './include/dfine_hgnetv2.yml'], 'config': 'configs/dfine/dfine_hgnetv2_l_coco.yml', 'seed': 0, 'test_only': False, 'print_method': 'builtin', 'print_rank': 0}}\n",
            "\u001b[92mIf the pretrained HGNetV2 can't be downloaded automatically. Please check your network connection.\u001b[0m\n",
            "\u001b[92mPlease check your network connection. Or download the model manually from \u001b[0mhttps://github.com/Peterande/storage/releases/download/dfinev1.0/PPHGNetV2_B4_stage1.pth\u001b[92m to \u001b[0mweight/hgnetv2/.\u001b[0m\n",
            "Downloading: \"https://github.com/Peterande/storage/releases/download/dfinev1.0/PPHGNetV2_B4_stage1.pth\" to weight/hgnetv2/PPHGNetV2_B4_stage1.pth\n",
            "100% 52.0M/52.0M [00:01<00:00, 31.1MB/s]\n",
            "/content/D-FINE/src/nn/backbone/hgnetv2.py:505: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(local_model_dir)\n",
            "/content/D-FINE/src/nn/backbone/hgnetv2.py:505: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(local_model_dir)\n",
            "/content/D-FINE/src/nn/backbone/hgnetv2.py:505: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(local_model_dir)\n",
            "Loaded stage1 B4 HGNetV2 from URL.\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/dfine_hgnetv2_l_coco.yml --use-amp --seed=0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "OLROposmmvOj",
        "outputId": "7eae1cf5-5709-45b9-f376-8e05f9535bb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "configs     LICENSE\t  README_ja.md\treference\t  src\t train.py\n",
            "Dockerfile  README_cn.md  README.md\trequirements.txt  tools\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n/content/D-FINE/configs/dfine/dfine_hgnetv2_l_coco.yml\\n/content/D-FINE/configs/dfine/custom/dfine_hgnetv2_l_custom.yml'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "!ls\n",
        "'''\n",
        "/content/D-FINE/configs/dfine/dfine_hgnetv2_l_coco.yml\n",
        "/content/D-FINE/configs/dfine/custom/dfine_hgnetv2_l_custom.yml'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VG_bQbPym4Xy",
        "outputId": "162d898f-db42-4282-b6fa-2848f5c8722a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W1120 15:24:00.462000 10949 torch/distributed/run.py:793] \n",
            "W1120 15:24:00.462000 10949 torch/distributed/run.py:793] *****************************************\n",
            "W1120 15:24:00.462000 10949 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
            "W1120 15:24:00.462000 10949 torch/distributed/run.py:793] *****************************************\n",
            "W1120 15:24:06.191000 10949 torch/distributed/elastic/agent/server/api.py:704] Received Signals.SIGINT death signal, shutting down workers\n",
            "W1120 15:24:06.191000 10949 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 10964 closing signal SIGINT\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/D-FINE/train.py\", line 15, in <module>\n",
            "    from src.misc import dist_utils\n",
            "  File \"/content/D-FINE/src/__init__.py\", line 6, in <module>\n",
            "W1120 15:24:06.192000 10949 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 10965 closing signal SIGINT\n",
            "    from . import optim\n",
            "  File \"/content/D-FINE/src/optim/__init__.py\", line 6, in <module>\n",
            "    from .ema import *\n",
            "  File \"/content/D-FINE/src/optim/ema.py\", line 10, in <module>\n",
            "W1120 15:24:06.192000 10949 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 10966 closing signal SIGINT\n",
            "    import torch\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2486, in <module>\n",
            "W1120 15:24:06.193000 10949 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 10967 closing signal SIGINT\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/D-FINE/train.py\", line 15, in <module>\n",
            "    from src.misc import dist_utils  File \"/content/D-FINE/train.py\", line 15, in <module>\n",
            "    from src.misc import dist_utils\n",
            "  File \"/content/D-FINE/src/__init__.py\", line 6, in <module>\n",
            "    from torch import _meta_registrations\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_meta_registrations.py\", line 563, in <module>\n",
            "        def meta_index_reduce_(from . import optim\n",
            "\n",
            "  File \"/content/D-FINE/src/optim/__init__.py\", line 6, in <module>\n",
            "  File \"/content/D-FINE/src/__init__.py\", line 6, in <module>\n",
            "    from .ema import *    from . import optim\n",
            "  File \"/content/D-FINE/src/optim/__init__.py\", line 6, in <module>\n",
            "    \n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_meta_registrations.py\", line 51, in wrapper\n",
            "    from .ema import *\n",
            "  File \"/content/D-FINE/src/optim/ema.py\", line 10, in <module>\n",
            "    pytree.tree_map_(register, op)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py\", line 997, in tree_map_\n",
            "import torch\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2486, in <module>\n",
            "\n",
            "  File \"/content/D-FINE/src/optim/ema.py\", line 10, in <module>\n",
            "    import torch\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2486, in <module>\n",
            "    from torch import _meta_registrations\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_meta_registrations.py\", line 10, in <module>\n",
            "    from torch._decomp import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_decomp/__init__.py\", line 249, in <module>\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/D-FINE/train.py\", line 15, in <module>\n",
            "    from src.misc import dist_utils\n",
            "  File \"/content/D-FINE/src/__init__.py\", line 6, in <module>\n",
            "    from . import optim\n",
            "  File \"/content/D-FINE/src/optim/__init__.py\", line 6, in <module>\n",
            "    from .ema import *\n",
            "  File \"/content/D-FINE/src/optim/ema.py\", line 10, in <module>\n",
            "    import torch\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2486, in <module>\n",
            "    from torch import _meta_registrations\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_meta_registrations.py\", line 10, in <module>\n",
            "        tuple(map(func, *flat_args))  # consume and exhaust the iterable\n",
            "KeyboardInterrupt\n",
            "from torch._decomp import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_decomp/__init__.py\", line 249, in <module>\n",
            "    import torch._decomp.decompositions\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_decomp/decompositions.py\", line 15, in <module>\n",
            "    import torch._decomp.decompositions\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_decomp/decompositions.py\", line 5075, in <module>\n",
            "    import torch._prims as prims\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_prims/__init__.py\", line 2931, in <module>\n",
            "        def take(self, index):\n",
            "_uniform_helper = _make_prim(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_prims_common/wrappers.py\", line 259, in _out_wrapper\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_prims/__init__.py\", line 319, in _make_prim\n",
            "    from torch import _meta_registrations\n",
            "    prim_def = torch.library.custom_op(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_library/custom_ops.py\", line 157, in custom_op\n",
            "    sig = inspect.signature(fn)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 3254, in signature\n",
            "    return inner(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_library/custom_ops.py\", line 138, in inner\n",
            "    result = CustomOpDef(namespace, opname, schema_str, fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_library/custom_ops.py\", line 186, in __init__\n",
            "    self._register_to_dispatcher()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_library/custom_ops.py\", line 618, in _register_to_dispatcher\n",
            "    autograd_impl = autograd.make_autograd_impl(self._opoverload, self)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_library/autograd.py\", line 29, in make_autograd_impl\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_meta_registrations.py\", line 10, in <module>\n",
            "    from torch._decomp import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_decomp/__init__.py\", line 250, in <module>\n",
            "    import torch._refs\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_refs/__init__.py\", line 6544, in <module>\n",
            "        class Metadata:\n",
            "  File \"/usr/lib/python3.10/dataclasses.py\", line 1184, in dataclass\n",
            "    return wrap(cls)\n",
            "  File \"/usr/lib/python3.10/dataclasses.py\", line 1175, in wrap\n",
            "return Signature.from_callable(obj, follow_wrapped=follow_wrapped,\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 3002, in from_callable\n",
            "    return _process_class(cls, init, repr, eq, order, unsafe_hash,\n",
            "  File \"/usr/lib/python3.10/dataclasses.py\", line 1024, in _process_class\n",
            "    _init_fn(all_init_fields,\n",
            "  File \"/usr/lib/python3.10/dataclasses.py\", line 579, in _init_fn\n",
            "        return _create_fn('__init__',\n",
            "  File \"/usr/lib/python3.10/dataclasses.py\", line 432, in _create_fn\n",
            "    exec(txt, globals, ns)\n",
            "  File \"<string>\", line 1, in <module>\n",
            "KeyboardInterrupt\n",
            "import torch._refs.nn.functional\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_refs/nn/functional/__init__.py\", line 1048, in <module>\n",
            "    return _signature_from_callable(obj, sigcls=cls,\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 2463, in _signature_from_callable\n",
            "    return _signature_from_function(sigcls, obj,\n",
            "    def gelu(a: TensorLikeType, approximate: str = \"none\") -> TensorLikeType:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_decomp/__init__.py\", line 191, in decomposition_decorator\n",
            "    pytree.tree_map_(register, aten_op)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py\", line 997, in tree_map_\n",
            "      File \"/usr/lib/python3.10/inspect.py\", line 2370, in _signature_from_function\n",
            "tuple(map(func, *flat_args))  # consume and exhaust the iterable\n",
            "KeyboardInterrupt\n",
            "    return cls(parameters,\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 2969, in __init__\n",
            "    params = OrderedDict((param.name, param) for param in parameters)\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/torchrun\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 919, in main\n",
            "    run(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 910, in run\n",
            "    elastic_launch(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 260, in launch_agent\n",
            "    result = agent.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/metrics/api.py\", line 137, in wrapper\n",
            "    result = f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 696, in run\n",
            "    result = self._invoke_run(role)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 855, in _invoke_run\n",
            "    time.sleep(monitor_interval)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 84, in _terminate_process_handler\n",
            "    raise SignalException(f\"Process {os.getpid()} got signal: {sigval}\", sigval=sigval)\n",
            "torch.distributed.elastic.multiprocessing.api.SignalException: Process 10949 got signal: 2\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c /content/D-FINE/configs/dfine/custom/dfine_hgnetv2_l_custom.yml --use-amp --seed=0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!export model=l  # n s m l x"
      ],
      "metadata": {
        "id": "7mx9HrNzJHmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(f\"Number of available GPUs: {torch.cuda.device_count()}\")\n",
        "'''/content/D-FINE/configs/dfine/custom/dfine_hgnetv2_l_custom.yml '''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sewGZ2zrsff",
        "outputId": "6b4f4f55-923b-4aba-a6bf-18ced128f766"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of available GPUs: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python train.py -c /content/D-FINE/configs/dfine/custom/dfine_hgnetv2_l_custom.yml --use-amp --seed=0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pOIl2w_x86E",
        "outputId": "22c5bfab-fc0e-44c3-a1c9-6beca8e8d392"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-21 07:55:34.606990: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-21 07:55:34.626815: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-21 07:55:34.632761: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-21 07:55:34.647364: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-21 07:55:35.695563: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Initialized distributed mode...\n",
            "cfg:  {'task': 'detection', '_model': None, '_postprocessor': None, '_criterion': None, '_optimizer': None, '_lr_scheduler': None, '_lr_warmup_scheduler': None, '_train_dataloader': None, '_val_dataloader': None, '_ema': None, '_scaler': None, '_train_dataset': None, '_val_dataset': None, '_collate_fn': None, '_evaluator': None, '_writer': None, 'num_workers': 0, 'batch_size': None, '_train_batch_size': None, '_val_batch_size': None, '_train_shuffle': None, '_val_shuffle': None, 'resume': None, 'tuning': None, 'epoches': 10, 'last_epoch': -1, 'use_amp': True, 'use_ema': True, 'ema_decay': 0.9999, 'ema_warmups': 2000, 'sync_bn': True, 'clip_max_norm': 0.1, 'find_unused_parameters': False, 'seed': 0, 'print_freq': 100, 'checkpoint_freq': 12, 'output_dir': './output/dfine_hgnetv2_l_custom', 'summary_dir': None, 'device': '', 'yaml_cfg': {'task': 'detection', 'evaluator': {'type': 'CocoEvaluator', 'iou_types': ['bbox']}, 'num_classes': 777, 'remap_mscoco_category': False, 'train_dataloader': {'type': 'DataLoader', 'dataset': {'type': 'CocoDetection', 'img_folder': '/content/drive/MyDrive/dataset/dataset/images/train', 'ann_file': '/content/drive/MyDrive/dataset/dataset/annotations/instances_train.json', 'return_masks': False, 'transforms': {'type': 'Compose', 'ops': [{'type': 'RandomPhotometricDistort', 'p': 0.5}, {'type': 'RandomZoomOut', 'fill': 0}, {'type': 'RandomIoUCrop', 'p': 0.8}, {'type': 'SanitizeBoundingBoxes', 'min_size': 1}, {'type': 'RandomHorizontalFlip'}, {'type': 'Resize', 'size': [640, 640]}, {'type': 'SanitizeBoundingBoxes', 'min_size': 1}, {'type': 'ConvertPILImage', 'dtype': 'float32', 'scale': True}, {'type': 'ConvertBoxes', 'fmt': 'cxcywh', 'normalize': True}], 'policy': {'name': 'stop_epoch', 'epoch': 72, 'ops': ['RandomPhotometricDistort', 'RandomZoomOut', 'RandomIoUCrop']}}}, 'shuffle': True, 'num_workers': 4, 'drop_last': True, 'collate_fn': {'type': 'BatchImageCollateFunction', 'base_size': 640, 'base_size_repeat': 4, 'stop_epoch': 72, 'ema_restart_decay': 0.9999}, 'total_batch_size': 2}, 'val_dataloader': {'type': 'DataLoader', 'dataset': {'type': 'CocoDetection', 'img_folder': '/content/drive/MyDrive/dataset/dataset/images/val', 'ann_file': '/content/drive/MyDrive/dataset/dataset/annotations/instances_val.json', 'return_masks': False, 'transforms': {'type': 'Compose', 'ops': [{'type': 'Resize', 'size': [640, 640]}, {'type': 'ConvertPILImage', 'dtype': 'float32', 'scale': True}]}}, 'shuffle': False, 'num_workers': 4, 'drop_last': False, 'collate_fn': {'type': 'BatchImageCollateFunction'}, 'total_batch_size': 4}, 'print_freq': 100, 'output_dir': './output/dfine_hgnetv2_l_custom', 'checkpoint_freq': 12, 'sync_bn': True, 'find_unused_parameters': False, 'use_amp': True, 'scaler': {'type': 'GradScaler', 'enabled': True}, 'use_ema': True, 'ema': {'type': 'ModelEMA', 'decay': 0.9999, 'warmups': 1000, 'start': 0}, 'epoches': 10, 'clip_max_norm': 0.1, 'optimizer': {'type': 'AdamW', 'params': [{'params': '^(?=.*backbone)(?!.*norm|bn).*$', 'lr': 1.25e-05}, {'params': '^(?=.*(?:encoder|decoder))(?=.*(?:norm|bn)).*$', 'weight_decay': 0.0}], 'lr': 0.00025, 'betas': [0.9, 0.999], 'weight_decay': 0.000125}, 'lr_scheduler': {'type': 'MultiStepLR', 'milestones': [500], 'gamma': 0.1}, 'lr_warmup_scheduler': {'type': 'LinearWarmup', 'warmup_duration': 500}, 'model': 'DFINE', 'criterion': 'DFINECriterion', 'postprocessor': 'DFINEPostProcessor', 'use_focal_loss': True, 'eval_spatial_size': [640, 640], 'DFINE': {'backbone': 'HGNetv2', 'encoder': 'HybridEncoder', 'decoder': 'DFINETransformer'}, 'HGNetv2': {'pretrained': True, 'local_model_dir': 'weight/hgnetv2/', 'name': 'B4', 'return_idx': [1, 2, 3], 'freeze_stem_only': True, 'freeze_at': 0, 'freeze_norm': True}, 'HybridEncoder': {'in_channels': [512, 1024, 2048], 'feat_strides': [8, 16, 32], 'hidden_dim': 256, 'use_encoder_idx': [2], 'num_encoder_layers': 1, 'nhead': 8, 'dim_feedforward': 1024, 'dropout': 0.0, 'enc_act': 'gelu', 'expansion': 1.0, 'depth_mult': 1, 'act': 'silu'}, 'DFINETransformer': {'feat_channels': [256, 256, 256], 'feat_strides': [8, 16, 32], 'hidden_dim': 256, 'num_levels': 3, 'num_layers': 6, 'eval_idx': -1, 'num_queries': 300, 'num_denoising': 100, 'label_noise_ratio': 0.5, 'box_noise_scale': 1.0, 'reg_max': 32, 'reg_scale': 4, 'layer_scale': 1, 'num_points': [3, 6, 3], 'cross_attn_method': 'default', 'query_select_method': 'default'}, 'DFINEPostProcessor': {'num_top_queries': 300}, 'DFINECriterion': {'weight_dict': {'loss_vfl': 1, 'loss_bbox': 5, 'loss_giou': 2, 'loss_fgl': 0.15, 'loss_ddf': 1.5}, 'losses': ['vfl', 'boxes', 'local'], 'alpha': 0.75, 'gamma': 2.0, 'reg_max': 32, 'matcher': {'type': 'HungarianMatcher', 'weight_dict': {'cost_class': 2, 'cost_bbox': 5, 'cost_giou': 2}, 'alpha': 0.25, 'gamma': 2.0}}, '__include__': ['../../dataset/custom_detection.yml', '../../runtime.yml', '../include/dataloader.yml', '../include/optimizer.yml', '../include/dfine_hgnetv2.yml'], 'config': '/content/D-FINE/configs/dfine/custom/dfine_hgnetv2_l_custom.yml', 'seed': 0, 'test_only': False, 'print_method': 'builtin', 'print_rank': 0}}\n",
            "/content/D-FINE/src/nn/backbone/hgnetv2.py:494: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(model_path, map_location='cpu')\n",
            "Loaded stage1 B4 HGNetV2 from local file.\n",
            "/content/D-FINE/src/core/workspace.py:180: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  return module(**module_kwargs)\n",
            "Initial lr: [1.25e-05, 0.00025, 0.00025]\n",
            "building train_dataloader with batch_size=2...\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "building val_dataloader with batch_size=4...\n",
            "\n",
            "------------------------------------- Calculate Flops Results -------------------------------------\n",
            "Notations:\n",
            "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
            "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
            "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
            "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
            "\n",
            "Total Training Params:                                                  31.24 M \n",
            "fwd MACs:                                                               47.0033 GMACs\n",
            "fwd FLOPs:                                                              94.1391 GFLOPS\n",
            "fwd+bwd MACs:                                                           141.01 GMACs\n",
            "fwd+bwd FLOPs:                                                          282.417 GFLOPS\n",
            "---------------------------------------------------------------------------------------------------\n",
            "{'Model FLOPs:94.1391 GFLOPS   MACs:47.0033 GMACs   Params:31260885'}\n",
            "------------------------------------------Start training-------------------------------------------\n",
            "Epoch: [0]  [  0/500]  eta: 0:34:49  lr: 0.000000  loss: 49.5673 (49.5673)  loss_vfl: 0.0563 (0.0563)  loss_bbox: 0.8785 (0.8785)  loss_giou: 2.7323 (2.7323)  loss_fgl: 0.0205 (0.0205)  loss_vfl_aux_0: 0.0572 (0.0572)  loss_bbox_aux_0: 0.8785 (0.8785)  loss_giou_aux_0: 2.7323 (2.7323)  loss_fgl_aux_0: 0.0205 (0.0205)  loss_vfl_aux_1: 0.0531 (0.0531)  loss_bbox_aux_1: 0.8785 (0.8785)  loss_giou_aux_1: 2.7323 (2.7323)  loss_fgl_aux_1: 0.0205 (0.0205)  loss_vfl_aux_2: 0.0554 (0.0554)  loss_bbox_aux_2: 0.8785 (0.8785)  loss_giou_aux_2: 2.7323 (2.7323)  loss_fgl_aux_2: 0.0205 (0.0205)  loss_vfl_aux_3: 0.0525 (0.0525)  loss_bbox_aux_3: 0.8785 (0.8785)  loss_giou_aux_3: 2.7323 (2.7323)  loss_fgl_aux_3: 0.0205 (0.0205)  loss_vfl_aux_4: 0.0522 (0.0522)  loss_bbox_aux_4: 0.8785 (0.8785)  loss_giou_aux_4: 2.7323 (2.7323)  loss_fgl_aux_4: 0.0205 (0.0205)  loss_vfl_pre: 0.0572 (0.0572)  loss_bbox_pre: 0.8785 (0.8785)  loss_giou_pre: 2.7323 (2.7323)  loss_vfl_enc_0: 0.0692 (0.0692)  loss_bbox_enc_0: 0.8785 (0.8785)  loss_giou_enc_0: 2.7323 (2.7323)  loss_vfl_dn_0: 0.7188 (0.7188)  loss_bbox_dn_0: 0.0675 (0.0675)  loss_giou_dn_0: 1.4098 (1.4098)  loss_fgl_dn_0: 0.7860 (0.7860)  loss_vfl_dn_1: 0.7524 (0.7524)  loss_bbox_dn_1: 0.0675 (0.0675)  loss_giou_dn_1: 1.4098 (1.4098)  loss_fgl_dn_1: 0.7860 (0.7860)  loss_vfl_dn_2: 0.7603 (0.7603)  loss_bbox_dn_2: 0.0675 (0.0675)  loss_giou_dn_2: 1.4098 (1.4098)  loss_fgl_dn_2: 0.7860 (0.7860)  loss_vfl_dn_3: 0.6846 (0.6846)  loss_bbox_dn_3: 0.0675 (0.0675)  loss_giou_dn_3: 1.4098 (1.4098)  loss_fgl_dn_3: 0.7860 (0.7860)  loss_vfl_dn_4: 0.7222 (0.7222)  loss_bbox_dn_4: 0.0675 (0.0675)  loss_giou_dn_4: 1.4098 (1.4098)  loss_fgl_dn_4: 0.7860 (0.7860)  loss_vfl_dn_5: 0.6899 (0.6899)  loss_bbox_dn_5: 0.0675 (0.0675)  loss_giou_dn_5: 1.4098 (1.4098)  loss_fgl_dn_5: 0.7860 (0.7860)  loss_vfl_dn_pre: 0.7188 (0.7188)  loss_bbox_dn_pre: 0.0675 (0.0675)  loss_giou_dn_pre: 1.4098 (1.4098)  time: 4.1786  data: 1.0590  max mem: 2183\n",
            "Epoch: [0]  [100/500]  eta: 0:04:40  lr: 0.000003  loss: 42.1363 (47.2895)  loss_vfl: 0.3777 (0.2272)  loss_bbox: 0.4971 (0.8380)  loss_giou: 1.6685 (2.1304)  loss_fgl: 0.4077 (0.2493)  loss_vfl_aux_0: 0.3516 (0.2160)  loss_bbox_aux_0: 0.5021 (0.8363)  loss_giou_aux_0: 1.6827 (2.1415)  loss_fgl_aux_0: 0.4031 (0.2472)  loss_vfl_aux_1: 0.3586 (0.2244)  loss_bbox_aux_1: 0.5009 (0.8368)  loss_giou_aux_1: 1.6801 (2.1386)  loss_fgl_aux_1: 0.4042 (0.2475)  loss_vfl_aux_2: 0.3716 (0.2239)  loss_bbox_aux_2: 0.4993 (0.8372)  loss_giou_aux_2: 1.6772 (2.1364)  loss_fgl_aux_2: 0.4057 (0.2479)  loss_vfl_aux_3: 0.3704 (0.2199)  loss_bbox_aux_3: 0.4984 (0.8374)  loss_giou_aux_3: 1.6739 (2.1341)  loss_fgl_aux_3: 0.4063 (0.2484)  loss_vfl_aux_4: 0.4072 (0.2265)  loss_bbox_aux_4: 0.4976 (0.8378)  loss_giou_aux_4: 1.6713 (2.1323)  loss_fgl_aux_4: 0.4067 (0.2488)  loss_vfl_pre: 0.3489 (0.2153)  loss_bbox_pre: 0.5029 (0.8359)  loss_giou_pre: 1.6836 (2.1428)  loss_vfl_enc_0: 0.3250 (0.2412)  loss_bbox_enc_0: 0.5049 (0.8345)  loss_giou_enc_0: 1.6864 (2.1471)  loss_vfl_dn_0: 0.3574 (0.5338)  loss_bbox_dn_0: 0.2217 (0.2757)  loss_giou_dn_0: 1.3731 (1.3797)  loss_fgl_dn_0: 0.8123 (0.8139)  loss_vfl_dn_1: 0.3701 (0.5703)  loss_bbox_dn_1: 0.2214 (0.2761)  loss_giou_dn_1: 1.3746 (1.3799)  loss_fgl_dn_1: 0.8030 (0.8118)  loss_vfl_dn_2: 0.3762 (0.5547)  loss_bbox_dn_2: 0.2216 (0.2767)  loss_giou_dn_2: 1.3762 (1.3800)  loss_fgl_dn_2: 0.7983 (0.8105)  loss_vfl_dn_3: 0.3740 (0.5316)  loss_bbox_dn_3: 0.2212 (0.2772)  loss_giou_dn_3: 1.3778 (1.3802)  loss_fgl_dn_3: 0.7953 (0.8091)  loss_vfl_dn_4: 0.3789 (0.5107)  loss_bbox_dn_4: 0.2211 (0.2778)  loss_giou_dn_4: 1.3778 (1.3803)  loss_fgl_dn_4: 0.7939 (0.8083)  loss_vfl_dn_5: 0.3867 (0.5048)  loss_bbox_dn_5: 0.2206 (0.2783)  loss_giou_dn_5: 1.3777 (1.3805)  loss_fgl_dn_5: 0.7927 (0.8073)  loss_vfl_dn_pre: 0.3574 (0.5339)  loss_bbox_dn_pre: 0.2217 (0.2755)  loss_giou_dn_pre: 1.3721 (1.3796)  loss_ddf_aux_0: 0.0067 (0.0022)  loss_ddf_aux_1: 0.0037 (0.0013)  loss_ddf_aux_2: 0.0020 (0.0007)  loss_ddf_aux_3: 0.0008 (0.0003)  loss_ddf_aux_4: 0.0003 (0.0001)  loss_ddf_dn_0: 0.0090 (0.0032)  loss_ddf_dn_1: 0.0046 (0.0018)  loss_ddf_dn_2: 0.0025 (0.0010)  loss_ddf_dn_3: 0.0009 (0.0004)  loss_ddf_dn_4: 0.0004 (0.0002)  time: 0.6886  data: 0.0095  max mem: 3024\n",
            "Epoch: [0]  [200/500]  eta: 0:03:26  lr: 0.000005  loss: 41.9644 (44.6448)  loss_vfl: 0.4319 (0.3332)  loss_bbox: 0.4426 (0.6775)  loss_giou: 1.5357 (1.8532)  loss_fgl: 0.5228 (0.3917)  loss_vfl_aux_0: 0.3511 (0.2929)  loss_bbox_aux_0: 0.4677 (0.6866)  loss_giou_aux_0: 1.5650 (1.8658)  loss_fgl_aux_0: 0.5056 (0.3835)  loss_vfl_aux_1: 0.3591 (0.3033)  loss_bbox_aux_1: 0.4553 (0.6824)  loss_giou_aux_1: 1.5568 (1.8631)  loss_fgl_aux_1: 0.5133 (0.3859)  loss_vfl_aux_2: 0.3879 (0.3082)  loss_bbox_aux_2: 0.4507 (0.6804)  loss_giou_aux_2: 1.5533 (1.8595)  loss_fgl_aux_2: 0.5137 (0.3882)  loss_vfl_aux_3: 0.3850 (0.3149)  loss_bbox_aux_3: 0.4465 (0.6788)  loss_giou_aux_3: 1.5429 (1.8566)  loss_fgl_aux_3: 0.5200 (0.3900)  loss_vfl_aux_4: 0.4204 (0.3289)  loss_bbox_aux_4: 0.4447 (0.6781)  loss_giou_aux_4: 1.5365 (1.8545)  loss_fgl_aux_4: 0.5190 (0.3909)  loss_vfl_pre: 0.3220 (0.2894)  loss_bbox_pre: 0.4782 (0.6904)  loss_giou_pre: 1.5717 (1.8672)  loss_vfl_enc_0: 0.3262 (0.3034)  loss_bbox_enc_0: 0.4907 (0.6928)  loss_giou_enc_0: 1.6103 (1.8729)  loss_vfl_dn_0: 0.3357 (0.4406)  loss_bbox_dn_0: 0.2784 (0.2686)  loss_giou_dn_0: 1.3682 (1.3794)  loss_fgl_dn_0: 0.7949 (0.8051)  loss_vfl_dn_1: 0.3423 (0.4584)  loss_bbox_dn_1: 0.2790 (0.2684)  loss_giou_dn_1: 1.3637 (1.3804)  loss_fgl_dn_1: 0.7937 (0.7992)  loss_vfl_dn_2: 0.3445 (0.4524)  loss_bbox_dn_2: 0.2836 (0.2692)  loss_giou_dn_2: 1.3551 (1.3794)  loss_fgl_dn_2: 0.7962 (0.7981)  loss_vfl_dn_3: 0.3533 (0.4427)  loss_bbox_dn_3: 0.2859 (0.2699)  loss_giou_dn_3: 1.3507 (1.3788)  loss_fgl_dn_3: 0.7975 (0.7972)  loss_vfl_dn_4: 0.3562 (0.4336)  loss_bbox_dn_4: 0.2889 (0.2711)  loss_giou_dn_4: 1.3488 (1.3784)  loss_fgl_dn_4: 0.7982 (0.7971)  loss_vfl_dn_5: 0.3518 (0.4317)  loss_bbox_dn_5: 0.2899 (0.2719)  loss_giou_dn_5: 1.3481 (1.3783)  loss_fgl_dn_5: 0.7989 (0.7966)  loss_vfl_dn_pre: 0.3364 (0.4414)  loss_bbox_dn_pre: 0.2791 (0.2695)  loss_giou_dn_pre: 1.3691 (1.3782)  loss_ddf_aux_0: 0.0199 (0.0129)  loss_ddf_aux_1: 0.0115 (0.0056)  loss_ddf_aux_2: 0.0043 (0.0025)  loss_ddf_aux_3: 0.0012 (0.0008)  loss_ddf_aux_4: 0.0003 (0.0003)  loss_ddf_dn_0: 0.0187 (0.0140)  loss_ddf_dn_1: 0.0104 (0.0059)  loss_ddf_dn_2: 0.0038 (0.0028)  loss_ddf_dn_3: 0.0012 (0.0009)  loss_ddf_dn_4: 0.0003 (0.0003)  time: 0.6976  data: 0.0086  max mem: 3024\n",
            "Epoch: [0]  [300/500]  eta: 0:02:16  lr: 0.000008  loss: 41.5892 (43.8987)  loss_vfl: 0.5093 (0.3891)  loss_bbox: 0.3547 (0.6092)  loss_giou: 1.4193 (1.7444)  loss_fgl: 0.7268 (0.4661)  loss_vfl_aux_0: 0.4734 (0.3303)  loss_bbox_aux_0: 0.3619 (0.6252)  loss_giou_aux_0: 1.4448 (1.7639)  loss_fgl_aux_0: 0.7135 (0.4544)  loss_vfl_aux_1: 0.4597 (0.3461)  loss_bbox_aux_1: 0.3394 (0.6176)  loss_giou_aux_1: 1.4446 (1.7584)  loss_fgl_aux_1: 0.7115 (0.4590)  loss_vfl_aux_2: 0.4868 (0.3581)  loss_bbox_aux_2: 0.3467 (0.6134)  loss_giou_aux_2: 1.4195 (1.7504)  loss_fgl_aux_2: 0.7202 (0.4630)  loss_vfl_aux_3: 0.4773 (0.3695)  loss_bbox_aux_3: 0.3496 (0.6107)  loss_giou_aux_3: 1.4113 (1.7475)  loss_fgl_aux_3: 0.7246 (0.4648)  loss_vfl_aux_4: 0.4971 (0.3811)  loss_bbox_aux_4: 0.3522 (0.6100)  loss_giou_aux_4: 1.4142 (1.7453)  loss_fgl_aux_4: 0.7263 (0.4656)  loss_vfl_pre: 0.4424 (0.3243)  loss_bbox_pre: 0.3837 (0.6326)  loss_giou_pre: 1.4391 (1.7706)  loss_vfl_enc_0: 0.3782 (0.3256)  loss_bbox_enc_0: 0.5286 (0.6489)  loss_giou_enc_0: 1.6522 (1.8002)  loss_vfl_dn_0: 0.3555 (0.4097)  loss_bbox_dn_0: 0.2300 (0.2688)  loss_giou_dn_0: 1.2942 (1.3684)  loss_fgl_dn_0: 0.8548 (0.8089)  loss_vfl_dn_1: 0.3672 (0.4229)  loss_bbox_dn_1: 0.2247 (0.2677)  loss_giou_dn_1: 1.2802 (1.3656)  loss_fgl_dn_1: 0.8599 (0.8058)  loss_vfl_dn_2: 0.3792 (0.4212)  loss_bbox_dn_2: 0.2163 (0.2679)  loss_giou_dn_2: 1.2670 (1.3609)  loss_fgl_dn_2: 0.8611 (0.8068)  loss_vfl_dn_3: 0.3804 (0.4161)  loss_bbox_dn_3: 0.2131 (0.2677)  loss_giou_dn_3: 1.2525 (1.3597)  loss_fgl_dn_3: 0.8680 (0.8064)  loss_vfl_dn_4: 0.3923 (0.4111)  loss_bbox_dn_4: 0.2108 (0.2686)  loss_giou_dn_4: 1.2497 (1.3589)  loss_fgl_dn_4: 0.8665 (0.8065)  loss_vfl_dn_5: 0.3850 (0.4108)  loss_bbox_dn_5: 0.2102 (0.2689)  loss_giou_dn_5: 1.2510 (1.3587)  loss_fgl_dn_5: 0.8673 (0.8063)  loss_vfl_dn_pre: 0.3513 (0.4093)  loss_bbox_dn_pre: 0.2310 (0.2703)  loss_giou_dn_pre: 1.3108 (1.3699)  loss_ddf_aux_0: 0.0526 (0.0237)  loss_ddf_aux_1: 0.0282 (0.0115)  loss_ddf_aux_2: 0.0091 (0.0040)  loss_ddf_aux_3: 0.0023 (0.0012)  loss_ddf_aux_4: 0.0005 (0.0003)  loss_ddf_dn_0: 0.0825 (0.0289)  loss_ddf_dn_1: 0.0439 (0.0145)  loss_ddf_dn_2: 0.0130 (0.0049)  loss_ddf_dn_3: 0.0034 (0.0015)  loss_ddf_dn_4: 0.0006 (0.0004)  time: 0.6615  data: 0.0101  max mem: 3024\n",
            "Epoch: [0]  [400/500]  eta: 0:01:07  lr: 0.000010  loss: 40.6296 (43.2436)  loss_vfl: 0.5752 (0.4348)  loss_bbox: 0.2677 (0.5367)  loss_giou: 1.2029 (1.6555)  loss_fgl: 0.8485 (0.5377)  loss_vfl_aux_0: 0.5391 (0.3741)  loss_bbox_aux_0: 0.2795 (0.5517)  loss_giou_aux_0: 1.2367 (1.6767)  loss_fgl_aux_0: 0.8434 (0.5251)  loss_vfl_aux_1: 0.5308 (0.3879)  loss_bbox_aux_1: 0.2752 (0.5449)  loss_giou_aux_1: 1.2183 (1.6706)  loss_fgl_aux_1: 0.8473 (0.5294)  loss_vfl_aux_2: 0.5708 (0.4040)  loss_bbox_aux_2: 0.2713 (0.5406)  loss_giou_aux_2: 1.2060 (1.6611)  loss_fgl_aux_2: 0.8457 (0.5341)  loss_vfl_aux_3: 0.5596 (0.4132)  loss_bbox_aux_3: 0.2701 (0.5381)  loss_giou_aux_3: 1.2080 (1.6584)  loss_fgl_aux_3: 0.8467 (0.5361)  loss_vfl_aux_4: 0.5688 (0.4256)  loss_bbox_aux_4: 0.2685 (0.5375)  loss_giou_aux_4: 1.2050 (1.6562)  loss_fgl_aux_4: 0.8485 (0.5371)  loss_vfl_pre: 0.5449 (0.3691)  loss_bbox_pre: 0.2786 (0.5570)  loss_giou_pre: 1.2352 (1.6813)  loss_vfl_enc_0: 0.5000 (0.3577)  loss_bbox_enc_0: 0.3236 (0.5846)  loss_giou_enc_0: 1.3544 (1.7312)  loss_vfl_dn_0: 0.3667 (0.3981)  loss_bbox_dn_0: 0.1997 (0.2585)  loss_giou_dn_0: 1.2557 (1.3489)  loss_fgl_dn_0: 0.8712 (0.8216)  loss_vfl_dn_1: 0.3745 (0.4095)  loss_bbox_dn_1: 0.2011 (0.2568)  loss_giou_dn_1: 1.2467 (1.3434)  loss_fgl_dn_1: 0.8761 (0.8205)  loss_vfl_dn_2: 0.3857 (0.4105)  loss_bbox_dn_2: 0.1964 (0.2558)  loss_giou_dn_2: 1.2299 (1.3363)  loss_fgl_dn_2: 0.8900 (0.8231)  loss_vfl_dn_3: 0.3923 (0.4072)  loss_bbox_dn_3: 0.1954 (0.2553)  loss_giou_dn_3: 1.2255 (1.3344)  loss_fgl_dn_3: 0.8960 (0.8237)  loss_vfl_dn_4: 0.3945 (0.4045)  loss_bbox_dn_4: 0.1932 (0.2558)  loss_giou_dn_4: 1.2142 (1.3332)  loss_fgl_dn_4: 0.9016 (0.8244)  loss_vfl_dn_5: 0.4019 (0.4047)  loss_bbox_dn_5: 0.1925 (0.2559)  loss_giou_dn_5: 1.2121 (1.3328)  loss_fgl_dn_5: 0.9040 (0.8246)  loss_vfl_dn_pre: 0.3638 (0.3970)  loss_bbox_dn_pre: 0.1982 (0.2595)  loss_giou_dn_pre: 1.2657 (1.3518)  loss_ddf_aux_0: 0.0802 (0.0349)  loss_ddf_aux_1: 0.0411 (0.0175)  loss_ddf_aux_2: 0.0101 (0.0053)  loss_ddf_aux_3: 0.0028 (0.0014)  loss_ddf_aux_4: 0.0004 (0.0003)  loss_ddf_dn_0: 0.1358 (0.0534)  loss_ddf_dn_1: 0.0594 (0.0268)  loss_ddf_dn_2: 0.0144 (0.0073)  loss_ddf_dn_3: 0.0040 (0.0021)  loss_ddf_dn_4: 0.0006 (0.0005)  time: 0.6532  data: 0.0103  max mem: 3024\n",
            "Epoch: [0]  [499/500]  eta: 0:00:00  lr: 0.000013  loss: 39.5884 (42.6929)  loss_vfl: 0.5928 (0.4667)  loss_bbox: 0.2356 (0.4827)  loss_giou: 1.2523 (1.5858)  loss_fgl: 0.8307 (0.5889)  loss_vfl_aux_0: 0.5063 (0.4006)  loss_bbox_aux_0: 0.2226 (0.4987)  loss_giou_aux_0: 1.2817 (1.6104)  loss_fgl_aux_0: 0.8344 (0.5748)  loss_vfl_aux_1: 0.5591 (0.4171)  loss_bbox_aux_1: 0.2140 (0.4915)  loss_giou_aux_1: 1.2751 (1.6025)  loss_fgl_aux_1: 0.8359 (0.5795)  loss_vfl_aux_2: 0.5522 (0.4353)  loss_bbox_aux_2: 0.2280 (0.4865)  loss_giou_aux_2: 1.2612 (1.5913)  loss_fgl_aux_2: 0.8310 (0.5852)  loss_vfl_aux_3: 0.5898 (0.4457)  loss_bbox_aux_3: 0.2379 (0.4841)  loss_giou_aux_3: 1.2629 (1.5883)  loss_fgl_aux_3: 0.8289 (0.5873)  loss_vfl_aux_4: 0.6025 (0.4581)  loss_bbox_aux_4: 0.2365 (0.4834)  loss_giou_aux_4: 1.2553 (1.5864)  loss_fgl_aux_4: 0.8301 (0.5884)  loss_vfl_pre: 0.5059 (0.3959)  loss_bbox_pre: 0.2257 (0.5033)  loss_giou_pre: 1.2902 (1.6143)  loss_vfl_enc_0: 0.4385 (0.3775)  loss_bbox_enc_0: 0.2800 (0.5355)  loss_giou_enc_0: 1.4490 (1.6759)  loss_vfl_dn_0: 0.3577 (0.3918)  loss_bbox_dn_0: 0.1719 (0.2491)  loss_giou_dn_0: 1.2717 (1.3328)  loss_fgl_dn_0: 0.8729 (0.8313)  loss_vfl_dn_1: 0.3811 (0.4035)  loss_bbox_dn_1: 0.1611 (0.2463)  loss_giou_dn_1: 1.2584 (1.3237)  loss_fgl_dn_1: 0.8804 (0.8321)  loss_vfl_dn_2: 0.3789 (0.4066)  loss_bbox_dn_2: 0.1509 (0.2438)  loss_giou_dn_2: 1.2443 (1.3139)  loss_fgl_dn_2: 0.8931 (0.8363)  loss_vfl_dn_3: 0.3877 (0.4045)  loss_bbox_dn_3: 0.1469 (0.2430)  loss_giou_dn_3: 1.2372 (1.3111)  loss_fgl_dn_3: 0.8963 (0.8377)  loss_vfl_dn_4: 0.3933 (0.4032)  loss_bbox_dn_4: 0.1429 (0.2430)  loss_giou_dn_4: 1.2324 (1.3094)  loss_fgl_dn_4: 0.8972 (0.8389)  loss_vfl_dn_5: 0.3931 (0.4035)  loss_bbox_dn_5: 0.1425 (0.2430)  loss_giou_dn_5: 1.2315 (1.3089)  loss_fgl_dn_5: 0.8976 (0.8392)  loss_vfl_dn_pre: 0.3579 (0.3902)  loss_bbox_dn_pre: 0.1739 (0.2494)  loss_giou_dn_pre: 1.2693 (1.3367)  loss_ddf_aux_0: 0.1082 (0.0463)  loss_ddf_aux_1: 0.0485 (0.0233)  loss_ddf_aux_2: 0.0101 (0.0065)  loss_ddf_aux_3: 0.0019 (0.0018)  loss_ddf_aux_4: 0.0002 (0.0003)  loss_ddf_dn_0: 0.1563 (0.0771)  loss_ddf_dn_1: 0.0482 (0.0333)  loss_ddf_dn_2: 0.0105 (0.0086)  loss_ddf_dn_3: 0.0028 (0.0025)  loss_ddf_dn_4: 0.0003 (0.0005)  time: 0.5732  data: 0.0085  max mem: 3024\n",
            "Epoch: [0] Total time: 0:05:37 (0.6751 s / it)\n",
            "Averaged stats: lr: 0.000013  loss: 39.5884 (42.6929)  loss_vfl: 0.5928 (0.4667)  loss_bbox: 0.2356 (0.4827)  loss_giou: 1.2523 (1.5858)  loss_fgl: 0.8307 (0.5889)  loss_vfl_aux_0: 0.5063 (0.4006)  loss_bbox_aux_0: 0.2226 (0.4987)  loss_giou_aux_0: 1.2817 (1.6104)  loss_fgl_aux_0: 0.8344 (0.5748)  loss_vfl_aux_1: 0.5591 (0.4171)  loss_bbox_aux_1: 0.2140 (0.4915)  loss_giou_aux_1: 1.2751 (1.6025)  loss_fgl_aux_1: 0.8359 (0.5795)  loss_vfl_aux_2: 0.5522 (0.4353)  loss_bbox_aux_2: 0.2280 (0.4865)  loss_giou_aux_2: 1.2612 (1.5913)  loss_fgl_aux_2: 0.8310 (0.5852)  loss_vfl_aux_3: 0.5898 (0.4457)  loss_bbox_aux_3: 0.2379 (0.4841)  loss_giou_aux_3: 1.2629 (1.5883)  loss_fgl_aux_3: 0.8289 (0.5873)  loss_vfl_aux_4: 0.6025 (0.4581)  loss_bbox_aux_4: 0.2365 (0.4834)  loss_giou_aux_4: 1.2553 (1.5864)  loss_fgl_aux_4: 0.8301 (0.5884)  loss_vfl_pre: 0.5059 (0.3959)  loss_bbox_pre: 0.2257 (0.5033)  loss_giou_pre: 1.2902 (1.6143)  loss_vfl_enc_0: 0.4385 (0.3775)  loss_bbox_enc_0: 0.2800 (0.5355)  loss_giou_enc_0: 1.4490 (1.6759)  loss_vfl_dn_0: 0.3577 (0.3918)  loss_bbox_dn_0: 0.1719 (0.2491)  loss_giou_dn_0: 1.2717 (1.3328)  loss_fgl_dn_0: 0.8729 (0.8313)  loss_vfl_dn_1: 0.3811 (0.4035)  loss_bbox_dn_1: 0.1611 (0.2463)  loss_giou_dn_1: 1.2584 (1.3237)  loss_fgl_dn_1: 0.8804 (0.8321)  loss_vfl_dn_2: 0.3789 (0.4066)  loss_bbox_dn_2: 0.1509 (0.2438)  loss_giou_dn_2: 1.2443 (1.3139)  loss_fgl_dn_2: 0.8931 (0.8363)  loss_vfl_dn_3: 0.3877 (0.4045)  loss_bbox_dn_3: 0.1469 (0.2430)  loss_giou_dn_3: 1.2372 (1.3111)  loss_fgl_dn_3: 0.8963 (0.8377)  loss_vfl_dn_4: 0.3933 (0.4032)  loss_bbox_dn_4: 0.1429 (0.2430)  loss_giou_dn_4: 1.2324 (1.3094)  loss_fgl_dn_4: 0.8972 (0.8389)  loss_vfl_dn_5: 0.3931 (0.4035)  loss_bbox_dn_5: 0.1425 (0.2430)  loss_giou_dn_5: 1.2315 (1.3089)  loss_fgl_dn_5: 0.8976 (0.8392)  loss_vfl_dn_pre: 0.3579 (0.3902)  loss_bbox_dn_pre: 0.1739 (0.2494)  loss_giou_dn_pre: 1.2693 (1.3367)  loss_ddf_aux_0: 0.1082 (0.0463)  loss_ddf_aux_1: 0.0485 (0.0233)  loss_ddf_aux_2: 0.0101 (0.0065)  loss_ddf_aux_3: 0.0019 (0.0018)  loss_ddf_aux_4: 0.0002 (0.0003)  loss_ddf_dn_0: 0.1563 (0.0771)  loss_ddf_dn_1: 0.0482 (0.0333)  loss_ddf_dn_2: 0.0105 (0.0086)  loss_ddf_dn_3: 0.0028 (0.0025)  loss_ddf_dn_4: 0.0003 (0.0005)\n",
            "Test:  [  0/250]  eta: 0:10:43    time: 2.5748  data: 2.1661  max mem: 3024\n",
            "Test:  [ 10/250]  eta: 0:02:27    time: 0.6134  data: 0.4010  max mem: 3024\n",
            "Test:  [ 20/250]  eta: 0:02:10    time: 0.4677  data: 0.2824  max mem: 3024\n",
            "Test:  [ 30/250]  eta: 0:01:55    time: 0.4763  data: 0.2971  max mem: 3024\n",
            "Test:  [ 40/250]  eta: 0:01:48    time: 0.4650  data: 0.2579  max mem: 3024\n",
            "Test:  [ 50/250]  eta: 0:01:40    time: 0.4695  data: 0.2631  max mem: 3024\n",
            "Test:  [ 60/250]  eta: 0:01:34    time: 0.4533  data: 0.2733  max mem: 3024\n",
            "Test:  [ 70/250]  eta: 0:01:27    time: 0.4428  data: 0.2558  max mem: 3024\n",
            "Test:  [ 80/250]  eta: 0:01:21    time: 0.4208  data: 0.2351  max mem: 3024\n",
            "Test:  [ 90/250]  eta: 0:01:15    time: 0.4337  data: 0.2402  max mem: 3024\n",
            "Test:  [100/250]  eta: 0:01:10    time: 0.4437  data: 0.2387  max mem: 3024\n",
            "Test:  [110/250]  eta: 0:01:05    time: 0.4530  data: 0.2613  max mem: 3024\n",
            "Test:  [120/250]  eta: 0:01:00    time: 0.4240  data: 0.2429  max mem: 3024\n",
            "Test:  [130/250]  eta: 0:00:56    time: 0.4614  data: 0.2452  max mem: 3024\n",
            "Test:  [140/250]  eta: 0:00:50    time: 0.4584  data: 0.2411  max mem: 3024\n",
            "Test:  [150/250]  eta: 0:00:46    time: 0.4459  data: 0.2668  max mem: 3024\n",
            "Test:  [160/250]  eta: 0:00:41    time: 0.4512  data: 0.2717  max mem: 3024\n",
            "Test:  [170/250]  eta: 0:00:37    time: 0.4388  data: 0.2502  max mem: 3024\n",
            "Test:  [180/250]  eta: 0:00:32    time: 0.4547  data: 0.2667  max mem: 3024\n",
            "Test:  [190/250]  eta: 0:00:27    time: 0.4653  data: 0.2676  max mem: 3024\n",
            "Test:  [200/250]  eta: 0:00:23    time: 0.4563  data: 0.2521  max mem: 3024\n",
            "Test:  [210/250]  eta: 0:00:18    time: 0.4488  data: 0.2628  max mem: 3024\n",
            "Test:  [220/250]  eta: 0:00:13    time: 0.4424  data: 0.2623  max mem: 3024\n",
            "Test:  [230/250]  eta: 0:00:09    time: 0.4908  data: 0.3047  max mem: 3024\n",
            "Test:  [240/250]  eta: 0:00:04    time: 0.5048  data: 0.3023  max mem: 3024\n",
            "Test:  [249/250]  eta: 0:00:00    time: 0.4307  data: 0.2354  max mem: 3024\n",
            "Test: Total time: 0:01:55 (0.4602 s / it)\n",
            "Averaged stats: \n",
            "Accumulating evaluation results...\n",
            "COCOeval_opt.accumulate() finished...\n",
            "DONE (t=0.79s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.015\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.038\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.010\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.006\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.018\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.044\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.033\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.070\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.102\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.037\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.142\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.192\n",
            " Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.236\n",
            " Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.075\n",
            "best_stat: {'epoch': 0, 'coco_eval_bbox': 0.014831990492518267}\n",
            "Epoch: [1]  [  0/500]  eta: 0:19:19  lr: 0.000013  loss: 38.7660 (38.7660)  loss_vfl: 0.6108 (0.6108)  loss_bbox: 0.1476 (0.1476)  loss_giou: 1.1800 (1.1800)  loss_fgl: 0.9127 (0.9127)  loss_vfl_aux_0: 0.5996 (0.5996)  loss_bbox_aux_0: 0.1492 (0.1492)  loss_giou_aux_0: 1.2050 (1.2050)  loss_fgl_aux_0: 0.8803 (0.8803)  loss_ddf_aux_0: 0.0614 (0.0614)  loss_vfl_aux_1: 0.6567 (0.6567)  loss_bbox_aux_1: 0.1469 (0.1469)  loss_giou_aux_1: 1.1957 (1.1957)  loss_fgl_aux_1: 0.8905 (0.8905)  loss_ddf_aux_1: 0.0302 (0.0302)  loss_vfl_aux_2: 0.6055 (0.6055)  loss_bbox_aux_2: 0.1461 (0.1461)  loss_giou_aux_2: 1.1794 (1.1794)  loss_fgl_aux_2: 0.9062 (0.9062)  loss_ddf_aux_2: 0.0059 (0.0059)  loss_vfl_aux_3: 0.6045 (0.6045)  loss_bbox_aux_3: 0.1469 (0.1469)  loss_giou_aux_3: 1.1783 (1.1783)  loss_fgl_aux_3: 0.9105 (0.9105)  loss_ddf_aux_3: 0.0013 (0.0013)  loss_vfl_aux_4: 0.6050 (0.6050)  loss_bbox_aux_4: 0.1471 (0.1471)  loss_giou_aux_4: 1.1783 (1.1783)  loss_fgl_aux_4: 0.9131 (0.9131)  loss_ddf_aux_4: 0.0002 (0.0002)  loss_vfl_pre: 0.5903 (0.5903)  loss_bbox_pre: 0.1536 (0.1536)  loss_giou_pre: 1.2126 (1.2126)  loss_vfl_enc_0: 0.3887 (0.3887)  loss_bbox_enc_0: 0.2137 (0.2137)  loss_giou_enc_0: 1.3928 (1.3928)  loss_vfl_dn_0: 0.3613 (0.3613)  loss_bbox_dn_0: 0.1095 (0.1095)  loss_giou_dn_0: 1.2565 (1.2565)  loss_fgl_dn_0: 0.8709 (0.8709)  loss_ddf_dn_0: 0.1098 (0.1098)  loss_vfl_dn_1: 0.3970 (0.3970)  loss_bbox_dn_1: 0.1086 (0.1086)  loss_giou_dn_1: 1.2200 (1.2200)  loss_fgl_dn_1: 0.8902 (0.8902)  loss_ddf_dn_1: 0.0406 (0.0406)  loss_vfl_dn_2: 0.4126 (0.4126)  loss_bbox_dn_2: 0.1080 (0.1080)  loss_giou_dn_2: 1.1909 (1.1909)  loss_fgl_dn_2: 0.9063 (0.9063)  loss_ddf_dn_2: 0.0083 (0.0083)  loss_vfl_dn_3: 0.4246 (0.4246)  loss_bbox_dn_3: 0.1085 (0.1085)  loss_giou_dn_3: 1.1840 (1.1840)  loss_fgl_dn_3: 0.9117 (0.9117)  loss_ddf_dn_3: 0.0023 (0.0023)  loss_vfl_dn_4: 0.4316 (0.4316)  loss_bbox_dn_4: 0.1085 (0.1085)  loss_giou_dn_4: 1.1800 (1.1800)  loss_fgl_dn_4: 0.9149 (0.9149)  loss_ddf_dn_4: 0.0005 (0.0005)  loss_vfl_dn_5: 0.4290 (0.4290)  loss_bbox_dn_5: 0.1087 (0.1087)  loss_giou_dn_5: 1.1790 (1.1790)  loss_fgl_dn_5: 0.9161 (0.9161)  loss_vfl_dn_pre: 0.3579 (0.3579)  loss_bbox_dn_pre: 0.1091 (0.1091)  loss_giou_dn_pre: 1.2626 (1.2626)  time: 2.3197  data: 0.8054  max mem: 3024\n",
            "Epoch: [1]  [100/500]  eta: 0:04:22  lr: 0.000013  loss: 39.4569 (41.0529)  loss_vfl: 0.6206 (0.6881)  loss_bbox: 0.1709 (0.2538)  loss_giou: 1.1357 (1.1921)  loss_fgl: 0.8939 (0.8733)  loss_vfl_aux_0: 0.5225 (0.6002)  loss_bbox_aux_0: 0.1901 (0.2768)  loss_giou_aux_0: 1.2018 (1.2357)  loss_fgl_aux_0: 0.8641 (0.8531)  loss_ddf_aux_0: 0.1571 (0.1260)  loss_vfl_aux_1: 0.5942 (0.6413)  loss_bbox_aux_1: 0.1761 (0.2650)  loss_giou_aux_1: 1.1627 (1.2127)  loss_fgl_aux_1: 0.8809 (0.8618)  loss_ddf_aux_1: 0.0532 (0.0555)  loss_vfl_aux_2: 0.6548 (0.6613)  loss_bbox_aux_2: 0.1719 (0.2573)  loss_giou_aux_2: 1.1375 (1.1956)  loss_fgl_aux_2: 0.8867 (0.8702)  loss_ddf_aux_2: 0.0095 (0.0119)  loss_vfl_aux_3: 0.6367 (0.6720)  loss_bbox_aux_3: 0.1704 (0.2549)  loss_giou_aux_3: 1.1356 (1.1923)  loss_fgl_aux_3: 0.8914 (0.8724)  loss_ddf_aux_3: 0.0014 (0.0024)  loss_vfl_aux_4: 0.6255 (0.6793)  loss_bbox_aux_4: 0.1706 (0.2537)  loss_giou_aux_4: 1.1360 (1.1920)  loss_fgl_aux_4: 0.8924 (0.8731)  loss_ddf_aux_4: 0.0002 (0.0002)  loss_vfl_pre: 0.5132 (0.5948)  loss_bbox_pre: 0.1886 (0.2780)  loss_giou_pre: 1.2082 (1.2366)  loss_vfl_enc_0: 0.5010 (0.5146)  loss_bbox_enc_0: 0.2457 (0.3441)  loss_giou_enc_0: 1.3551 (1.3722)  loss_vfl_dn_0: 0.3838 (0.3823)  loss_bbox_dn_0: 0.1786 (0.2525)  loss_giou_dn_0: 1.1965 (1.2321)  loss_fgl_dn_0: 0.9073 (0.8956)  loss_ddf_dn_0: 0.1770 (0.1750)  loss_vfl_dn_1: 0.4121 (0.4071)  loss_bbox_dn_1: 0.1714 (0.2391)  loss_giou_dn_1: 1.1620 (1.1889)  loss_fgl_dn_1: 0.9195 (0.9116)  loss_ddf_dn_1: 0.0675 (0.0719)  loss_vfl_dn_2: 0.4229 (0.4196)  loss_bbox_dn_2: 0.1632 (0.2288)  loss_giou_dn_2: 1.1338 (1.1613)  loss_fgl_dn_2: 0.9295 (0.9249)  loss_ddf_dn_2: 0.0145 (0.0172)  loss_vfl_dn_3: 0.4243 (0.4261)  loss_bbox_dn_3: 0.1631 (0.2260)  loss_giou_dn_3: 1.1288 (1.1525)  loss_fgl_dn_3: 0.9312 (0.9295)  loss_ddf_dn_3: 0.0022 (0.0037)  loss_vfl_dn_4: 0.4304 (0.4291)  loss_bbox_dn_4: 0.1665 (0.2251)  loss_giou_dn_4: 1.1263 (1.1501)  loss_fgl_dn_4: 0.9335 (0.9312)  loss_ddf_dn_4: 0.0002 (0.0004)  loss_vfl_dn_5: 0.4253 (0.4298)  loss_bbox_dn_5: 0.1673 (0.2250)  loss_giou_dn_5: 1.1249 (1.1491)  loss_fgl_dn_5: 0.9342 (0.9319)  loss_vfl_dn_pre: 0.3777 (0.3766)  loss_bbox_dn_pre: 0.1772 (0.2499)  loss_giou_dn_pre: 1.2014 (1.2444)  time: 0.6822  data: 0.0092  max mem: 3024\n",
            "Epoch: [1]  [200/500]  eta: 0:03:10  lr: 0.000013  loss: 39.2327 (40.6873)  loss_vfl: 0.5376 (0.6759)  loss_bbox: 0.2010 (0.2421)  loss_giou: 1.2149 (1.1838)  loss_fgl: 0.8346 (0.8735)  loss_vfl_aux_0: 0.4619 (0.5866)  loss_bbox_aux_0: 0.2209 (0.2631)  loss_giou_aux_0: 1.2588 (1.2253)  loss_fgl_aux_0: 0.7958 (0.8554)  loss_ddf_aux_0: 0.1091 (0.1320)  loss_vfl_aux_1: 0.5312 (0.6257)  loss_bbox_aux_1: 0.2092 (0.2512)  loss_giou_aux_1: 1.2356 (1.2007)  loss_fgl_aux_1: 0.8169 (0.8642)  loss_ddf_aux_1: 0.0331 (0.0524)  loss_vfl_aux_2: 0.5503 (0.6478)  loss_bbox_aux_2: 0.2015 (0.2448)  loss_giou_aux_2: 1.2256 (1.1871)  loss_fgl_aux_2: 0.8265 (0.8707)  loss_ddf_aux_2: 0.0050 (0.0102)  loss_vfl_aux_3: 0.5347 (0.6565)  loss_bbox_aux_3: 0.2011 (0.2430)  loss_giou_aux_3: 1.2180 (1.1844)  loss_fgl_aux_3: 0.8344 (0.8726)  loss_ddf_aux_3: 0.0008 (0.0022)  loss_vfl_aux_4: 0.5366 (0.6652)  loss_bbox_aux_4: 0.2009 (0.2421)  loss_giou_aux_4: 1.2145 (1.1839)  loss_fgl_aux_4: 0.8345 (0.8733)  loss_ddf_aux_4: 0.0001 (0.0002)  loss_vfl_pre: 0.4590 (0.5818)  loss_bbox_pre: 0.2224 (0.2642)  loss_giou_pre: 1.2605 (1.2260)  loss_vfl_enc_0: 0.3926 (0.5083)  loss_bbox_enc_0: 0.3015 (0.3326)  loss_giou_enc_0: 1.4528 (1.3672)  loss_vfl_dn_0: 0.3875 (0.3840)  loss_bbox_dn_0: 0.2017 (0.2377)  loss_giou_dn_0: 1.1795 (1.2253)  loss_fgl_dn_0: 0.9072 (0.8998)  loss_ddf_dn_0: 0.1996 (0.2067)  loss_vfl_dn_1: 0.4126 (0.4088)  loss_bbox_dn_1: 0.1869 (0.2238)  loss_giou_dn_1: 1.1227 (1.1767)  loss_fgl_dn_1: 0.9348 (0.9172)  loss_ddf_dn_1: 0.0727 (0.0796)  loss_vfl_dn_2: 0.4255 (0.4190)  loss_bbox_dn_2: 0.1753 (0.2152)  loss_giou_dn_2: 1.1245 (1.1522)  loss_fgl_dn_2: 0.9489 (0.9289)  loss_ddf_dn_2: 0.0110 (0.0163)  loss_vfl_dn_3: 0.4241 (0.4233)  loss_bbox_dn_3: 0.1768 (0.2128)  loss_giou_dn_3: 1.1119 (1.1443)  loss_fgl_dn_3: 0.9576 (0.9331)  loss_ddf_dn_3: 0.0025 (0.0037)  loss_vfl_dn_4: 0.4243 (0.4262)  loss_bbox_dn_4: 0.1757 (0.2117)  loss_giou_dn_4: 1.1061 (1.1412)  loss_fgl_dn_4: 0.9607 (0.9350)  loss_ddf_dn_4: 0.0003 (0.0004)  loss_vfl_dn_5: 0.4331 (0.4290)  loss_bbox_dn_5: 0.1753 (0.2115)  loss_giou_dn_5: 1.1047 (1.1400)  loss_fgl_dn_5: 0.9601 (0.9357)  loss_vfl_dn_pre: 0.3843 (0.3781)  loss_bbox_dn_pre: 0.2030 (0.2371)  loss_giou_dn_pre: 1.1951 (1.2375)  time: 0.5489  data: 0.0072  max mem: 3024\n",
            "Epoch: [1]  [300/500]  eta: 0:02:09  lr: 0.000013  loss: 39.0410 (40.6078)  loss_vfl: 0.7520 (0.6887)  loss_bbox: 0.1449 (0.2328)  loss_giou: 0.9096 (1.1489)  loss_fgl: 1.0315 (0.8948)  loss_vfl_aux_0: 0.6514 (0.6054)  loss_bbox_aux_0: 0.1787 (0.2533)  loss_giou_aux_0: 0.9824 (1.1922)  loss_fgl_aux_0: 1.0268 (0.8768)  loss_ddf_aux_0: 0.1712 (0.1541)  loss_vfl_aux_1: 0.6851 (0.6443)  loss_bbox_aux_1: 0.1616 (0.2404)  loss_giou_aux_1: 0.9308 (1.1642)  loss_fgl_aux_1: 1.0282 (0.8858)  loss_ddf_aux_1: 0.0448 (0.0536)  loss_vfl_aux_2: 0.6953 (0.6651)  loss_bbox_aux_2: 0.1551 (0.2350)  loss_giou_aux_2: 0.9142 (1.1519)  loss_fgl_aux_2: 1.0305 (0.8919)  loss_ddf_aux_2: 0.0090 (0.0100)  loss_vfl_aux_3: 0.7134 (0.6701)  loss_bbox_aux_3: 0.1462 (0.2336)  loss_giou_aux_3: 0.9112 (1.1496)  loss_fgl_aux_3: 1.0305 (0.8938)  loss_ddf_aux_3: 0.0018 (0.0021)  loss_vfl_aux_4: 0.7368 (0.6771)  loss_bbox_aux_4: 0.1450 (0.2328)  loss_giou_aux_4: 0.9096 (1.1489)  loss_fgl_aux_4: 1.0312 (0.8946)  loss_ddf_aux_4: 0.0001 (0.0002)  loss_vfl_pre: 0.6440 (0.6002)  loss_bbox_pre: 0.1773 (0.2546)  loss_giou_pre: 0.9812 (1.1926)  loss_vfl_enc_0: 0.5718 (0.5192)  loss_bbox_enc_0: 0.2406 (0.3237)  loss_giou_enc_0: 1.1346 (1.3424)  loss_vfl_dn_0: 0.4175 (0.3914)  loss_bbox_dn_0: 0.2045 (0.2311)  loss_giou_dn_0: 1.1216 (1.2019)  loss_fgl_dn_0: 0.9677 (0.9156)  loss_ddf_dn_0: 0.3353 (0.2708)  loss_vfl_dn_1: 0.4431 (0.4188)  loss_bbox_dn_1: 0.1691 (0.2155)  loss_giou_dn_1: 1.0326 (1.1473)  loss_fgl_dn_1: 0.9815 (0.9344)  loss_ddf_dn_1: 0.0835 (0.0917)  loss_vfl_dn_2: 0.4446 (0.4269)  loss_bbox_dn_2: 0.1605 (0.2075)  loss_giou_dn_2: 1.0225 (1.1241)  loss_fgl_dn_2: 0.9930 (0.9453)  loss_ddf_dn_2: 0.0184 (0.0186)  loss_vfl_dn_3: 0.4451 (0.4311)  loss_bbox_dn_3: 0.1572 (0.2046)  loss_giou_dn_3: 1.0160 (1.1158)  loss_fgl_dn_3: 1.0017 (0.9495)  loss_ddf_dn_3: 0.0041 (0.0044)  loss_vfl_dn_4: 0.4509 (0.4339)  loss_bbox_dn_4: 0.1555 (0.2033)  loss_giou_dn_4: 1.0178 (1.1125)  loss_fgl_dn_4: 1.0065 (0.9515)  loss_ddf_dn_4: 0.0006 (0.0005)  loss_vfl_dn_5: 0.4626 (0.4374)  loss_bbox_dn_5: 0.1545 (0.2030)  loss_giou_dn_5: 1.0174 (1.1113)  loss_fgl_dn_5: 1.0080 (0.9523)  loss_vfl_dn_pre: 0.4092 (0.3855)  loss_bbox_dn_pre: 0.1968 (0.2314)  loss_giou_dn_pre: 1.1352 (1.2142)  time: 0.6757  data: 0.0094  max mem: 3024\n",
            "Epoch: [1]  [400/500]  eta: 0:01:04  lr: 0.000013  loss: 39.2574 (40.4369)  loss_vfl: 0.6709 (0.6840)  loss_bbox: 0.1888 (0.2263)  loss_giou: 1.0169 (1.1344)  loss_fgl: 0.9510 (0.8993)  loss_vfl_aux_0: 0.5923 (0.6066)  loss_bbox_aux_0: 0.1954 (0.2475)  loss_giou_aux_0: 1.0596 (1.1787)  loss_fgl_aux_0: 0.9439 (0.8829)  loss_ddf_aux_0: 0.1876 (0.1669)  loss_vfl_aux_1: 0.6416 (0.6425)  loss_bbox_aux_1: 0.1951 (0.2337)  loss_giou_aux_1: 1.0313 (1.1489)  loss_fgl_aux_1: 0.9550 (0.8913)  loss_ddf_aux_1: 0.0420 (0.0524)  loss_vfl_aux_2: 0.6016 (0.6628)  loss_bbox_aux_2: 0.1916 (0.2287)  loss_giou_aux_2: 1.0230 (1.1376)  loss_fgl_aux_2: 0.9488 (0.8967)  loss_ddf_aux_2: 0.0085 (0.0096)  loss_vfl_aux_3: 0.6621 (0.6662)  loss_bbox_aux_3: 0.1895 (0.2273)  loss_giou_aux_3: 1.0194 (1.1352)  loss_fgl_aux_3: 0.9486 (0.8984)  loss_ddf_aux_3: 0.0020 (0.0020)  loss_vfl_aux_4: 0.6548 (0.6730)  loss_bbox_aux_4: 0.1890 (0.2264)  loss_giou_aux_4: 1.0174 (1.1345)  loss_fgl_aux_4: 0.9503 (0.8991)  loss_ddf_aux_4: 0.0001 (0.0002)  loss_vfl_pre: 0.5942 (0.6015)  loss_bbox_pre: 0.1965 (0.2486)  loss_giou_pre: 1.0516 (1.1789)  loss_vfl_enc_0: 0.5254 (0.5244)  loss_bbox_enc_0: 0.2512 (0.3181)  loss_giou_enc_0: 1.2014 (1.3306)  loss_vfl_dn_0: 0.4092 (0.3941)  loss_bbox_dn_0: 0.1726 (0.2252)  loss_giou_dn_0: 1.1505 (1.1903)  loss_fgl_dn_0: 0.9455 (0.9231)  loss_ddf_dn_0: 0.3244 (0.3022)  loss_vfl_dn_1: 0.4224 (0.4224)  loss_bbox_dn_1: 0.1655 (0.2089)  loss_giou_dn_1: 1.0834 (1.1317)  loss_fgl_dn_1: 0.9468 (0.9426)  loss_ddf_dn_1: 0.0816 (0.0950)  loss_vfl_dn_2: 0.4312 (0.4300)  loss_bbox_dn_2: 0.1553 (0.2012)  loss_giou_dn_2: 1.0551 (1.1092)  loss_fgl_dn_2: 0.9552 (0.9531)  loss_ddf_dn_2: 0.0215 (0.0200)  loss_vfl_dn_3: 0.4307 (0.4332)  loss_bbox_dn_3: 0.1481 (0.1982)  loss_giou_dn_3: 1.0334 (1.1006)  loss_fgl_dn_3: 0.9640 (0.9572)  loss_ddf_dn_3: 0.0050 (0.0045)  loss_vfl_dn_4: 0.4309 (0.4363)  loss_bbox_dn_4: 0.1462 (0.1970)  loss_giou_dn_4: 1.0205 (1.0972)  loss_fgl_dn_4: 0.9706 (0.9593)  loss_ddf_dn_4: 0.0008 (0.0005)  loss_vfl_dn_5: 0.4314 (0.4398)  loss_bbox_dn_5: 0.1465 (0.1967)  loss_giou_dn_5: 1.0182 (1.0959)  loss_fgl_dn_5: 0.9719 (0.9601)  loss_vfl_dn_pre: 0.3982 (0.3887)  loss_bbox_dn_pre: 0.1761 (0.2261)  loss_giou_dn_pre: 1.1464 (1.2015)  time: 0.6701  data: 0.0077  max mem: 3024\n",
            "Epoch: [1]  [499/500]  eta: 0:00:00  lr: 0.000013  loss: 38.8131 (40.3186)  loss_vfl: 0.6787 (0.6857)  loss_bbox: 0.1759 (0.2215)  loss_giou: 0.9896 (1.1186)  loss_fgl: 0.9857 (0.9070)  loss_vfl_aux_0: 0.6050 (0.6132)  loss_bbox_aux_0: 0.1805 (0.2417)  loss_giou_aux_0: 1.0170 (1.1633)  loss_fgl_aux_0: 0.9905 (0.8916)  loss_ddf_aux_0: 0.1677 (0.1741)  loss_vfl_aux_1: 0.6406 (0.6492)  loss_bbox_aux_1: 0.1811 (0.2282)  loss_giou_aux_1: 0.9927 (1.1323)  loss_fgl_aux_1: 0.9881 (0.8995)  loss_ddf_aux_1: 0.0317 (0.0505)  loss_vfl_aux_2: 0.6465 (0.6682)  loss_bbox_aux_2: 0.1782 (0.2236)  loss_giou_aux_2: 0.9898 (1.1217)  loss_fgl_aux_2: 0.9831 (0.9044)  loss_ddf_aux_2: 0.0053 (0.0093)  loss_vfl_aux_3: 0.6660 (0.6691)  loss_bbox_aux_3: 0.1757 (0.2223)  loss_giou_aux_3: 0.9895 (1.1193)  loss_fgl_aux_3: 0.9836 (0.9061)  loss_ddf_aux_3: 0.0008 (0.0019)  loss_vfl_aux_4: 0.6787 (0.6756)  loss_bbox_aux_4: 0.1759 (0.2215)  loss_giou_aux_4: 0.9897 (1.1186)  loss_fgl_aux_4: 0.9848 (0.9068)  loss_ddf_aux_4: 0.0001 (0.0002)  loss_vfl_pre: 0.5938 (0.6087)  loss_bbox_pre: 0.1842 (0.2428)  loss_giou_pre: 1.0199 (1.1632)  loss_vfl_enc_0: 0.5371 (0.5314)  loss_bbox_enc_0: 0.2354 (0.3121)  loss_giou_enc_0: 1.1995 (1.3167)  loss_vfl_dn_0: 0.4070 (0.3977)  loss_bbox_dn_0: 0.1534 (0.2182)  loss_giou_dn_0: 1.1404 (1.1767)  loss_fgl_dn_0: 0.9485 (0.9322)  loss_ddf_dn_0: 0.3215 (0.3312)  loss_vfl_dn_1: 0.4343 (0.4261)  loss_bbox_dn_1: 0.1513 (0.2016)  loss_giou_dn_1: 1.0632 (1.1163)  loss_fgl_dn_1: 0.9517 (0.9519)  loss_ddf_dn_1: 0.0727 (0.0970)  loss_vfl_dn_2: 0.4324 (0.4321)  loss_bbox_dn_2: 0.1537 (0.1944)  loss_giou_dn_2: 1.0767 (1.0947)  loss_fgl_dn_2: 0.9667 (0.9618)  loss_ddf_dn_2: 0.0166 (0.0209)  loss_vfl_dn_3: 0.4395 (0.4352)  loss_bbox_dn_3: 0.1539 (0.1916)  loss_giou_dn_3: 1.0736 (1.0863)  loss_fgl_dn_3: 0.9722 (0.9659)  loss_ddf_dn_3: 0.0030 (0.0046)  loss_vfl_dn_4: 0.4290 (0.4382)  loss_bbox_dn_4: 0.1552 (0.1904)  loss_giou_dn_4: 1.0747 (1.0832)  loss_fgl_dn_4: 0.9729 (0.9678)  loss_ddf_dn_4: 0.0005 (0.0005)  loss_vfl_dn_5: 0.4238 (0.4418)  loss_bbox_dn_5: 0.1553 (0.1901)  loss_giou_dn_5: 1.0750 (1.0820)  loss_fgl_dn_5: 0.9731 (0.9685)  loss_vfl_dn_pre: 0.4050 (0.3928)  loss_bbox_dn_pre: 0.1514 (0.2198)  loss_giou_dn_pre: 1.1365 (1.1870)  time: 0.5221  data: 0.0071  max mem: 3024\n",
            "Epoch: [1] Total time: 0:05:20 (0.6409 s / it)\n",
            "Averaged stats: lr: 0.000013  loss: 38.8131 (40.3186)  loss_vfl: 0.6787 (0.6857)  loss_bbox: 0.1759 (0.2215)  loss_giou: 0.9896 (1.1186)  loss_fgl: 0.9857 (0.9070)  loss_vfl_aux_0: 0.6050 (0.6132)  loss_bbox_aux_0: 0.1805 (0.2417)  loss_giou_aux_0: 1.0170 (1.1633)  loss_fgl_aux_0: 0.9905 (0.8916)  loss_ddf_aux_0: 0.1677 (0.1741)  loss_vfl_aux_1: 0.6406 (0.6492)  loss_bbox_aux_1: 0.1811 (0.2282)  loss_giou_aux_1: 0.9927 (1.1323)  loss_fgl_aux_1: 0.9881 (0.8995)  loss_ddf_aux_1: 0.0317 (0.0505)  loss_vfl_aux_2: 0.6465 (0.6682)  loss_bbox_aux_2: 0.1782 (0.2236)  loss_giou_aux_2: 0.9898 (1.1217)  loss_fgl_aux_2: 0.9831 (0.9044)  loss_ddf_aux_2: 0.0053 (0.0093)  loss_vfl_aux_3: 0.6660 (0.6691)  loss_bbox_aux_3: 0.1757 (0.2223)  loss_giou_aux_3: 0.9895 (1.1193)  loss_fgl_aux_3: 0.9836 (0.9061)  loss_ddf_aux_3: 0.0008 (0.0019)  loss_vfl_aux_4: 0.6787 (0.6756)  loss_bbox_aux_4: 0.1759 (0.2215)  loss_giou_aux_4: 0.9897 (1.1186)  loss_fgl_aux_4: 0.9848 (0.9068)  loss_ddf_aux_4: 0.0001 (0.0002)  loss_vfl_pre: 0.5938 (0.6087)  loss_bbox_pre: 0.1842 (0.2428)  loss_giou_pre: 1.0199 (1.1632)  loss_vfl_enc_0: 0.5371 (0.5314)  loss_bbox_enc_0: 0.2354 (0.3121)  loss_giou_enc_0: 1.1995 (1.3167)  loss_vfl_dn_0: 0.4070 (0.3977)  loss_bbox_dn_0: 0.1534 (0.2182)  loss_giou_dn_0: 1.1404 (1.1767)  loss_fgl_dn_0: 0.9485 (0.9322)  loss_ddf_dn_0: 0.3215 (0.3312)  loss_vfl_dn_1: 0.4343 (0.4261)  loss_bbox_dn_1: 0.1513 (0.2016)  loss_giou_dn_1: 1.0632 (1.1163)  loss_fgl_dn_1: 0.9517 (0.9519)  loss_ddf_dn_1: 0.0727 (0.0970)  loss_vfl_dn_2: 0.4324 (0.4321)  loss_bbox_dn_2: 0.1537 (0.1944)  loss_giou_dn_2: 1.0767 (1.0947)  loss_fgl_dn_2: 0.9667 (0.9618)  loss_ddf_dn_2: 0.0166 (0.0209)  loss_vfl_dn_3: 0.4395 (0.4352)  loss_bbox_dn_3: 0.1539 (0.1916)  loss_giou_dn_3: 1.0736 (1.0863)  loss_fgl_dn_3: 0.9722 (0.9659)  loss_ddf_dn_3: 0.0030 (0.0046)  loss_vfl_dn_4: 0.4290 (0.4382)  loss_bbox_dn_4: 0.1552 (0.1904)  loss_giou_dn_4: 1.0747 (1.0832)  loss_fgl_dn_4: 0.9729 (0.9678)  loss_ddf_dn_4: 0.0005 (0.0005)  loss_vfl_dn_5: 0.4238 (0.4418)  loss_bbox_dn_5: 0.1553 (0.1901)  loss_giou_dn_5: 1.0750 (1.0820)  loss_fgl_dn_5: 0.9731 (0.9685)  loss_vfl_dn_pre: 0.4050 (0.3928)  loss_bbox_dn_pre: 0.1514 (0.2198)  loss_giou_dn_pre: 1.1365 (1.1870)\n",
            "Test:  [  0/250]  eta: 0:05:38    time: 1.3554  data: 1.0770  max mem: 3024\n",
            "Test:  [ 10/250]  eta: 0:01:12    time: 0.3006  data: 0.1138  max mem: 3024\n",
            "Test:  [ 20/250]  eta: 0:00:57    time: 0.1958  data: 0.0184  max mem: 3024\n",
            "Test:  [ 30/250]  eta: 0:00:51    time: 0.1961  data: 0.0193  max mem: 3024\n",
            "Test:  [ 40/250]  eta: 0:00:49    time: 0.2227  data: 0.0196  max mem: 3024\n",
            "Test:  [ 50/250]  eta: 0:00:48    time: 0.2529  data: 0.0326  max mem: 3024\n",
            "Test:  [ 60/250]  eta: 0:00:46    time: 0.2640  data: 0.0417  max mem: 3024\n",
            "Test:  [ 70/250]  eta: 0:00:44    time: 0.2553  data: 0.0374  max mem: 3024\n",
            "Test:  [ 80/250]  eta: 0:00:40    time: 0.2183  data: 0.0289  max mem: 3024\n",
            "Test:  [ 90/250]  eta: 0:00:37    time: 0.1977  data: 0.0207  max mem: 3024\n",
            "Test:  [100/250]  eta: 0:00:34    time: 0.1966  data: 0.0195  max mem: 3024\n",
            "Test:  [110/250]  eta: 0:00:32    time: 0.2121  data: 0.0193  max mem: 3024\n",
            "Test:  [120/250]  eta: 0:00:29    time: 0.2230  data: 0.0245  max mem: 3024\n",
            "Test:  [130/250]  eta: 0:00:27    time: 0.2431  data: 0.0366  max mem: 3024\n",
            "Test:  [140/250]  eta: 0:00:25    time: 0.2777  data: 0.0455  max mem: 3024\n",
            "Test:  [150/250]  eta: 0:00:23    time: 0.2469  data: 0.0352  max mem: 3024\n",
            "Test:  [160/250]  eta: 0:00:20    time: 0.2019  data: 0.0217  max mem: 3024\n",
            "Test:  [170/250]  eta: 0:00:18    time: 0.1985  data: 0.0202  max mem: 3024\n",
            "Test:  [180/250]  eta: 0:00:16    time: 0.2136  data: 0.0208  max mem: 3024\n",
            "Test:  [190/250]  eta: 0:00:13    time: 0.2118  data: 0.0202  max mem: 3024\n",
            "Test:  [200/250]  eta: 0:00:11    time: 0.2287  data: 0.0312  max mem: 3024\n",
            "Test:  [210/250]  eta: 0:00:09    time: 0.2607  data: 0.0427  max mem: 3024\n",
            "Test:  [220/250]  eta: 0:00:06    time: 0.2625  data: 0.0414  max mem: 3024\n",
            "Test:  [230/250]  eta: 0:00:04    time: 0.2319  data: 0.0306  max mem: 3024\n",
            "Test:  [240/250]  eta: 0:00:02    time: 0.2126  data: 0.0201  max mem: 3024\n",
            "Test:  [249/250]  eta: 0:00:00    time: 0.2093  data: 0.0185  max mem: 3024\n",
            "Test: Total time: 0:00:57 (0.2305 s / it)\n",
            "Averaged stats: \n",
            "Accumulating evaluation results...\n",
            "COCOeval_opt.accumulate() finished...\n",
            "DONE (t=0.82s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.041\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.084\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.037\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.015\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.051\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.136\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.067\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.146\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.199\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.076\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.271\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.416\n",
            " Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.388\n",
            " Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.177\n",
            "best_stat: {'epoch': 1, 'coco_eval_bbox': 0.041012492281609414}\n",
            "Epoch: [2]  [  0/500]  eta: 0:33:57  lr: 0.000013  loss: 40.4145 (40.4145)  loss_vfl: 0.6763 (0.6763)  loss_bbox: 0.2568 (0.2568)  loss_giou: 0.9771 (0.9771)  loss_fgl: 0.9966 (0.9966)  loss_vfl_aux_0: 0.7026 (0.7026)  loss_bbox_aux_0: 0.2933 (0.2933)  loss_giou_aux_0: 1.0291 (1.0291)  loss_fgl_aux_0: 0.9714 (0.9714)  loss_ddf_aux_0: 0.2617 (0.2617)  loss_vfl_aux_1: 0.7236 (0.7236)  loss_bbox_aux_1: 0.2597 (0.2597)  loss_giou_aux_1: 0.9847 (0.9847)  loss_fgl_aux_1: 0.9811 (0.9811)  loss_ddf_aux_1: 0.0494 (0.0494)  loss_vfl_aux_2: 0.7354 (0.7354)  loss_bbox_aux_2: 0.2578 (0.2578)  loss_giou_aux_2: 0.9764 (0.9764)  loss_fgl_aux_2: 0.9892 (0.9892)  loss_ddf_aux_2: 0.0103 (0.0103)  loss_vfl_aux_3: 0.6689 (0.6689)  loss_bbox_aux_3: 0.2576 (0.2576)  loss_giou_aux_3: 0.9771 (0.9771)  loss_fgl_aux_3: 0.9950 (0.9950)  loss_ddf_aux_3: 0.0019 (0.0019)  loss_vfl_aux_4: 0.6533 (0.6533)  loss_bbox_aux_4: 0.2570 (0.2570)  loss_giou_aux_4: 0.9774 (0.9774)  loss_fgl_aux_4: 0.9967 (0.9967)  loss_ddf_aux_4: 0.0003 (0.0003)  loss_vfl_pre: 0.6982 (0.6982)  loss_bbox_pre: 0.2942 (0.2942)  loss_giou_pre: 1.0282 (1.0282)  loss_vfl_enc_0: 0.5806 (0.5806)  loss_bbox_enc_0: 0.4171 (0.4171)  loss_giou_enc_0: 1.2037 (1.2037)  loss_vfl_dn_0: 0.4146 (0.4146)  loss_bbox_dn_0: 0.2541 (0.2541)  loss_giou_dn_0: 1.0829 (1.0829)  loss_fgl_dn_0: 0.9779 (0.9779)  loss_ddf_dn_0: 0.5067 (0.5067)  loss_vfl_dn_1: 0.4556 (0.4556)  loss_bbox_dn_1: 0.2160 (0.2160)  loss_giou_dn_1: 0.9723 (0.9723)  loss_fgl_dn_1: 1.0101 (1.0101)  loss_ddf_dn_1: 0.1338 (0.1338)  loss_vfl_dn_2: 0.4731 (0.4731)  loss_bbox_dn_2: 0.1973 (0.1973)  loss_giou_dn_2: 0.9418 (0.9418)  loss_fgl_dn_2: 1.0229 (1.0229)  loss_ddf_dn_2: 0.0322 (0.0322)  loss_vfl_dn_3: 0.4585 (0.4585)  loss_bbox_dn_3: 0.1909 (0.1909)  loss_giou_dn_3: 0.9314 (0.9314)  loss_fgl_dn_3: 1.0281 (1.0281)  loss_ddf_dn_3: 0.0059 (0.0059)  loss_vfl_dn_4: 0.4609 (0.4609)  loss_bbox_dn_4: 0.1865 (0.1865)  loss_giou_dn_4: 0.9238 (0.9238)  loss_fgl_dn_4: 1.0302 (1.0302)  loss_ddf_dn_4: 0.0007 (0.0007)  loss_vfl_dn_5: 0.4744 (0.4744)  loss_bbox_dn_5: 0.1847 (0.1847)  loss_giou_dn_5: 0.9200 (0.9200)  loss_fgl_dn_5: 1.0317 (1.0317)  loss_vfl_dn_pre: 0.4138 (0.4138)  loss_bbox_dn_pre: 0.2520 (0.2520)  loss_giou_dn_pre: 1.0896 (1.0896)  time: 4.0744  data: 1.4432  max mem: 3024\n",
            "Epoch: [2]  [100/500]  eta: 0:04:27  lr: 0.000013  loss: 39.8950 (39.4829)  loss_vfl: 0.6606 (0.6710)  loss_bbox: 0.1718 (0.1820)  loss_giou: 0.9543 (1.0378)  loss_fgl: 0.9697 (0.9470)  loss_vfl_aux_0: 0.6289 (0.6165)  loss_bbox_aux_0: 0.1882 (0.2050)  loss_giou_aux_0: 1.0318 (1.0846)  loss_fgl_aux_0: 0.9687 (0.9368)  loss_ddf_aux_0: 0.2504 (0.2273)  loss_vfl_aux_1: 0.6533 (0.6551)  loss_bbox_aux_1: 0.1727 (0.1865)  loss_giou_aux_1: 0.9650 (1.0460)  loss_fgl_aux_1: 0.9809 (0.9424)  loss_ddf_aux_1: 0.0422 (0.0416)  loss_vfl_aux_2: 0.6646 (0.6633)  loss_bbox_aux_2: 0.1717 (0.1831)  loss_giou_aux_2: 0.9532 (1.0394)  loss_fgl_aux_2: 0.9748 (0.9451)  loss_ddf_aux_2: 0.0080 (0.0073)  loss_vfl_aux_3: 0.6606 (0.6637)  loss_bbox_aux_3: 0.1717 (0.1824)  loss_giou_aux_3: 0.9543 (1.0381)  loss_fgl_aux_3: 0.9717 (0.9463)  loss_ddf_aux_3: 0.0014 (0.0013)  loss_vfl_aux_4: 0.6597 (0.6653)  loss_bbox_aux_4: 0.1718 (0.1820)  loss_giou_aux_4: 0.9539 (1.0379)  loss_fgl_aux_4: 0.9699 (0.9468)  loss_ddf_aux_4: 0.0001 (0.0001)  loss_vfl_pre: 0.6401 (0.6121)  loss_bbox_pre: 0.1918 (0.2059)  loss_giou_pre: 1.0308 (1.0847)  loss_vfl_enc_0: 0.5151 (0.5505)  loss_bbox_enc_0: 0.2289 (0.2722)  loss_giou_enc_0: 1.1558 (1.2334)  loss_vfl_dn_0: 0.4236 (0.4074)  loss_bbox_dn_0: 0.1817 (0.1892)  loss_giou_dn_0: 1.0876 (1.1249)  loss_fgl_dn_0: 0.9735 (0.9681)  loss_ddf_dn_0: 0.5708 (0.5152)  loss_vfl_dn_1: 0.4597 (0.4404)  loss_bbox_dn_1: 0.1570 (0.1696)  loss_giou_dn_1: 1.0137 (1.0509)  loss_fgl_dn_1: 0.9900 (0.9876)  loss_ddf_dn_1: 0.1372 (0.1188)  loss_vfl_dn_2: 0.4482 (0.4429)  loss_bbox_dn_2: 0.1456 (0.1629)  loss_giou_dn_2: 0.9937 (1.0314)  loss_fgl_dn_2: 0.9970 (0.9953)  loss_ddf_dn_2: 0.0342 (0.0282)  loss_vfl_dn_3: 0.4565 (0.4440)  loss_bbox_dn_3: 0.1427 (0.1599)  loss_giou_dn_3: 0.9894 (1.0234)  loss_fgl_dn_3: 1.0053 (0.9988)  loss_ddf_dn_3: 0.0068 (0.0052)  loss_vfl_dn_4: 0.4585 (0.4455)  loss_bbox_dn_4: 0.1415 (0.1588)  loss_giou_dn_4: 0.9841 (1.0206)  loss_fgl_dn_4: 1.0058 (1.0001)  loss_ddf_dn_4: 0.0007 (0.0006)  loss_vfl_dn_5: 0.4585 (0.4479)  loss_bbox_dn_5: 0.1418 (0.1584)  loss_giou_dn_5: 0.9825 (1.0197)  loss_fgl_dn_5: 1.0058 (1.0006)  loss_vfl_dn_pre: 0.4241 (0.4063)  loss_bbox_dn_pre: 0.1811 (0.1924)  loss_giou_dn_pre: 1.0876 (1.1276)  time: 0.5530  data: 0.0075  max mem: 3024\n",
            "Epoch: [2]  [200/500]  eta: 0:03:16  lr: 0.000013  loss: 39.1386 (39.5247)  loss_vfl: 0.6416 (0.6745)  loss_bbox: 0.1638 (0.1799)  loss_giou: 1.0354 (1.0349)  loss_fgl: 0.9577 (0.9486)  loss_vfl_aux_0: 0.5674 (0.6270)  loss_bbox_aux_0: 0.1832 (0.2020)  loss_giou_aux_0: 1.0811 (1.0855)  loss_fgl_aux_0: 0.9508 (0.9392)  loss_ddf_aux_0: 0.2538 (0.2370)  loss_vfl_aux_1: 0.6099 (0.6550)  loss_bbox_aux_1: 0.1681 (0.1848)  loss_giou_aux_1: 1.0481 (1.0450)  loss_fgl_aux_1: 0.9574 (0.9447)  loss_ddf_aux_1: 0.0481 (0.0443)  loss_vfl_aux_2: 0.6255 (0.6666)  loss_bbox_aux_2: 0.1667 (0.1813)  loss_giou_aux_2: 1.0385 (1.0371)  loss_fgl_aux_2: 0.9562 (0.9470)  loss_ddf_aux_2: 0.0074 (0.0079)  loss_vfl_aux_3: 0.6274 (0.6662)  loss_bbox_aux_3: 0.1636 (0.1803)  loss_giou_aux_3: 1.0357 (1.0353)  loss_fgl_aux_3: 0.9578 (0.9481)  loss_ddf_aux_3: 0.0011 (0.0013)  loss_vfl_aux_4: 0.6255 (0.6693)  loss_bbox_aux_4: 0.1637 (0.1799)  loss_giou_aux_4: 1.0353 (1.0349)  loss_fgl_aux_4: 0.9577 (0.9484)  loss_ddf_aux_4: 0.0001 (0.0001)  loss_vfl_pre: 0.5684 (0.6219)  loss_bbox_pre: 0.1833 (0.2033)  loss_giou_pre: 1.0991 (1.0865)  loss_vfl_enc_0: 0.5220 (0.5539)  loss_bbox_enc_0: 0.2353 (0.2715)  loss_giou_enc_0: 1.2503 (1.2405)  loss_vfl_dn_0: 0.4128 (0.4096)  loss_bbox_dn_0: 0.1579 (0.1846)  loss_giou_dn_0: 1.0918 (1.1208)  loss_fgl_dn_0: 0.9627 (0.9697)  loss_ddf_dn_0: 0.5882 (0.5537)  loss_vfl_dn_1: 0.4443 (0.4439)  loss_bbox_dn_1: 0.1350 (0.1643)  loss_giou_dn_1: 0.9807 (1.0429)  loss_fgl_dn_1: 0.9978 (0.9916)  loss_ddf_dn_1: 0.1237 (0.1213)  loss_vfl_dn_2: 0.4460 (0.4469)  loss_bbox_dn_2: 0.1250 (0.1572)  loss_giou_dn_2: 0.9570 (1.0230)  loss_fgl_dn_2: 1.0032 (0.9991)  loss_ddf_dn_2: 0.0241 (0.0280)  loss_vfl_dn_3: 0.4421 (0.4481)  loss_bbox_dn_3: 0.1255 (0.1543)  loss_giou_dn_3: 0.9476 (1.0147)  loss_fgl_dn_3: 1.0058 (1.0025)  loss_ddf_dn_3: 0.0047 (0.0052)  loss_vfl_dn_4: 0.4453 (0.4491)  loss_bbox_dn_4: 0.1252 (0.1533)  loss_giou_dn_4: 0.9432 (1.0117)  loss_fgl_dn_4: 1.0067 (1.0038)  loss_ddf_dn_4: 0.0007 (0.0006)  loss_vfl_dn_5: 0.4465 (0.4522)  loss_bbox_dn_5: 0.1256 (0.1530)  loss_giou_dn_5: 0.9414 (1.0107)  loss_fgl_dn_5: 1.0068 (1.0043)  loss_vfl_dn_pre: 0.4062 (0.4076)  loss_bbox_dn_pre: 0.1582 (0.1881)  loss_giou_dn_pre: 1.1068 (1.1253)  time: 0.6509  data: 0.0086  max mem: 3024\n",
            "Epoch: [2]  [300/500]  eta: 0:02:09  lr: 0.000013  loss: 38.8876 (39.4966)  loss_vfl: 0.6099 (0.6742)  loss_bbox: 0.1510 (0.1784)  loss_giou: 1.0711 (1.0345)  loss_fgl: 0.9408 (0.9521)  loss_vfl_aux_0: 0.5737 (0.6303)  loss_bbox_aux_0: 0.1608 (0.1992)  loss_giou_aux_0: 1.0356 (1.0819)  loss_fgl_aux_0: 0.9464 (0.9437)  loss_ddf_aux_0: 0.2169 (0.2342)  loss_vfl_aux_1: 0.5928 (0.6561)  loss_bbox_aux_1: 0.1528 (0.1829)  loss_giou_aux_1: 1.0523 (1.0439)  loss_fgl_aux_1: 0.9469 (0.9484)  loss_ddf_aux_1: 0.0303 (0.0441)  loss_vfl_aux_2: 0.5962 (0.6677)  loss_bbox_aux_2: 0.1517 (0.1796)  loss_giou_aux_2: 1.0623 (1.0364)  loss_fgl_aux_2: 0.9433 (0.9506)  loss_ddf_aux_2: 0.0048 (0.0075)  loss_vfl_aux_3: 0.6099 (0.6682)  loss_bbox_aux_3: 0.1510 (0.1788)  loss_giou_aux_3: 1.0664 (1.0348)  loss_fgl_aux_3: 0.9421 (0.9517)  loss_ddf_aux_3: 0.0009 (0.0012)  loss_vfl_aux_4: 0.6045 (0.6696)  loss_bbox_aux_4: 0.1510 (0.1784)  loss_giou_aux_4: 1.0703 (1.0345)  loss_fgl_aux_4: 0.9411 (0.9520)  loss_ddf_aux_4: 0.0001 (0.0001)  loss_vfl_pre: 0.5703 (0.6258)  loss_bbox_pre: 0.1605 (0.2007)  loss_giou_pre: 1.0407 (1.0826)  loss_vfl_enc_0: 0.5127 (0.5587)  loss_bbox_enc_0: 0.2018 (0.2680)  loss_giou_enc_0: 1.1971 (1.2343)  loss_vfl_dn_0: 0.4045 (0.4096)  loss_bbox_dn_0: 0.1456 (0.1844)  loss_giou_dn_0: 1.0815 (1.1183)  loss_fgl_dn_0: 0.9851 (0.9722)  loss_ddf_dn_0: 0.4967 (0.5460)  loss_vfl_dn_1: 0.4363 (0.4430)  loss_bbox_dn_1: 0.1160 (0.1634)  loss_giou_dn_1: 0.9876 (1.0406)  loss_fgl_dn_1: 1.0086 (0.9941)  loss_ddf_dn_1: 0.0889 (0.1169)  loss_vfl_dn_2: 0.4407 (0.4460)  loss_bbox_dn_2: 0.1084 (0.1562)  loss_giou_dn_2: 0.9711 (1.0203)  loss_fgl_dn_2: 1.0141 (1.0014)  loss_ddf_dn_2: 0.0178 (0.0257)  loss_vfl_dn_3: 0.4368 (0.4466)  loss_bbox_dn_3: 0.1086 (0.1537)  loss_giou_dn_3: 0.9714 (1.0126)  loss_fgl_dn_3: 1.0191 (1.0045)  loss_ddf_dn_3: 0.0037 (0.0048)  loss_vfl_dn_4: 0.4338 (0.4474)  loss_bbox_dn_4: 0.1083 (0.1527)  loss_giou_dn_4: 0.9669 (1.0098)  loss_fgl_dn_4: 1.0211 (1.0058)  loss_ddf_dn_4: 0.0005 (0.0006)  loss_vfl_dn_5: 0.4417 (0.4503)  loss_bbox_dn_5: 0.1080 (0.1524)  loss_giou_dn_5: 0.9653 (1.0089)  loss_fgl_dn_5: 1.0221 (1.0063)  loss_vfl_dn_pre: 0.4033 (0.4077)  loss_bbox_dn_pre: 0.1465 (0.1873)  loss_giou_dn_pre: 1.0924 (1.1223)  time: 0.6786  data: 0.0078  max mem: 3024\n",
            "Epoch: [2]  [400/500]  eta: 0:01:04  lr: 0.000013  loss: 38.1855 (39.4459)  loss_vfl: 0.5879 (0.6718)  loss_bbox: 0.1433 (0.1783)  loss_giou: 1.1064 (1.0412)  loss_fgl: 0.8827 (0.9453)  loss_vfl_aux_0: 0.5693 (0.6287)  loss_bbox_aux_0: 0.1584 (0.1986)  loss_giou_aux_0: 1.1407 (1.0861)  loss_fgl_aux_0: 0.8749 (0.9386)  loss_ddf_aux_0: 0.1825 (0.2230)  loss_vfl_aux_1: 0.5771 (0.6544)  loss_bbox_aux_1: 0.1442 (0.1828)  loss_giou_aux_1: 1.1189 (1.0506)  loss_fgl_aux_1: 0.8832 (0.9421)  loss_ddf_aux_1: 0.0342 (0.0414)  loss_vfl_aux_2: 0.6001 (0.6658)  loss_bbox_aux_2: 0.1435 (0.1796)  loss_giou_aux_2: 1.1101 (1.0432)  loss_fgl_aux_2: 0.8827 (0.9438)  loss_ddf_aux_2: 0.0060 (0.0071)  loss_vfl_aux_3: 0.5596 (0.6649)  loss_bbox_aux_3: 0.1434 (0.1787)  loss_giou_aux_3: 1.1074 (1.0415)  loss_fgl_aux_3: 0.8825 (0.9449)  loss_ddf_aux_3: 0.0011 (0.0012)  loss_vfl_aux_4: 0.5894 (0.6660)  loss_bbox_aux_4: 0.1433 (0.1783)  loss_giou_aux_4: 1.1063 (1.0412)  loss_fgl_aux_4: 0.8833 (0.9452)  loss_ddf_aux_4: 0.0001 (0.0001)  loss_vfl_pre: 0.5605 (0.6248)  loss_bbox_pre: 0.1592 (0.2002)  loss_giou_pre: 1.1386 (1.0867)  loss_vfl_enc_0: 0.5259 (0.5597)  loss_bbox_enc_0: 0.1958 (0.2678)  loss_giou_enc_0: 1.2633 (1.2343)  loss_vfl_dn_0: 0.4131 (0.4102)  loss_bbox_dn_0: 0.1416 (0.1853)  loss_giou_dn_0: 1.1185 (1.1158)  loss_fgl_dn_0: 0.9672 (0.9731)  loss_ddf_dn_0: 0.4957 (0.5395)  loss_vfl_dn_1: 0.4448 (0.4431)  loss_bbox_dn_1: 0.1160 (0.1638)  loss_giou_dn_1: 1.0383 (1.0379)  loss_fgl_dn_1: 0.9792 (0.9941)  loss_ddf_dn_1: 0.0902 (0.1145)  loss_vfl_dn_2: 0.4453 (0.4456)  loss_bbox_dn_2: 0.1106 (0.1566)  loss_giou_dn_2: 1.0066 (1.0175)  loss_fgl_dn_2: 0.9885 (1.0015)  loss_ddf_dn_2: 0.0188 (0.0252)  loss_vfl_dn_3: 0.4531 (0.4461)  loss_bbox_dn_3: 0.1104 (0.1539)  loss_giou_dn_3: 0.9971 (1.0099)  loss_fgl_dn_3: 0.9927 (1.0045)  loss_ddf_dn_3: 0.0033 (0.0049)  loss_vfl_dn_4: 0.4558 (0.4472)  loss_bbox_dn_4: 0.1104 (0.1528)  loss_giou_dn_4: 0.9959 (1.0070)  loss_fgl_dn_4: 0.9923 (1.0058)  loss_ddf_dn_4: 0.0004 (0.0006)  loss_vfl_dn_5: 0.4561 (0.4503)  loss_bbox_dn_5: 0.1112 (0.1524)  loss_giou_dn_5: 0.9944 (1.0062)  loss_fgl_dn_5: 0.9924 (1.0063)  loss_vfl_dn_pre: 0.4058 (0.4082)  loss_bbox_dn_pre: 0.1516 (0.1882)  loss_giou_dn_pre: 1.1122 (1.1199)  time: 0.6180  data: 0.0090  max mem: 3024\n",
            "Epoch: [2]  [499/500]  eta: 0:00:00  lr: 0.000013  loss: 37.7479 (39.3545)  loss_vfl: 0.6099 (0.6676)  loss_bbox: 0.1510 (0.1777)  loss_giou: 0.9805 (1.0392)  loss_fgl: 0.9232 (0.9440)  loss_vfl_aux_0: 0.5991 (0.6267)  loss_bbox_aux_0: 0.1702 (0.1974)  loss_giou_aux_0: 1.0512 (1.0833)  loss_fgl_aux_0: 0.9540 (0.9379)  loss_ddf_aux_0: 0.1581 (0.2183)  loss_vfl_aux_1: 0.6035 (0.6510)  loss_bbox_aux_1: 0.1569 (0.1818)  loss_giou_aux_1: 0.9908 (1.0483)  loss_fgl_aux_1: 0.9215 (0.9409)  loss_ddf_aux_1: 0.0247 (0.0400)  loss_vfl_aux_2: 0.6250 (0.6624)  loss_bbox_aux_2: 0.1564 (0.1788)  loss_giou_aux_2: 0.9807 (1.0410)  loss_fgl_aux_2: 0.9233 (0.9426)  loss_ddf_aux_2: 0.0034 (0.0068)  loss_vfl_aux_3: 0.6128 (0.6609)  loss_bbox_aux_3: 0.1535 (0.1780)  loss_giou_aux_3: 0.9815 (1.0395)  loss_fgl_aux_3: 0.9237 (0.9435)  loss_ddf_aux_3: 0.0007 (0.0012)  loss_vfl_aux_4: 0.6216 (0.6625)  loss_bbox_aux_4: 0.1510 (0.1777)  loss_giou_aux_4: 0.9807 (1.0392)  loss_fgl_aux_4: 0.9232 (0.9439)  loss_ddf_aux_4: 0.0001 (0.0001)  loss_vfl_pre: 0.6079 (0.6232)  loss_bbox_pre: 0.1707 (0.1985)  loss_giou_pre: 1.0501 (1.0838)  loss_vfl_enc_0: 0.5474 (0.5630)  loss_bbox_enc_0: 0.2018 (0.2644)  loss_giou_enc_0: 1.2075 (1.2268)  loss_vfl_dn_0: 0.4097 (0.4114)  loss_bbox_dn_0: 0.1744 (0.1848)  loss_giou_dn_0: 1.0587 (1.1125)  loss_fgl_dn_0: 0.9966 (0.9751)  loss_ddf_dn_0: 0.4587 (0.5420)  loss_vfl_dn_1: 0.4312 (0.4439)  loss_bbox_dn_1: 0.1441 (0.1626)  loss_giou_dn_1: 0.9637 (1.0329)  loss_fgl_dn_1: 0.9972 (0.9959)  loss_ddf_dn_1: 0.0989 (0.1130)  loss_vfl_dn_2: 0.4270 (0.4464)  loss_bbox_dn_2: 0.1261 (0.1551)  loss_giou_dn_2: 0.9383 (1.0122)  loss_fgl_dn_2: 0.9952 (1.0031)  loss_ddf_dn_2: 0.0200 (0.0244)  loss_vfl_dn_3: 0.4297 (0.4466)  loss_bbox_dn_3: 0.1204 (0.1525)  loss_giou_dn_3: 0.9267 (1.0046)  loss_fgl_dn_3: 0.9967 (1.0061)  loss_ddf_dn_3: 0.0038 (0.0047)  loss_vfl_dn_4: 0.4395 (0.4478)  loss_bbox_dn_4: 0.1187 (0.1514)  loss_giou_dn_4: 0.9202 (1.0018)  loss_fgl_dn_4: 0.9989 (1.0073)  loss_ddf_dn_4: 0.0004 (0.0005)  loss_vfl_dn_5: 0.4382 (0.4505)  loss_bbox_dn_5: 0.1183 (0.1511)  loss_giou_dn_5: 0.9214 (1.0011)  loss_fgl_dn_5: 0.9993 (1.0078)  loss_vfl_dn_pre: 0.4082 (0.4095)  loss_bbox_dn_pre: 0.1778 (0.1877)  loss_giou_dn_pre: 1.0652 (1.1163)  time: 0.6065  data: 0.0086  max mem: 3024\n",
            "Epoch: [2] Total time: 0:05:17 (0.6341 s / it)\n",
            "Averaged stats: lr: 0.000013  loss: 37.7479 (39.3545)  loss_vfl: 0.6099 (0.6676)  loss_bbox: 0.1510 (0.1777)  loss_giou: 0.9805 (1.0392)  loss_fgl: 0.9232 (0.9440)  loss_vfl_aux_0: 0.5991 (0.6267)  loss_bbox_aux_0: 0.1702 (0.1974)  loss_giou_aux_0: 1.0512 (1.0833)  loss_fgl_aux_0: 0.9540 (0.9379)  loss_ddf_aux_0: 0.1581 (0.2183)  loss_vfl_aux_1: 0.6035 (0.6510)  loss_bbox_aux_1: 0.1569 (0.1818)  loss_giou_aux_1: 0.9908 (1.0483)  loss_fgl_aux_1: 0.9215 (0.9409)  loss_ddf_aux_1: 0.0247 (0.0400)  loss_vfl_aux_2: 0.6250 (0.6624)  loss_bbox_aux_2: 0.1564 (0.1788)  loss_giou_aux_2: 0.9807 (1.0410)  loss_fgl_aux_2: 0.9233 (0.9426)  loss_ddf_aux_2: 0.0034 (0.0068)  loss_vfl_aux_3: 0.6128 (0.6609)  loss_bbox_aux_3: 0.1535 (0.1780)  loss_giou_aux_3: 0.9815 (1.0395)  loss_fgl_aux_3: 0.9237 (0.9435)  loss_ddf_aux_3: 0.0007 (0.0012)  loss_vfl_aux_4: 0.6216 (0.6625)  loss_bbox_aux_4: 0.1510 (0.1777)  loss_giou_aux_4: 0.9807 (1.0392)  loss_fgl_aux_4: 0.9232 (0.9439)  loss_ddf_aux_4: 0.0001 (0.0001)  loss_vfl_pre: 0.6079 (0.6232)  loss_bbox_pre: 0.1707 (0.1985)  loss_giou_pre: 1.0501 (1.0838)  loss_vfl_enc_0: 0.5474 (0.5630)  loss_bbox_enc_0: 0.2018 (0.2644)  loss_giou_enc_0: 1.2075 (1.2268)  loss_vfl_dn_0: 0.4097 (0.4114)  loss_bbox_dn_0: 0.1744 (0.1848)  loss_giou_dn_0: 1.0587 (1.1125)  loss_fgl_dn_0: 0.9966 (0.9751)  loss_ddf_dn_0: 0.4587 (0.5420)  loss_vfl_dn_1: 0.4312 (0.4439)  loss_bbox_dn_1: 0.1441 (0.1626)  loss_giou_dn_1: 0.9637 (1.0329)  loss_fgl_dn_1: 0.9972 (0.9959)  loss_ddf_dn_1: 0.0989 (0.1130)  loss_vfl_dn_2: 0.4270 (0.4464)  loss_bbox_dn_2: 0.1261 (0.1551)  loss_giou_dn_2: 0.9383 (1.0122)  loss_fgl_dn_2: 0.9952 (1.0031)  loss_ddf_dn_2: 0.0200 (0.0244)  loss_vfl_dn_3: 0.4297 (0.4466)  loss_bbox_dn_3: 0.1204 (0.1525)  loss_giou_dn_3: 0.9267 (1.0046)  loss_fgl_dn_3: 0.9967 (1.0061)  loss_ddf_dn_3: 0.0038 (0.0047)  loss_vfl_dn_4: 0.4395 (0.4478)  loss_bbox_dn_4: 0.1187 (0.1514)  loss_giou_dn_4: 0.9202 (1.0018)  loss_fgl_dn_4: 0.9989 (1.0073)  loss_ddf_dn_4: 0.0004 (0.0005)  loss_vfl_dn_5: 0.4382 (0.4505)  loss_bbox_dn_5: 0.1183 (0.1511)  loss_giou_dn_5: 0.9214 (1.0011)  loss_fgl_dn_5: 0.9993 (1.0078)  loss_vfl_dn_pre: 0.4082 (0.4095)  loss_bbox_dn_pre: 0.1778 (0.1877)  loss_giou_dn_pre: 1.0652 (1.1163)\n",
            "Test:  [  0/250]  eta: 0:07:57    time: 1.9118  data: 1.4859  max mem: 3024\n",
            "Test:  [ 10/250]  eta: 0:01:43    time: 0.4311  data: 0.1718  max mem: 3024\n",
            "Test:  [ 20/250]  eta: 0:01:19    time: 0.2651  data: 0.0369  max mem: 3024\n",
            "Test:  [ 30/250]  eta: 0:01:05    time: 0.2223  data: 0.0269  max mem: 3024\n",
            "Test:  [ 40/250]  eta: 0:00:57    time: 0.2001  data: 0.0212  max mem: 3024\n",
            "Test:  [ 50/250]  eta: 0:00:51    time: 0.2004  data: 0.0214  max mem: 3024\n",
            "Test:  [ 60/250]  eta: 0:00:47    time: 0.1972  data: 0.0200  max mem: 3024\n",
            "Test:  [ 70/250]  eta: 0:00:45    time: 0.2368  data: 0.0245  max mem: 3024\n",
            "Test:  [ 80/250]  eta: 0:00:43    time: 0.2722  data: 0.0353  max mem: 3024\n",
            "Test:  [ 90/250]  eta: 0:00:40    time: 0.2681  data: 0.0411  max mem: 3024\n",
            "Test:  [100/250]  eta: 0:00:37    time: 0.2384  data: 0.0324  max mem: 3024\n",
            "Test:  [110/250]  eta: 0:00:35    time: 0.2402  data: 0.0347  max mem: 3024\n",
            "Test:  [120/250]  eta: 0:00:33    time: 0.2826  data: 0.0486  max mem: 3024\n",
            "Test:  [130/250]  eta: 0:00:30    time: 0.2734  data: 0.0431  max mem: 3024\n",
            "Test:  [140/250]  eta: 0:00:28    time: 0.2784  data: 0.0376  max mem: 3024\n",
            "Test:  [150/250]  eta: 0:00:25    time: 0.2830  data: 0.0397  max mem: 3024\n",
            "Test:  [160/250]  eta: 0:00:23    time: 0.2330  data: 0.0307  max mem: 3024\n",
            "Test:  [170/250]  eta: 0:00:20    time: 0.2009  data: 0.0214  max mem: 3024\n",
            "Test:  [180/250]  eta: 0:00:17    time: 0.1991  data: 0.0206  max mem: 3024\n",
            "Test:  [190/250]  eta: 0:00:14    time: 0.1990  data: 0.0213  max mem: 3024\n",
            "Test:  [200/250]  eta: 0:00:12    time: 0.2022  data: 0.0221  max mem: 3024\n",
            "Test:  [210/250]  eta: 0:00:09    time: 0.2556  data: 0.0331  max mem: 3024\n",
            "Test:  [220/250]  eta: 0:00:07    time: 0.2869  data: 0.0452  max mem: 3024\n",
            "Test:  [230/250]  eta: 0:00:04    time: 0.2435  data: 0.0383  max mem: 3024\n",
            "Test:  [240/250]  eta: 0:00:02    time: 0.2079  data: 0.0253  max mem: 3024\n",
            "Test:  [249/250]  eta: 0:00:00    time: 0.1951  data: 0.0200  max mem: 3024\n",
            "Test: Total time: 0:01:01 (0.2445 s / it)\n",
            "Averaged stats: \n",
            "Accumulating evaluation results...\n",
            "COCOeval_opt.accumulate() finished...\n",
            "DONE (t=0.81s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.060\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.123\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.053\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.025\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.075\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.177\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.081\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.177\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.233\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.113\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.304\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.453\n",
            " Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.446\n",
            " Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.210\n",
            "best_stat: {'epoch': 2, 'coco_eval_bbox': 0.05952815304802394}\n",
            "Epoch: [3]  [  0/500]  eta: 0:34:57  lr: 0.000013  loss: 40.9127 (40.9127)  loss_vfl: 0.7261 (0.7261)  loss_bbox: 0.2121 (0.2121)  loss_giou: 0.8512 (0.8512)  loss_fgl: 1.1688 (1.1688)  loss_vfl_aux_0: 0.7559 (0.7559)  loss_bbox_aux_0: 0.2120 (0.2120)  loss_giou_aux_0: 0.8525 (0.8525)  loss_fgl_aux_0: 1.1574 (1.1574)  loss_ddf_aux_0: 0.2019 (0.2019)  loss_vfl_aux_1: 0.7222 (0.7222)  loss_bbox_aux_1: 0.2107 (0.2107)  loss_giou_aux_1: 0.8506 (0.8506)  loss_fgl_aux_1: 1.1625 (1.1625)  loss_ddf_aux_1: 0.0355 (0.0355)  loss_vfl_aux_2: 0.7349 (0.7349)  loss_bbox_aux_2: 0.2126 (0.2126)  loss_giou_aux_2: 0.8507 (0.8507)  loss_fgl_aux_2: 1.1662 (1.1662)  loss_ddf_aux_2: 0.0056 (0.0056)  loss_vfl_aux_3: 0.7495 (0.7495)  loss_bbox_aux_3: 0.2126 (0.2126)  loss_giou_aux_3: 0.8519 (0.8519)  loss_fgl_aux_3: 1.1667 (1.1667)  loss_ddf_aux_3: 0.0009 (0.0009)  loss_vfl_aux_4: 0.7129 (0.7129)  loss_bbox_aux_4: 0.2122 (0.2122)  loss_giou_aux_4: 0.8514 (0.8514)  loss_fgl_aux_4: 1.1680 (1.1680)  loss_ddf_aux_4: 0.0002 (0.0002)  loss_vfl_pre: 0.7480 (0.7480)  loss_bbox_pre: 0.2109 (0.2109)  loss_giou_pre: 0.8458 (0.8458)  loss_vfl_enc_0: 0.7305 (0.7305)  loss_bbox_enc_0: 0.2803 (0.2803)  loss_giou_enc_0: 0.9263 (0.9263)  loss_vfl_dn_0: 0.3909 (0.3909)  loss_bbox_dn_0: 0.2310 (0.2310)  loss_giou_dn_0: 1.0751 (1.0751)  loss_fgl_dn_0: 1.0436 (1.0436)  loss_ddf_dn_0: 1.0431 (1.0431)  loss_vfl_dn_1: 0.4380 (0.4380)  loss_bbox_dn_1: 0.2000 (0.2000)  loss_giou_dn_1: 0.9620 (0.9620)  loss_fgl_dn_1: 1.0946 (1.0946)  loss_ddf_dn_1: 0.2396 (0.2396)  loss_vfl_dn_2: 0.4446 (0.4446)  loss_bbox_dn_2: 0.1929 (0.1929)  loss_giou_dn_2: 0.9268 (0.9268)  loss_fgl_dn_2: 1.1151 (1.1151)  loss_ddf_dn_2: 0.0470 (0.0470)  loss_vfl_dn_3: 0.4453 (0.4453)  loss_bbox_dn_3: 0.1914 (0.1914)  loss_giou_dn_3: 0.9148 (0.9148)  loss_fgl_dn_3: 1.1270 (1.1270)  loss_ddf_dn_3: 0.0078 (0.0078)  loss_vfl_dn_4: 0.4446 (0.4446)  loss_bbox_dn_4: 0.1911 (0.1911)  loss_giou_dn_4: 0.9169 (0.9169)  loss_fgl_dn_4: 1.1258 (1.1258)  loss_ddf_dn_4: 0.0007 (0.0007)  loss_vfl_dn_5: 0.4441 (0.4441)  loss_bbox_dn_5: 0.1910 (0.1910)  loss_giou_dn_5: 0.9165 (0.9165)  loss_fgl_dn_5: 1.1265 (1.1265)  loss_vfl_dn_pre: 0.4058 (0.4058)  loss_bbox_dn_pre: 0.2260 (0.2260)  loss_giou_dn_pre: 1.0356 (1.0356)  time: 4.1941  data: 1.9785  max mem: 3024\n",
            "Epoch: [3]  [100/500]  eta: 0:04:26  lr: 0.000013  loss: 38.6598 (38.9279)  loss_vfl: 0.7539 (0.6477)  loss_bbox: 0.1448 (0.1690)  loss_giou: 0.8946 (1.0111)  loss_fgl: 1.0482 (0.9628)  loss_vfl_aux_0: 0.7495 (0.6242)  loss_bbox_aux_0: 0.1473 (0.1824)  loss_giou_aux_0: 0.9365 (1.0493)  loss_fgl_aux_0: 1.0891 (0.9622)  loss_ddf_aux_0: 0.1522 (0.1978)  loss_vfl_aux_1: 0.7451 (0.6391)  loss_bbox_aux_1: 0.1457 (0.1711)  loss_giou_aux_1: 0.9099 (1.0180)  loss_fgl_aux_1: 1.0609 (0.9615)  loss_ddf_aux_1: 0.0198 (0.0313)  loss_vfl_aux_2: 0.7397 (0.6450)  loss_bbox_aux_2: 0.1460 (0.1693)  loss_giou_aux_2: 0.8989 (1.0121)  loss_fgl_aux_2: 1.0536 (0.9620)  loss_ddf_aux_2: 0.0030 (0.0047)  loss_vfl_aux_3: 0.7671 (0.6393)  loss_bbox_aux_3: 0.1451 (0.1691)  loss_giou_aux_3: 0.8951 (1.0114)  loss_fgl_aux_3: 1.0511 (0.9625)  loss_ddf_aux_3: 0.0007 (0.0009)  loss_vfl_aux_4: 0.7397 (0.6421)  loss_bbox_aux_4: 0.1450 (0.1690)  loss_giou_aux_4: 0.8946 (1.0110)  loss_fgl_aux_4: 1.0493 (0.9628)  loss_ddf_aux_4: 0.0001 (0.0001)  loss_vfl_pre: 0.7422 (0.6187)  loss_bbox_pre: 0.1444 (0.1828)  loss_giou_pre: 0.9375 (1.0481)  loss_vfl_enc_0: 0.6162 (0.5801)  loss_bbox_enc_0: 0.2186 (0.2452)  loss_giou_enc_0: 1.1071 (1.1851)  loss_vfl_dn_0: 0.4297 (0.4209)  loss_bbox_dn_0: 0.1676 (0.1751)  loss_giou_dn_0: 1.0258 (1.0756)  loss_fgl_dn_0: 1.0215 (0.9983)  loss_ddf_dn_0: 0.6029 (0.6169)  loss_vfl_dn_1: 0.4697 (0.4548)  loss_bbox_dn_1: 0.1416 (0.1524)  loss_giou_dn_1: 0.9461 (0.9896)  loss_fgl_dn_1: 1.0337 (1.0178)  loss_ddf_dn_1: 0.1037 (0.1197)  loss_vfl_dn_2: 0.4678 (0.4563)  loss_bbox_dn_2: 0.1319 (0.1444)  loss_giou_dn_2: 0.9226 (0.9665)  loss_fgl_dn_2: 1.0372 (1.0248)  loss_ddf_dn_2: 0.0179 (0.0222)  loss_vfl_dn_3: 0.4717 (0.4579)  loss_bbox_dn_3: 0.1306 (0.1424)  loss_giou_dn_3: 0.9201 (0.9595)  loss_fgl_dn_3: 1.0398 (1.0279)  loss_ddf_dn_3: 0.0038 (0.0041)  loss_vfl_dn_4: 0.4761 (0.4602)  loss_bbox_dn_4: 0.1302 (0.1416)  loss_giou_dn_4: 0.9161 (0.9573)  loss_fgl_dn_4: 1.0411 (1.0289)  loss_ddf_dn_4: 0.0005 (0.0005)  loss_vfl_dn_5: 0.4771 (0.4622)  loss_bbox_dn_5: 0.1297 (0.1415)  loss_giou_dn_5: 0.9149 (0.9567)  loss_fgl_dn_5: 1.0406 (1.0293)  loss_vfl_dn_pre: 0.4258 (0.4194)  loss_bbox_dn_pre: 0.1683 (0.1773)  loss_giou_dn_pre: 1.0221 (1.0773)  time: 0.6427  data: 0.0080  max mem: 3024\n",
            "Epoch: [3]  [200/500]  eta: 0:03:14  lr: 0.000013  loss: 37.7392 (38.6052)  loss_vfl: 0.6567 (0.6317)  loss_bbox: 0.1426 (0.1601)  loss_giou: 0.9849 (1.0239)  loss_fgl: 0.9726 (0.9516)  loss_vfl_aux_0: 0.5571 (0.6065)  loss_bbox_aux_0: 0.1569 (0.1722)  loss_giou_aux_0: 1.0095 (1.0594)  loss_fgl_aux_0: 0.9677 (0.9519)  loss_ddf_aux_0: 0.1474 (0.1831)  loss_vfl_aux_1: 0.6323 (0.6197)  loss_bbox_aux_1: 0.1484 (0.1622)  loss_giou_aux_1: 0.9834 (1.0308)  loss_fgl_aux_1: 0.9757 (0.9506)  loss_ddf_aux_1: 0.0286 (0.0296)  loss_vfl_aux_2: 0.5977 (0.6256)  loss_bbox_aux_2: 0.1441 (0.1606)  loss_giou_aux_2: 0.9859 (1.0250)  loss_fgl_aux_2: 0.9738 (0.9509)  loss_ddf_aux_2: 0.0044 (0.0047)  loss_vfl_aux_3: 0.6445 (0.6241)  loss_bbox_aux_3: 0.1431 (0.1603)  loss_giou_aux_3: 0.9850 (1.0241)  loss_fgl_aux_3: 0.9732 (0.9513)  loss_ddf_aux_3: 0.0008 (0.0009)  loss_vfl_aux_4: 0.6348 (0.6256)  loss_bbox_aux_4: 0.1423 (0.1601)  loss_giou_aux_4: 0.9849 (1.0239)  loss_fgl_aux_4: 0.9727 (0.9515)  loss_ddf_aux_4: 0.0001 (0.0001)  loss_vfl_pre: 0.5557 (0.6029)  loss_bbox_pre: 0.1573 (0.1728)  loss_giou_pre: 1.0202 (1.0589)  loss_vfl_enc_0: 0.5518 (0.5675)  loss_bbox_enc_0: 0.2021 (0.2284)  loss_giou_enc_0: 1.1466 (1.1870)  loss_vfl_dn_0: 0.4114 (0.4170)  loss_bbox_dn_0: 0.1311 (0.1679)  loss_giou_dn_0: 1.0628 (1.0803)  loss_fgl_dn_0: 0.9992 (0.9935)  loss_ddf_dn_0: 0.4728 (0.5885)  loss_vfl_dn_1: 0.4460 (0.4494)  loss_bbox_dn_1: 0.1153 (0.1456)  loss_giou_dn_1: 0.9770 (0.9953)  loss_fgl_dn_1: 1.0190 (1.0123)  loss_ddf_dn_1: 0.0987 (0.1106)  loss_vfl_dn_2: 0.4500 (0.4509)  loss_bbox_dn_2: 0.1092 (0.1382)  loss_giou_dn_2: 0.9636 (0.9740)  loss_fgl_dn_2: 1.0227 (1.0184)  loss_ddf_dn_2: 0.0151 (0.0209)  loss_vfl_dn_3: 0.4451 (0.4518)  loss_bbox_dn_3: 0.1059 (0.1360)  loss_giou_dn_3: 0.9574 (0.9673)  loss_fgl_dn_3: 1.0242 (1.0212)  loss_ddf_dn_3: 0.0018 (0.0037)  loss_vfl_dn_4: 0.4451 (0.4530)  loss_bbox_dn_4: 0.1055 (0.1353)  loss_giou_dn_4: 0.9499 (0.9655)  loss_fgl_dn_4: 1.0246 (1.0221)  loss_ddf_dn_4: 0.0002 (0.0004)  loss_vfl_dn_5: 0.4465 (0.4551)  loss_bbox_dn_5: 0.1055 (0.1352)  loss_giou_dn_5: 0.9486 (0.9651)  loss_fgl_dn_5: 1.0251 (1.0225)  loss_vfl_dn_pre: 0.4116 (0.4155)  loss_bbox_dn_pre: 0.1285 (0.1700)  loss_giou_dn_pre: 1.0615 (1.0830)  time: 0.6748  data: 0.0097  max mem: 3024\n",
            "Epoch: [3]  [300/500]  eta: 0:02:09  lr: 0.000013  loss: 38.1678 (38.5838)  loss_vfl: 0.5894 (0.6377)  loss_bbox: 0.1365 (0.1576)  loss_giou: 1.0763 (1.0206)  loss_fgl: 0.9194 (0.9532)  loss_vfl_aux_0: 0.5562 (0.6064)  loss_bbox_aux_0: 0.1459 (0.1720)  loss_giou_aux_0: 1.1113 (1.0588)  loss_fgl_aux_0: 0.9003 (0.9529)  loss_ddf_aux_0: 0.1622 (0.1803)  loss_vfl_aux_1: 0.5806 (0.6219)  loss_bbox_aux_1: 0.1367 (0.1601)  loss_giou_aux_1: 1.0778 (1.0280)  loss_fgl_aux_1: 0.9155 (0.9522)  loss_ddf_aux_1: 0.0309 (0.0298)  loss_vfl_aux_2: 0.5864 (0.6297)  loss_bbox_aux_2: 0.1375 (0.1582)  loss_giou_aux_2: 1.0775 (1.0219)  loss_fgl_aux_2: 0.9176 (0.9525)  loss_ddf_aux_2: 0.0046 (0.0048)  loss_vfl_aux_3: 0.5894 (0.6307)  loss_bbox_aux_3: 0.1374 (0.1578)  loss_giou_aux_3: 1.0760 (1.0209)  loss_fgl_aux_3: 0.9188 (0.9529)  loss_ddf_aux_3: 0.0007 (0.0009)  loss_vfl_aux_4: 0.5879 (0.6323)  loss_bbox_aux_4: 0.1360 (0.1576)  loss_giou_aux_4: 1.0763 (1.0206)  loss_fgl_aux_4: 0.9195 (0.9531)  loss_ddf_aux_4: 0.0001 (0.0001)  loss_vfl_pre: 0.5605 (0.6025)  loss_bbox_pre: 0.1493 (0.1727)  loss_giou_pre: 1.1182 (1.0583)  loss_vfl_enc_0: 0.5049 (0.5741)  loss_bbox_enc_0: 0.1929 (0.2279)  loss_giou_enc_0: 1.2412 (1.1854)  loss_vfl_dn_0: 0.4172 (0.4176)  loss_bbox_dn_0: 0.1136 (0.1692)  loss_giou_dn_0: 1.0881 (1.0768)  loss_fgl_dn_0: 0.9840 (0.9950)  loss_ddf_dn_0: 0.5152 (0.5900)  loss_vfl_dn_1: 0.4448 (0.4502)  loss_bbox_dn_1: 0.1006 (0.1460)  loss_giou_dn_1: 1.0029 (0.9890)  loss_fgl_dn_1: 1.0062 (1.0138)  loss_ddf_dn_1: 0.0962 (0.1070)  loss_vfl_dn_2: 0.4475 (0.4513)  loss_bbox_dn_2: 0.0969 (0.1387)  loss_giou_dn_2: 0.9831 (0.9680)  loss_fgl_dn_2: 1.0147 (1.0197)  loss_ddf_dn_2: 0.0163 (0.0199)  loss_vfl_dn_3: 0.4438 (0.4519)  loss_bbox_dn_3: 0.0948 (0.1365)  loss_giou_dn_3: 0.9739 (0.9616)  loss_fgl_dn_3: 1.0107 (1.0220)  loss_ddf_dn_3: 0.0025 (0.0035)  loss_vfl_dn_4: 0.4426 (0.4525)  loss_bbox_dn_4: 0.0945 (0.1358)  loss_giou_dn_4: 0.9749 (0.9597)  loss_fgl_dn_4: 1.0104 (1.0229)  loss_ddf_dn_4: 0.0003 (0.0004)  loss_vfl_dn_5: 0.4482 (0.4542)  loss_bbox_dn_5: 0.0944 (0.1356)  loss_giou_dn_5: 0.9742 (0.9593)  loss_fgl_dn_5: 1.0101 (1.0233)  loss_vfl_dn_pre: 0.4136 (0.4165)  loss_bbox_dn_pre: 0.1147 (0.1707)  loss_giou_dn_pre: 1.0912 (1.0786)  time: 0.6272  data: 0.0091  max mem: 3024\n",
            "Epoch: [3]  [400/500]  eta: 0:01:03  lr: 0.000013  loss: 38.4221 (38.7313)  loss_vfl: 0.5757 (0.6424)  loss_bbox: 0.1718 (0.1645)  loss_giou: 0.9587 (1.0166)  loss_fgl: 0.9269 (0.9564)  loss_vfl_aux_0: 0.5352 (0.6118)  loss_bbox_aux_0: 0.1963 (0.1798)  loss_giou_aux_0: 1.0217 (1.0554)  loss_fgl_aux_0: 0.9186 (0.9558)  loss_ddf_aux_0: 0.1947 (0.1837)  loss_vfl_aux_1: 0.5615 (0.6293)  loss_bbox_aux_1: 0.1830 (0.1674)  loss_giou_aux_1: 0.9597 (1.0241)  loss_fgl_aux_1: 0.9185 (0.9555)  loss_ddf_aux_1: 0.0320 (0.0301)  loss_vfl_aux_2: 0.5566 (0.6351)  loss_bbox_aux_2: 0.1724 (0.1652)  loss_giou_aux_2: 0.9623 (1.0181)  loss_fgl_aux_2: 0.9257 (0.9557)  loss_ddf_aux_2: 0.0047 (0.0049)  loss_vfl_aux_3: 0.5586 (0.6361)  loss_bbox_aux_3: 0.1720 (0.1647)  loss_giou_aux_3: 0.9601 (1.0170)  loss_fgl_aux_3: 0.9272 (0.9562)  loss_ddf_aux_3: 0.0007 (0.0009)  loss_vfl_aux_4: 0.5601 (0.6375)  loss_bbox_aux_4: 0.1721 (0.1646)  loss_giou_aux_4: 0.9591 (1.0166)  loss_fgl_aux_4: 0.9266 (0.9563)  loss_ddf_aux_4: 0.0001 (0.0001)  loss_vfl_pre: 0.5210 (0.6078)  loss_bbox_pre: 0.2002 (0.1806)  loss_giou_pre: 1.0304 (1.0549)  loss_vfl_enc_0: 0.4854 (0.5752)  loss_bbox_enc_0: 0.2765 (0.2368)  loss_giou_enc_0: 1.1924 (1.1870)  loss_vfl_dn_0: 0.4280 (0.4177)  loss_bbox_dn_0: 0.1699 (0.1768)  loss_giou_dn_0: 1.0662 (1.0755)  loss_fgl_dn_0: 0.9979 (0.9967)  loss_ddf_dn_0: 0.5717 (0.5956)  loss_vfl_dn_1: 0.4482 (0.4505)  loss_bbox_dn_1: 0.1241 (0.1525)  loss_giou_dn_1: 0.9306 (0.9855)  loss_fgl_dn_1: 1.0517 (1.0165)  loss_ddf_dn_1: 0.1015 (0.1074)  loss_vfl_dn_2: 0.4429 (0.4517)  loss_bbox_dn_2: 0.1190 (0.1448)  loss_giou_dn_2: 0.9294 (0.9642)  loss_fgl_dn_2: 1.0585 (1.0226)  loss_ddf_dn_2: 0.0151 (0.0201)  loss_vfl_dn_3: 0.4556 (0.4525)  loss_bbox_dn_3: 0.1178 (0.1427)  loss_giou_dn_3: 0.9335 (0.9579)  loss_fgl_dn_3: 1.0617 (1.0249)  loss_ddf_dn_3: 0.0026 (0.0036)  loss_vfl_dn_4: 0.4531 (0.4527)  loss_bbox_dn_4: 0.1173 (0.1420)  loss_giou_dn_4: 0.9357 (0.9561)  loss_fgl_dn_4: 1.0639 (1.0259)  loss_ddf_dn_4: 0.0002 (0.0004)  loss_vfl_dn_5: 0.4565 (0.4545)  loss_bbox_dn_5: 0.1170 (0.1419)  loss_giou_dn_5: 0.9342 (0.9557)  loss_fgl_dn_5: 1.0647 (1.0262)  loss_vfl_dn_pre: 0.4238 (0.4166)  loss_bbox_dn_pre: 0.1667 (0.1783)  loss_giou_dn_pre: 1.0782 (1.0772)  time: 0.5793  data: 0.0078  max mem: 3024\n",
            "Epoch: [3]  [499/500]  eta: 0:00:00  lr: 0.000013  loss: 38.0522 (38.7521)  loss_vfl: 0.5479 (0.6406)  loss_bbox: 0.1790 (0.1679)  loss_giou: 1.0141 (1.0153)  loss_fgl: 0.9348 (0.9564)  loss_vfl_aux_0: 0.5493 (0.6108)  loss_bbox_aux_0: 0.1877 (0.1848)  loss_giou_aux_0: 1.0674 (1.0556)  loss_fgl_aux_0: 0.9423 (0.9555)  loss_ddf_aux_0: 0.1531 (0.1886)  loss_vfl_aux_1: 0.5684 (0.6282)  loss_bbox_aux_1: 0.1812 (0.1712)  loss_giou_aux_1: 1.0325 (1.0232)  loss_fgl_aux_1: 0.9308 (0.9556)  loss_ddf_aux_1: 0.0246 (0.0318)  loss_vfl_aux_2: 0.5938 (0.6333)  loss_bbox_aux_2: 0.1802 (0.1686)  loss_giou_aux_2: 1.0222 (1.0168)  loss_fgl_aux_2: 0.9345 (0.9557)  loss_ddf_aux_2: 0.0034 (0.0052)  loss_vfl_aux_3: 0.5352 (0.6341)  loss_bbox_aux_3: 0.1801 (0.1681)  loss_giou_aux_3: 1.0163 (1.0157)  loss_fgl_aux_3: 0.9349 (0.9561)  loss_ddf_aux_3: 0.0006 (0.0009)  loss_vfl_aux_4: 0.5518 (0.6357)  loss_bbox_aux_4: 0.1791 (0.1680)  loss_giou_aux_4: 1.0142 (1.0153)  loss_fgl_aux_4: 0.9348 (0.9563)  loss_ddf_aux_4: 0.0000 (0.0001)  loss_vfl_pre: 0.5488 (0.6059)  loss_bbox_pre: 0.1894 (0.1857)  loss_giou_pre: 1.0599 (1.0555)  loss_vfl_enc_0: 0.5576 (0.5740)  loss_bbox_enc_0: 0.2416 (0.2423)  loss_giou_enc_0: 1.1742 (1.1865)  loss_vfl_dn_0: 0.4019 (0.4181)  loss_bbox_dn_0: 0.1869 (0.1800)  loss_giou_dn_0: 1.0841 (1.0720)  loss_fgl_dn_0: 0.9984 (0.9986)  loss_ddf_dn_0: 0.6456 (0.5958)  loss_vfl_dn_1: 0.4487 (0.4513)  loss_bbox_dn_1: 0.1465 (0.1549)  loss_giou_dn_1: 0.9361 (0.9803)  loss_fgl_dn_1: 1.0266 (1.0189)  loss_ddf_dn_1: 0.1159 (0.1086)  loss_vfl_dn_2: 0.4468 (0.4524)  loss_bbox_dn_2: 0.1366 (0.1469)  loss_giou_dn_2: 0.9125 (0.9582)  loss_fgl_dn_2: 1.0398 (1.0249)  loss_ddf_dn_2: 0.0231 (0.0200)  loss_vfl_dn_3: 0.4458 (0.4531)  loss_bbox_dn_3: 0.1353 (0.1448)  loss_giou_dn_3: 0.9098 (0.9518)  loss_fgl_dn_3: 1.0416 (1.0273)  loss_ddf_dn_3: 0.0034 (0.0036)  loss_vfl_dn_4: 0.4475 (0.4532)  loss_bbox_dn_4: 0.1327 (0.1440)  loss_giou_dn_4: 0.9090 (0.9500)  loss_fgl_dn_4: 1.0426 (1.0282)  loss_ddf_dn_4: 0.0005 (0.0004)  loss_vfl_dn_5: 0.4460 (0.4552)  loss_bbox_dn_5: 0.1319 (0.1439)  loss_giou_dn_5: 0.9083 (0.9496)  loss_fgl_dn_5: 1.0425 (1.0286)  loss_vfl_dn_pre: 0.4055 (0.4171)  loss_bbox_dn_pre: 0.1853 (0.1813)  loss_giou_dn_pre: 1.0584 (1.0738)  time: 0.6398  data: 0.0080  max mem: 3024\n",
            "Epoch: [3] Total time: 0:05:20 (0.6412 s / it)\n",
            "Averaged stats: lr: 0.000013  loss: 38.0522 (38.7521)  loss_vfl: 0.5479 (0.6406)  loss_bbox: 0.1790 (0.1679)  loss_giou: 1.0141 (1.0153)  loss_fgl: 0.9348 (0.9564)  loss_vfl_aux_0: 0.5493 (0.6108)  loss_bbox_aux_0: 0.1877 (0.1848)  loss_giou_aux_0: 1.0674 (1.0556)  loss_fgl_aux_0: 0.9423 (0.9555)  loss_ddf_aux_0: 0.1531 (0.1886)  loss_vfl_aux_1: 0.5684 (0.6282)  loss_bbox_aux_1: 0.1812 (0.1712)  loss_giou_aux_1: 1.0325 (1.0232)  loss_fgl_aux_1: 0.9308 (0.9556)  loss_ddf_aux_1: 0.0246 (0.0318)  loss_vfl_aux_2: 0.5938 (0.6333)  loss_bbox_aux_2: 0.1802 (0.1686)  loss_giou_aux_2: 1.0222 (1.0168)  loss_fgl_aux_2: 0.9345 (0.9557)  loss_ddf_aux_2: 0.0034 (0.0052)  loss_vfl_aux_3: 0.5352 (0.6341)  loss_bbox_aux_3: 0.1801 (0.1681)  loss_giou_aux_3: 1.0163 (1.0157)  loss_fgl_aux_3: 0.9349 (0.9561)  loss_ddf_aux_3: 0.0006 (0.0009)  loss_vfl_aux_4: 0.5518 (0.6357)  loss_bbox_aux_4: 0.1791 (0.1680)  loss_giou_aux_4: 1.0142 (1.0153)  loss_fgl_aux_4: 0.9348 (0.9563)  loss_ddf_aux_4: 0.0000 (0.0001)  loss_vfl_pre: 0.5488 (0.6059)  loss_bbox_pre: 0.1894 (0.1857)  loss_giou_pre: 1.0599 (1.0555)  loss_vfl_enc_0: 0.5576 (0.5740)  loss_bbox_enc_0: 0.2416 (0.2423)  loss_giou_enc_0: 1.1742 (1.1865)  loss_vfl_dn_0: 0.4019 (0.4181)  loss_bbox_dn_0: 0.1869 (0.1800)  loss_giou_dn_0: 1.0841 (1.0720)  loss_fgl_dn_0: 0.9984 (0.9986)  loss_ddf_dn_0: 0.6456 (0.5958)  loss_vfl_dn_1: 0.4487 (0.4513)  loss_bbox_dn_1: 0.1465 (0.1549)  loss_giou_dn_1: 0.9361 (0.9803)  loss_fgl_dn_1: 1.0266 (1.0189)  loss_ddf_dn_1: 0.1159 (0.1086)  loss_vfl_dn_2: 0.4468 (0.4524)  loss_bbox_dn_2: 0.1366 (0.1469)  loss_giou_dn_2: 0.9125 (0.9582)  loss_fgl_dn_2: 1.0398 (1.0249)  loss_ddf_dn_2: 0.0231 (0.0200)  loss_vfl_dn_3: 0.4458 (0.4531)  loss_bbox_dn_3: 0.1353 (0.1448)  loss_giou_dn_3: 0.9098 (0.9518)  loss_fgl_dn_3: 1.0416 (1.0273)  loss_ddf_dn_3: 0.0034 (0.0036)  loss_vfl_dn_4: 0.4475 (0.4532)  loss_bbox_dn_4: 0.1327 (0.1440)  loss_giou_dn_4: 0.9090 (0.9500)  loss_fgl_dn_4: 1.0426 (1.0282)  loss_ddf_dn_4: 0.0005 (0.0004)  loss_vfl_dn_5: 0.4460 (0.4552)  loss_bbox_dn_5: 0.1319 (0.1439)  loss_giou_dn_5: 0.9083 (0.9496)  loss_fgl_dn_5: 1.0425 (1.0286)  loss_vfl_dn_pre: 0.4055 (0.4171)  loss_bbox_dn_pre: 0.1853 (0.1813)  loss_giou_dn_pre: 1.0584 (1.0738)\n",
            "Test:  [  0/250]  eta: 0:05:10    time: 1.2433  data: 0.9270  max mem: 3024\n",
            "Test:  [ 10/250]  eta: 0:01:09    time: 0.2913  data: 0.1013  max mem: 3024\n",
            "Test:  [ 20/250]  eta: 0:01:02    time: 0.2226  data: 0.0254  max mem: 3024\n",
            "Test:  [ 30/250]  eta: 0:00:58    time: 0.2539  data: 0.0371  max mem: 3024\n",
            "Test:  [ 40/250]  eta: 0:00:58    time: 0.2850  data: 0.0424  max mem: 3024\n",
            "Test:  [ 50/250]  eta: 0:00:52    time: 0.2544  data: 0.0314  max mem: 3024\n",
            "Test:  [ 60/250]  eta: 0:00:48    time: 0.2016  data: 0.0206  max mem: 3024\n",
            "Test:  [ 70/250]  eta: 0:00:44    time: 0.2022  data: 0.0215  max mem: 3024\n",
            "Test:  [ 80/250]  eta: 0:00:40    time: 0.1982  data: 0.0214  max mem: 3024\n",
            "Test:  [ 90/250]  eta: 0:00:37    time: 0.2067  data: 0.0213  max mem: 3024\n",
            "Test:  [100/250]  eta: 0:00:35    time: 0.2388  data: 0.0321  max mem: 3024\n",
            "Test:  [110/250]  eta: 0:00:34    time: 0.2934  data: 0.0431  max mem: 3024\n",
            "Test:  [120/250]  eta: 0:00:31    time: 0.2750  data: 0.0366  max mem: 3024\n",
            "Test:  [130/250]  eta: 0:00:29    time: 0.2127  data: 0.0260  max mem: 3024\n",
            "Test:  [140/250]  eta: 0:00:26    time: 0.1999  data: 0.0217  max mem: 3024\n",
            "Test:  [150/250]  eta: 0:00:23    time: 0.1999  data: 0.0208  max mem: 3024\n",
            "Test:  [160/250]  eta: 0:00:21    time: 0.1987  data: 0.0208  max mem: 3024\n",
            "Test:  [170/250]  eta: 0:00:18    time: 0.2170  data: 0.0287  max mem: 3024\n",
            "Test:  [180/250]  eta: 0:00:16    time: 0.2489  data: 0.0411  max mem: 3024\n",
            "Test:  [190/250]  eta: 0:00:14    time: 0.2917  data: 0.0449  max mem: 3024\n",
            "Test:  [200/250]  eta: 0:00:11    time: 0.2679  data: 0.0354  max mem: 3024\n",
            "Test:  [210/250]  eta: 0:00:09    time: 0.2055  data: 0.0232  max mem: 3024\n",
            "Test:  [220/250]  eta: 0:00:07    time: 0.1977  data: 0.0200  max mem: 3024\n",
            "Test:  [230/250]  eta: 0:00:04    time: 0.1988  data: 0.0204  max mem: 3024\n",
            "Test:  [240/250]  eta: 0:00:02    time: 0.1989  data: 0.0204  max mem: 3024\n",
            "Test:  [249/250]  eta: 0:00:00    time: 0.2006  data: 0.0208  max mem: 3024\n",
            "Test: Total time: 0:00:58 (0.2321 s / it)\n",
            "Averaged stats: \n",
            "Accumulating evaluation results...\n",
            "COCOeval_opt.accumulate() finished...\n",
            "DONE (t=1.33s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.068\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.141\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.059\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.029\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.090\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.180\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.083\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.189\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.238\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.119\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.302\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.428\n",
            " Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.467\n",
            " Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.211\n",
            "best_stat: {'epoch': 3, 'coco_eval_bbox': 0.06825418641384003}\n",
            "Epoch: [4]  [  0/500]  eta: 0:18:37  lr: 0.000013  loss: 38.1821 (38.1821)  loss_vfl: 0.5103 (0.5103)  loss_bbox: 0.1637 (0.1637)  loss_giou: 1.2831 (1.2831)  loss_fgl: 0.8340 (0.8340)  loss_vfl_aux_0: 0.4963 (0.4963)  loss_bbox_aux_0: 0.1697 (0.1697)  loss_giou_aux_0: 1.2976 (1.2976)  loss_fgl_aux_0: 0.8355 (0.8355)  loss_ddf_aux_0: 0.1591 (0.1591)  loss_vfl_aux_1: 0.4563 (0.4563)  loss_bbox_aux_1: 0.1638 (0.1638)  loss_giou_aux_1: 1.2861 (1.2861)  loss_fgl_aux_1: 0.8334 (0.8334)  loss_ddf_aux_1: 0.0221 (0.0221)  loss_vfl_aux_2: 0.4651 (0.4651)  loss_bbox_aux_2: 0.1635 (0.1635)  loss_giou_aux_2: 1.2827 (1.2827)  loss_fgl_aux_2: 0.8342 (0.8342)  loss_ddf_aux_2: 0.0024 (0.0024)  loss_vfl_aux_3: 0.4885 (0.4885)  loss_bbox_aux_3: 0.1636 (0.1636)  loss_giou_aux_3: 1.2828 (1.2828)  loss_fgl_aux_3: 0.8346 (0.8346)  loss_ddf_aux_3: 0.0005 (0.0005)  loss_vfl_aux_4: 0.4990 (0.4990)  loss_bbox_aux_4: 0.1637 (0.1637)  loss_giou_aux_4: 1.2831 (1.2831)  loss_fgl_aux_4: 0.8342 (0.8342)  loss_ddf_aux_4: 0.0001 (0.0001)  loss_vfl_pre: 0.4900 (0.4900)  loss_bbox_pre: 0.1714 (0.1714)  loss_giou_pre: 1.2938 (1.2938)  loss_vfl_enc_0: 0.4153 (0.4153)  loss_bbox_enc_0: 0.1982 (0.1982)  loss_giou_enc_0: 1.4004 (1.4004)  loss_vfl_dn_0: 0.4265 (0.4265)  loss_bbox_dn_0: 0.0782 (0.0782)  loss_giou_dn_0: 1.0990 (1.0990)  loss_fgl_dn_0: 0.9849 (0.9849)  loss_ddf_dn_0: 0.4494 (0.4494)  loss_vfl_dn_1: 0.4294 (0.4294)  loss_bbox_dn_1: 0.0735 (0.0735)  loss_giou_dn_1: 1.0606 (1.0606)  loss_fgl_dn_1: 0.9778 (0.9778)  loss_ddf_dn_1: 0.0529 (0.0529)  loss_vfl_dn_2: 0.4270 (0.4270)  loss_bbox_dn_2: 0.0753 (0.0753)  loss_giou_dn_2: 1.0573 (1.0573)  loss_fgl_dn_2: 0.9778 (0.9778)  loss_ddf_dn_2: 0.0066 (0.0066)  loss_vfl_dn_3: 0.4277 (0.4277)  loss_bbox_dn_3: 0.0762 (0.0762)  loss_giou_dn_3: 1.0625 (1.0625)  loss_fgl_dn_3: 0.9760 (0.9760)  loss_ddf_dn_3: 0.0013 (0.0013)  loss_vfl_dn_4: 0.4275 (0.4275)  loss_bbox_dn_4: 0.0754 (0.0754)  loss_giou_dn_4: 1.0616 (1.0616)  loss_fgl_dn_4: 0.9758 (0.9758)  loss_ddf_dn_4: 0.0003 (0.0003)  loss_vfl_dn_5: 0.4236 (0.4236)  loss_bbox_dn_5: 0.0750 (0.0750)  loss_giou_dn_5: 1.0607 (1.0607)  loss_fgl_dn_5: 0.9764 (0.9764)  loss_vfl_dn_pre: 0.4192 (0.4192)  loss_bbox_dn_pre: 0.0789 (0.0789)  loss_giou_dn_pre: 1.1097 (1.1097)  time: 2.2350  data: 1.1236  max mem: 3024\n",
            "Epoch: [4]  [100/500]  eta: 0:04:19  lr: 0.000013  loss: 37.8357 (39.0914)  loss_vfl: 0.4939 (0.5775)  loss_bbox: 0.1539 (0.2052)  loss_giou: 1.1268 (1.0986)  loss_fgl: 0.8402 (0.9031)  loss_vfl_aux_0: 0.4844 (0.5562)  loss_bbox_aux_0: 0.1661 (0.2255)  loss_giou_aux_0: 1.1508 (1.1450)  loss_fgl_aux_0: 0.8343 (0.8991)  loss_ddf_aux_0: 0.1670 (0.2143)  loss_vfl_aux_1: 0.5020 (0.5815)  loss_bbox_aux_1: 0.1557 (0.2079)  loss_giou_aux_1: 1.1264 (1.1075)  loss_fgl_aux_1: 0.8454 (0.9004)  loss_ddf_aux_1: 0.0260 (0.0339)  loss_vfl_aux_2: 0.4709 (0.5803)  loss_bbox_aux_2: 0.1525 (0.2056)  loss_giou_aux_2: 1.1270 (1.1002)  loss_fgl_aux_2: 0.8398 (0.9019)  loss_ddf_aux_2: 0.0038 (0.0053)  loss_vfl_aux_3: 0.4961 (0.5786)  loss_bbox_aux_3: 0.1528 (0.2053)  loss_giou_aux_3: 1.1277 (1.0989)  loss_fgl_aux_3: 0.8398 (0.9027)  loss_ddf_aux_3: 0.0005 (0.0010)  loss_vfl_aux_4: 0.4836 (0.5752)  loss_bbox_aux_4: 0.1537 (0.2052)  loss_giou_aux_4: 1.1274 (1.0986)  loss_fgl_aux_4: 0.8401 (0.9030)  loss_ddf_aux_4: 0.0001 (0.0001)  loss_vfl_pre: 0.4834 (0.5527)  loss_bbox_pre: 0.1686 (0.2261)  loss_giou_pre: 1.1486 (1.1434)  loss_vfl_enc_0: 0.4602 (0.5259)  loss_bbox_enc_0: 0.2128 (0.2893)  loss_giou_enc_0: 1.3220 (1.2766)  loss_vfl_dn_0: 0.4080 (0.4202)  loss_bbox_dn_0: 0.1364 (0.1862)  loss_giou_dn_0: 1.1044 (1.0689)  loss_fgl_dn_0: 0.9679 (1.0025)  loss_ddf_dn_0: 0.5165 (0.6504)  loss_vfl_dn_1: 0.4358 (0.4541)  loss_bbox_dn_1: 0.1171 (0.1564)  loss_giou_dn_1: 1.0137 (0.9696)  loss_fgl_dn_1: 0.9867 (1.0243)  loss_ddf_dn_1: 0.0893 (0.1177)  loss_vfl_dn_2: 0.4304 (0.4512)  loss_bbox_dn_2: 0.1115 (0.1489)  loss_giou_dn_2: 1.0040 (0.9503)  loss_fgl_dn_2: 0.9885 (1.0301)  loss_ddf_dn_2: 0.0150 (0.0237)  loss_vfl_dn_3: 0.4309 (0.4519)  loss_bbox_dn_3: 0.1112 (0.1470)  loss_giou_dn_3: 1.0007 (0.9442)  loss_fgl_dn_3: 0.9886 (1.0321)  loss_ddf_dn_3: 0.0027 (0.0047)  loss_vfl_dn_4: 0.4355 (0.4512)  loss_bbox_dn_4: 0.1096 (0.1459)  loss_giou_dn_4: 1.0025 (0.9423)  loss_fgl_dn_4: 0.9889 (1.0331)  loss_ddf_dn_4: 0.0003 (0.0005)  loss_vfl_dn_5: 0.4434 (0.4534)  loss_bbox_dn_5: 0.1093 (0.1457)  loss_giou_dn_5: 1.0031 (0.9419)  loss_fgl_dn_5: 0.9890 (1.0334)  loss_vfl_dn_pre: 0.4062 (0.4193)  loss_bbox_dn_pre: 0.1359 (0.1879)  loss_giou_dn_pre: 1.0982 (1.0706)  time: 0.6455  data: 0.0111  max mem: 3024\n",
            "Epoch: [4]  [200/500]  eta: 0:03:15  lr: 0.000013  loss: 37.3753 (38.7128)  loss_vfl: 0.6089 (0.5996)  loss_bbox: 0.1311 (0.1845)  loss_giou: 0.9488 (1.0569)  loss_fgl: 0.9560 (0.9192)  loss_vfl_aux_0: 0.5942 (0.5734)  loss_bbox_aux_0: 0.1500 (0.2040)  loss_giou_aux_0: 1.0022 (1.1015)  loss_fgl_aux_0: 0.9628 (0.9200)  loss_ddf_aux_0: 0.1850 (0.2169)  loss_vfl_aux_1: 0.6050 (0.5939)  loss_bbox_aux_1: 0.1326 (0.1877)  loss_giou_aux_1: 0.9699 (1.0660)  loss_fgl_aux_1: 0.9540 (0.9178)  loss_ddf_aux_1: 0.0346 (0.0362)  loss_vfl_aux_2: 0.6372 (0.5991)  loss_bbox_aux_2: 0.1304 (0.1851)  loss_giou_aux_2: 0.9482 (1.0584)  loss_fgl_aux_2: 0.9558 (0.9184)  loss_ddf_aux_2: 0.0037 (0.0052)  loss_vfl_aux_3: 0.6265 (0.5997)  loss_bbox_aux_3: 0.1311 (0.1847)  loss_giou_aux_3: 0.9479 (1.0572)  loss_fgl_aux_3: 0.9561 (0.9190)  loss_ddf_aux_3: 0.0008 (0.0009)  loss_vfl_aux_4: 0.6138 (0.5952)  loss_bbox_aux_4: 0.1311 (0.1845)  loss_giou_aux_4: 0.9483 (1.0569)  loss_fgl_aux_4: 0.9557 (0.9191)  loss_ddf_aux_4: 0.0001 (0.0001)  loss_vfl_pre: 0.5898 (0.5693)  loss_bbox_pre: 0.1541 (0.2050)  loss_giou_pre: 0.9949 (1.1003)  loss_vfl_enc_0: 0.5483 (0.5491)  loss_bbox_enc_0: 0.1943 (0.2694)  loss_giou_enc_0: 1.1273 (1.2336)  loss_vfl_dn_0: 0.4348 (0.4230)  loss_bbox_dn_0: 0.1399 (0.1753)  loss_giou_dn_0: 1.0398 (1.0566)  loss_fgl_dn_0: 1.0126 (1.0066)  loss_ddf_dn_0: 0.4664 (0.6374)  loss_vfl_dn_1: 0.4517 (0.4545)  loss_bbox_dn_1: 0.1141 (0.1481)  loss_giou_dn_1: 0.9436 (0.9612)  loss_fgl_dn_1: 1.0181 (1.0248)  loss_ddf_dn_1: 0.0991 (0.1176)  loss_vfl_dn_2: 0.4529 (0.4539)  loss_bbox_dn_2: 0.1070 (0.1400)  loss_giou_dn_2: 0.9227 (0.9406)  loss_fgl_dn_2: 1.0242 (1.0296)  loss_ddf_dn_2: 0.0146 (0.0215)  loss_vfl_dn_3: 0.4536 (0.4542)  loss_bbox_dn_3: 0.1058 (0.1381)  loss_giou_dn_3: 0.9245 (0.9351)  loss_fgl_dn_3: 1.0229 (1.0313)  loss_ddf_dn_3: 0.0026 (0.0040)  loss_vfl_dn_4: 0.4543 (0.4532)  loss_bbox_dn_4: 0.1042 (0.1371)  loss_giou_dn_4: 0.9253 (0.9334)  loss_fgl_dn_4: 1.0237 (1.0321)  loss_ddf_dn_4: 0.0004 (0.0004)  loss_vfl_dn_5: 0.4578 (0.4554)  loss_bbox_dn_5: 0.1043 (0.1369)  loss_giou_dn_5: 0.9240 (0.9330)  loss_fgl_dn_5: 1.0239 (1.0324)  loss_vfl_dn_pre: 0.4326 (0.4219)  loss_bbox_dn_pre: 0.1385 (0.1768)  loss_giou_dn_pre: 1.0169 (1.0590)  time: 0.6891  data: 0.0082  max mem: 3024\n",
            "Epoch: [4]  [300/500]  eta: 0:02:10  lr: 0.000013  loss: 37.5272 (38.6103)  loss_vfl: 0.6021 (0.6060)  loss_bbox: 0.1572 (0.1786)  loss_giou: 0.9676 (1.0490)  loss_fgl: 0.9612 (0.9237)  loss_vfl_aux_0: 0.5835 (0.5780)  loss_bbox_aux_0: 0.1734 (0.1969)  loss_giou_aux_0: 0.9715 (1.0921)  loss_fgl_aux_0: 0.9590 (0.9242)  loss_ddf_aux_0: 0.2116 (0.2141)  loss_vfl_aux_1: 0.5679 (0.5983)  loss_bbox_aux_1: 0.1547 (0.1815)  loss_giou_aux_1: 0.9672 (1.0581)  loss_fgl_aux_1: 0.9597 (0.9227)  loss_ddf_aux_1: 0.0349 (0.0368)  loss_vfl_aux_2: 0.5757 (0.6036)  loss_bbox_aux_2: 0.1564 (0.1790)  loss_giou_aux_2: 0.9739 (1.0504)  loss_fgl_aux_2: 0.9607 (0.9230)  loss_ddf_aux_2: 0.0050 (0.0053)  loss_vfl_aux_3: 0.5957 (0.6024)  loss_bbox_aux_3: 0.1564 (0.1787)  loss_giou_aux_3: 0.9699 (1.0493)  loss_fgl_aux_3: 0.9605 (0.9234)  loss_ddf_aux_3: 0.0009 (0.0009)  loss_vfl_aux_4: 0.5894 (0.6003)  loss_bbox_aux_4: 0.1572 (0.1786)  loss_giou_aux_4: 0.9678 (1.0490)  loss_fgl_aux_4: 0.9609 (0.9236)  loss_ddf_aux_4: 0.0001 (0.0001)  loss_vfl_pre: 0.5752 (0.5740)  loss_bbox_pre: 0.1705 (0.1977)  loss_giou_pre: 0.9710 (1.0912)  loss_vfl_enc_0: 0.5586 (0.5505)  loss_bbox_enc_0: 0.2265 (0.2574)  loss_giou_enc_0: 1.1006 (1.2209)  loss_vfl_dn_0: 0.4128 (0.4214)  loss_bbox_dn_0: 0.1291 (0.1687)  loss_giou_dn_0: 1.0218 (1.0602)  loss_fgl_dn_0: 1.0274 (1.0051)  loss_ddf_dn_0: 0.6464 (0.6320)  loss_vfl_dn_1: 0.4380 (0.4523)  loss_bbox_dn_1: 0.1087 (0.1431)  loss_giou_dn_1: 0.8919 (0.9671)  loss_fgl_dn_1: 1.0470 (1.0226)  loss_ddf_dn_1: 0.1212 (0.1147)  loss_vfl_dn_2: 0.4448 (0.4521)  loss_bbox_dn_2: 0.1004 (0.1356)  loss_giou_dn_2: 0.8680 (0.9469)  loss_fgl_dn_2: 1.0537 (1.0270)  loss_ddf_dn_2: 0.0216 (0.0201)  loss_vfl_dn_3: 0.4392 (0.4513)  loss_bbox_dn_3: 0.0994 (0.1339)  loss_giou_dn_3: 0.8690 (0.9416)  loss_fgl_dn_3: 1.0586 (1.0287)  loss_ddf_dn_3: 0.0038 (0.0037)  loss_vfl_dn_4: 0.4399 (0.4513)  loss_bbox_dn_4: 0.0994 (0.1332)  loss_giou_dn_4: 0.8655 (0.9399)  loss_fgl_dn_4: 1.0603 (1.0295)  loss_ddf_dn_4: 0.0003 (0.0004)  loss_vfl_dn_5: 0.4399 (0.4534)  loss_bbox_dn_5: 0.0997 (0.1330)  loss_giou_dn_5: 0.8647 (0.9396)  loss_fgl_dn_5: 1.0606 (1.0298)  loss_vfl_dn_pre: 0.4192 (0.4202)  loss_bbox_dn_pre: 0.1276 (0.1699)  loss_giou_dn_pre: 0.9840 (1.0628)  time: 0.5631  data: 0.0079  max mem: 3024\n",
            "Epoch: [4]  [400/500]  eta: 0:01:04  lr: 0.000013  loss: 37.2946 (38.4470)  loss_vfl: 0.5444 (0.6048)  loss_bbox: 0.1375 (0.1720)  loss_giou: 0.9135 (1.0419)  loss_fgl: 0.9554 (0.9262)  loss_vfl_aux_0: 0.5273 (0.5766)  loss_bbox_aux_0: 0.1415 (0.1903)  loss_giou_aux_0: 0.9647 (1.0853)  loss_fgl_aux_0: 0.9732 (0.9271)  loss_ddf_aux_0: 0.2142 (0.2156)  loss_vfl_aux_1: 0.5605 (0.5972)  loss_bbox_aux_1: 0.1391 (0.1750)  loss_giou_aux_1: 0.9181 (1.0510)  loss_fgl_aux_1: 0.9566 (0.9251)  loss_ddf_aux_1: 0.0343 (0.0370)  loss_vfl_aux_2: 0.5679 (0.6044)  loss_bbox_aux_2: 0.1374 (0.1725)  loss_giou_aux_2: 0.9084 (1.0433)  loss_fgl_aux_2: 0.9524 (0.9255)  loss_ddf_aux_2: 0.0050 (0.0054)  loss_vfl_aux_3: 0.5703 (0.6004)  loss_bbox_aux_3: 0.1376 (0.1721)  loss_giou_aux_3: 0.9117 (1.0422)  loss_fgl_aux_3: 0.9545 (0.9260)  loss_ddf_aux_3: 0.0009 (0.0009)  loss_vfl_aux_4: 0.5420 (0.5998)  loss_bbox_aux_4: 0.1377 (0.1720)  loss_giou_aux_4: 0.9129 (1.0419)  loss_fgl_aux_4: 0.9558 (0.9262)  loss_ddf_aux_4: 0.0001 (0.0001)  loss_vfl_pre: 0.5610 (0.5735)  loss_bbox_pre: 0.1402 (0.1912)  loss_giou_pre: 0.9804 (1.0842)  loss_vfl_enc_0: 0.5112 (0.5519)  loss_bbox_enc_0: 0.1792 (0.2495)  loss_giou_enc_0: 1.1468 (1.2123)  loss_vfl_dn_0: 0.4390 (0.4221)  loss_bbox_dn_0: 0.1284 (0.1656)  loss_giou_dn_0: 0.9981 (1.0548)  loss_fgl_dn_0: 1.0408 (1.0076)  loss_ddf_dn_0: 0.5963 (0.6275)  loss_vfl_dn_1: 0.4739 (0.4529)  loss_bbox_dn_1: 0.0986 (0.1397)  loss_giou_dn_1: 0.8857 (0.9604)  loss_fgl_dn_1: 1.0500 (1.0238)  loss_ddf_dn_1: 0.0910 (0.1124)  loss_vfl_dn_2: 0.4700 (0.4529)  loss_bbox_dn_2: 0.0932 (0.1325)  loss_giou_dn_2: 0.8535 (0.9399)  loss_fgl_dn_2: 1.0512 (1.0279)  loss_ddf_dn_2: 0.0144 (0.0196)  loss_vfl_dn_3: 0.4670 (0.4521)  loss_bbox_dn_3: 0.0922 (0.1307)  loss_giou_dn_3: 0.8445 (0.9344)  loss_fgl_dn_3: 1.0505 (1.0296)  loss_ddf_dn_3: 0.0033 (0.0035)  loss_vfl_dn_4: 0.4646 (0.4516)  loss_bbox_dn_4: 0.0921 (0.1300)  loss_giou_dn_4: 0.8400 (0.9328)  loss_fgl_dn_4: 1.0516 (1.0304)  loss_ddf_dn_4: 0.0003 (0.0004)  loss_vfl_dn_5: 0.4604 (0.4535)  loss_bbox_dn_5: 0.0919 (0.1298)  loss_giou_dn_5: 0.8388 (0.9325)  loss_fgl_dn_5: 1.0520 (1.0307)  loss_vfl_dn_pre: 0.4387 (0.4209)  loss_bbox_dn_pre: 0.1313 (0.1669)  loss_giou_dn_pre: 0.9972 (1.0576)  time: 0.6445  data: 0.0090  max mem: 3024\n",
            "Epoch: [4]  [499/500]  eta: 0:00:00  lr: 0.000013  loss: 37.7300 (38.4598)  loss_vfl: 0.5747 (0.6087)  loss_bbox: 0.1452 (0.1713)  loss_giou: 0.9882 (1.0319)  loss_fgl: 0.9087 (0.9328)  loss_vfl_aux_0: 0.5449 (0.5822)  loss_bbox_aux_0: 0.1478 (0.1903)  loss_giou_aux_0: 1.0480 (1.0765)  loss_fgl_aux_0: 0.9363 (0.9339)  loss_ddf_aux_0: 0.1780 (0.2145)  loss_vfl_aux_1: 0.5283 (0.6021)  loss_bbox_aux_1: 0.1445 (0.1750)  loss_giou_aux_1: 0.9940 (1.0415)  loss_fgl_aux_1: 0.9151 (0.9317)  loss_ddf_aux_1: 0.0283 (0.0377)  loss_vfl_aux_2: 0.5562 (0.6080)  loss_bbox_aux_2: 0.1470 (0.1719)  loss_giou_aux_2: 0.9895 (1.0333)  loss_fgl_aux_2: 0.9102 (0.9320)  loss_ddf_aux_2: 0.0036 (0.0055)  loss_vfl_aux_3: 0.5605 (0.6058)  loss_bbox_aux_3: 0.1459 (0.1715)  loss_giou_aux_3: 0.9880 (1.0321)  loss_fgl_aux_3: 0.9089 (0.9325)  loss_ddf_aux_3: 0.0007 (0.0010)  loss_vfl_aux_4: 0.5684 (0.6040)  loss_bbox_aux_4: 0.1452 (0.1714)  loss_giou_aux_4: 0.9881 (1.0319)  loss_fgl_aux_4: 0.9091 (0.9328)  loss_ddf_aux_4: 0.0001 (0.0001)  loss_vfl_pre: 0.5396 (0.5792)  loss_bbox_pre: 0.1491 (0.1913)  loss_giou_pre: 1.0488 (1.0751)  loss_vfl_enc_0: 0.5356 (0.5595)  loss_bbox_enc_0: 0.2031 (0.2487)  loss_giou_enc_0: 1.1352 (1.2010)  loss_vfl_dn_0: 0.4272 (0.4223)  loss_bbox_dn_0: 0.1306 (0.1697)  loss_giou_dn_0: 1.0074 (1.0520)  loss_fgl_dn_0: 1.0153 (1.0099)  loss_ddf_dn_0: 0.6100 (0.6323)  loss_vfl_dn_1: 0.4504 (0.4531)  loss_bbox_dn_1: 0.1034 (0.1429)  loss_giou_dn_1: 0.9135 (0.9558)  loss_fgl_dn_1: 1.0244 (1.0265)  loss_ddf_dn_1: 0.0950 (0.1116)  loss_vfl_dn_2: 0.4521 (0.4534)  loss_bbox_dn_2: 0.1007 (0.1353)  loss_giou_dn_2: 0.9011 (0.9350)  loss_fgl_dn_2: 1.0250 (1.0306)  loss_ddf_dn_2: 0.0148 (0.0191)  loss_vfl_dn_3: 0.4558 (0.4526)  loss_bbox_dn_3: 0.0992 (0.1333)  loss_giou_dn_3: 0.9036 (0.9294)  loss_fgl_dn_3: 1.0287 (1.0325)  loss_ddf_dn_3: 0.0020 (0.0035)  loss_vfl_dn_4: 0.4548 (0.4520)  loss_bbox_dn_4: 0.0984 (0.1325)  loss_giou_dn_4: 0.9011 (0.9278)  loss_fgl_dn_4: 1.0290 (1.0333)  loss_ddf_dn_4: 0.0002 (0.0004)  loss_vfl_dn_5: 0.4551 (0.4542)  loss_bbox_dn_5: 0.0981 (0.1324)  loss_giou_dn_5: 0.9003 (0.9275)  loss_fgl_dn_5: 1.0292 (1.0336)  loss_vfl_dn_pre: 0.4314 (0.4213)  loss_bbox_dn_pre: 0.1327 (0.1711)  loss_giou_dn_pre: 1.0069 (1.0542)  time: 0.6154  data: 0.0093  max mem: 3024\n",
            "Epoch: [4] Total time: 0:05:21 (0.6422 s / it)\n",
            "Averaged stats: lr: 0.000013  loss: 37.7300 (38.4598)  loss_vfl: 0.5747 (0.6087)  loss_bbox: 0.1452 (0.1713)  loss_giou: 0.9882 (1.0319)  loss_fgl: 0.9087 (0.9328)  loss_vfl_aux_0: 0.5449 (0.5822)  loss_bbox_aux_0: 0.1478 (0.1903)  loss_giou_aux_0: 1.0480 (1.0765)  loss_fgl_aux_0: 0.9363 (0.9339)  loss_ddf_aux_0: 0.1780 (0.2145)  loss_vfl_aux_1: 0.5283 (0.6021)  loss_bbox_aux_1: 0.1445 (0.1750)  loss_giou_aux_1: 0.9940 (1.0415)  loss_fgl_aux_1: 0.9151 (0.9317)  loss_ddf_aux_1: 0.0283 (0.0377)  loss_vfl_aux_2: 0.5562 (0.6080)  loss_bbox_aux_2: 0.1470 (0.1719)  loss_giou_aux_2: 0.9895 (1.0333)  loss_fgl_aux_2: 0.9102 (0.9320)  loss_ddf_aux_2: 0.0036 (0.0055)  loss_vfl_aux_3: 0.5605 (0.6058)  loss_bbox_aux_3: 0.1459 (0.1715)  loss_giou_aux_3: 0.9880 (1.0321)  loss_fgl_aux_3: 0.9089 (0.9325)  loss_ddf_aux_3: 0.0007 (0.0010)  loss_vfl_aux_4: 0.5684 (0.6040)  loss_bbox_aux_4: 0.1452 (0.1714)  loss_giou_aux_4: 0.9881 (1.0319)  loss_fgl_aux_4: 0.9091 (0.9328)  loss_ddf_aux_4: 0.0001 (0.0001)  loss_vfl_pre: 0.5396 (0.5792)  loss_bbox_pre: 0.1491 (0.1913)  loss_giou_pre: 1.0488 (1.0751)  loss_vfl_enc_0: 0.5356 (0.5595)  loss_bbox_enc_0: 0.2031 (0.2487)  loss_giou_enc_0: 1.1352 (1.2010)  loss_vfl_dn_0: 0.4272 (0.4223)  loss_bbox_dn_0: 0.1306 (0.1697)  loss_giou_dn_0: 1.0074 (1.0520)  loss_fgl_dn_0: 1.0153 (1.0099)  loss_ddf_dn_0: 0.6100 (0.6323)  loss_vfl_dn_1: 0.4504 (0.4531)  loss_bbox_dn_1: 0.1034 (0.1429)  loss_giou_dn_1: 0.9135 (0.9558)  loss_fgl_dn_1: 1.0244 (1.0265)  loss_ddf_dn_1: 0.0950 (0.1116)  loss_vfl_dn_2: 0.4521 (0.4534)  loss_bbox_dn_2: 0.1007 (0.1353)  loss_giou_dn_2: 0.9011 (0.9350)  loss_fgl_dn_2: 1.0250 (1.0306)  loss_ddf_dn_2: 0.0148 (0.0191)  loss_vfl_dn_3: 0.4558 (0.4526)  loss_bbox_dn_3: 0.0992 (0.1333)  loss_giou_dn_3: 0.9036 (0.9294)  loss_fgl_dn_3: 1.0287 (1.0325)  loss_ddf_dn_3: 0.0020 (0.0035)  loss_vfl_dn_4: 0.4548 (0.4520)  loss_bbox_dn_4: 0.0984 (0.1325)  loss_giou_dn_4: 0.9011 (0.9278)  loss_fgl_dn_4: 1.0290 (1.0333)  loss_ddf_dn_4: 0.0002 (0.0004)  loss_vfl_dn_5: 0.4551 (0.4542)  loss_bbox_dn_5: 0.0981 (0.1324)  loss_giou_dn_5: 0.9003 (0.9275)  loss_fgl_dn_5: 1.0292 (1.0336)  loss_vfl_dn_pre: 0.4314 (0.4213)  loss_bbox_dn_pre: 0.1327 (0.1711)  loss_giou_dn_pre: 1.0069 (1.0542)\n",
            "Test:  [  0/250]  eta: 0:05:26    time: 1.3056  data: 0.9813  max mem: 3024\n",
            "Test:  [ 10/250]  eta: 0:01:10    time: 0.2951  data: 0.1048  max mem: 3024\n",
            "Test:  [ 20/250]  eta: 0:01:01    time: 0.2177  data: 0.0193  max mem: 3024\n",
            "Test:  [ 30/250]  eta: 0:00:59    time: 0.2551  data: 0.0373  max mem: 3024\n",
            "Test:  [ 40/250]  eta: 0:00:56    time: 0.2670  data: 0.0480  max mem: 3024\n",
            "Test:  [ 50/250]  eta: 0:00:52    time: 0.2525  data: 0.0391  max mem: 3024\n",
            "Test:  [ 60/250]  eta: 0:00:47    time: 0.2195  data: 0.0281  max mem: 3024\n",
            "Test:  [ 70/250]  eta: 0:00:44    time: 0.2020  data: 0.0220  max mem: 3024\n",
            "Test:  [ 80/250]  eta: 0:00:41    time: 0.2211  data: 0.0237  max mem: 3024\n",
            "Test:  [ 90/250]  eta: 0:00:38    time: 0.2222  data: 0.0241  max mem: 3024\n",
            "Test:  [100/250]  eta: 0:00:36    time: 0.2247  data: 0.0291  max mem: 3024\n",
            "Test:  [110/250]  eta: 0:00:34    time: 0.2545  data: 0.0371  max mem: 3024\n",
            "Test:  [120/250]  eta: 0:00:32    time: 0.2753  data: 0.0430  max mem: 3024\n",
            "Test:  [130/250]  eta: 0:00:29    time: 0.2446  data: 0.0353  max mem: 3024\n",
            "Test:  [140/250]  eta: 0:00:26    time: 0.2019  data: 0.0228  max mem: 3024\n",
            "Test:  [150/250]  eta: 0:00:23    time: 0.2148  data: 0.0214  max mem: 3024\n",
            "Test:  [160/250]  eta: 0:00:21    time: 0.2150  data: 0.0214  max mem: 3024\n",
            "Test:  [170/250]  eta: 0:00:18    time: 0.2004  data: 0.0217  max mem: 3024\n",
            "Test:  [180/250]  eta: 0:00:16    time: 0.2331  data: 0.0308  max mem: 3024\n",
            "Test:  [190/250]  eta: 0:00:14    time: 0.2658  data: 0.0405  max mem: 3024\n",
            "Test:  [200/250]  eta: 0:00:11    time: 0.2681  data: 0.0439  max mem: 3024\n",
            "Test:  [210/250]  eta: 0:00:09    time: 0.2531  data: 0.0343  max mem: 3024\n",
            "Test:  [220/250]  eta: 0:00:07    time: 0.2172  data: 0.0215  max mem: 3024\n",
            "Test:  [230/250]  eta: 0:00:04    time: 0.1977  data: 0.0208  max mem: 3024\n",
            "Test:  [240/250]  eta: 0:00:02    time: 0.1983  data: 0.0212  max mem: 3024\n",
            "Test:  [249/250]  eta: 0:00:00    time: 0.1993  data: 0.0211  max mem: 3024\n",
            "Test: Total time: 0:00:58 (0.2344 s / it)\n",
            "Averaged stats: \n",
            "Accumulating evaluation results...\n",
            "COCOeval_opt.accumulate() finished...\n",
            "DONE (t=1.28s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.080\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.157\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.074\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.036\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.098\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.209\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.094\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.203\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.262\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.132\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.319\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.472\n",
            " Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.504\n",
            " Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.231\n",
            "best_stat: {'epoch': 4, 'coco_eval_bbox': 0.07969418213856418}\n",
            "Epoch: [5]  [  0/500]  eta: 0:21:00  lr: 0.000013  loss: 38.4201 (38.4201)  loss_vfl: 0.6426 (0.6426)  loss_bbox: 0.1688 (0.1688)  loss_giou: 0.8832 (0.8832)  loss_fgl: 0.9721 (0.9721)  loss_vfl_aux_0: 0.6382 (0.6382)  loss_bbox_aux_0: 0.1825 (0.1825)  loss_giou_aux_0: 0.9602 (0.9602)  loss_fgl_aux_0: 0.9716 (0.9716)  loss_ddf_aux_0: 0.2461 (0.2461)  loss_vfl_aux_1: 0.6313 (0.6313)  loss_bbox_aux_1: 0.1760 (0.1760)  loss_giou_aux_1: 0.8861 (0.8861)  loss_fgl_aux_1: 0.9820 (0.9820)  loss_ddf_aux_1: 0.0450 (0.0450)  loss_vfl_aux_2: 0.6089 (0.6089)  loss_bbox_aux_2: 0.1721 (0.1721)  loss_giou_aux_2: 0.8819 (0.8819)  loss_fgl_aux_2: 0.9751 (0.9751)  loss_ddf_aux_2: 0.0102 (0.0102)  loss_vfl_aux_3: 0.6973 (0.6973)  loss_bbox_aux_3: 0.1688 (0.1688)  loss_giou_aux_3: 0.8817 (0.8817)  loss_fgl_aux_3: 0.9734 (0.9734)  loss_ddf_aux_3: 0.0011 (0.0011)  loss_vfl_aux_4: 0.6484 (0.6484)  loss_bbox_aux_4: 0.1688 (0.1688)  loss_giou_aux_4: 0.8830 (0.8830)  loss_fgl_aux_4: 0.9722 (0.9722)  loss_ddf_aux_4: 0.0003 (0.0003)  loss_vfl_pre: 0.6255 (0.6255)  loss_bbox_pre: 0.1856 (0.1856)  loss_giou_pre: 0.9742 (0.9742)  loss_vfl_enc_0: 0.6099 (0.6099)  loss_bbox_enc_0: 0.3095 (0.3095)  loss_giou_enc_0: 1.2245 (1.2245)  loss_vfl_dn_0: 0.4409 (0.4409)  loss_bbox_dn_0: 0.2001 (0.2001)  loss_giou_dn_0: 1.0759 (1.0759)  loss_fgl_dn_0: 1.0123 (1.0123)  loss_ddf_dn_0: 0.4940 (0.4940)  loss_vfl_dn_1: 0.4609 (0.4609)  loss_bbox_dn_1: 0.1742 (0.1742)  loss_giou_dn_1: 0.9745 (0.9745)  loss_fgl_dn_1: 1.0266 (1.0266)  loss_ddf_dn_1: 0.0859 (0.0859)  loss_vfl_dn_2: 0.4614 (0.4614)  loss_bbox_dn_2: 0.1666 (0.1666)  loss_giou_dn_2: 0.9519 (0.9519)  loss_fgl_dn_2: 1.0244 (1.0244)  loss_ddf_dn_2: 0.0170 (0.0170)  loss_vfl_dn_3: 0.4575 (0.4575)  loss_bbox_dn_3: 0.1685 (0.1685)  loss_giou_dn_3: 0.9455 (0.9455)  loss_fgl_dn_3: 1.0238 (1.0238)  loss_ddf_dn_3: 0.0023 (0.0023)  loss_vfl_dn_4: 0.4509 (0.4509)  loss_bbox_dn_4: 0.1703 (0.1703)  loss_giou_dn_4: 0.9465 (0.9465)  loss_fgl_dn_4: 1.0222 (1.0222)  loss_ddf_dn_4: 0.0001 (0.0001)  loss_vfl_dn_5: 0.4448 (0.4448)  loss_bbox_dn_5: 0.1743 (0.1743)  loss_giou_dn_5: 0.9487 (0.9487)  loss_fgl_dn_5: 1.0206 (1.0206)  loss_vfl_dn_pre: 0.4365 (0.4365)  loss_bbox_dn_pre: 0.1981 (0.1981)  loss_giou_dn_pre: 1.0849 (1.0849)  time: 2.5208  data: 1.1580  max mem: 3024\n",
            "Epoch: [5]  [100/500]  eta: 0:04:29  lr: 0.000013  loss: 37.3886 (37.7886)  loss_vfl: 0.5771 (0.5915)  loss_bbox: 0.1229 (0.1499)  loss_giou: 0.9958 (1.0159)  loss_fgl: 0.9661 (0.9403)  loss_vfl_aux_0: 0.5586 (0.5758)  loss_bbox_aux_0: 0.1617 (0.1672)  loss_giou_aux_0: 1.0341 (1.0575)  loss_fgl_aux_0: 0.9748 (0.9444)  loss_ddf_aux_0: 0.1507 (0.1694)  loss_vfl_aux_1: 0.5933 (0.5971)  loss_bbox_aux_1: 0.1370 (0.1533)  loss_giou_aux_1: 1.0110 (1.0228)  loss_fgl_aux_1: 0.9727 (0.9422)  loss_ddf_aux_1: 0.0304 (0.0295)  loss_vfl_aux_2: 0.5688 (0.5934)  loss_bbox_aux_2: 0.1299 (0.1503)  loss_giou_aux_2: 0.9990 (1.0167)  loss_fgl_aux_2: 0.9673 (0.9404)  loss_ddf_aux_2: 0.0035 (0.0039)  loss_vfl_aux_3: 0.5806 (0.5900)  loss_bbox_aux_3: 0.1257 (0.1502)  loss_giou_aux_3: 0.9963 (1.0162)  loss_fgl_aux_3: 0.9666 (0.9402)  loss_ddf_aux_3: 0.0006 (0.0006)  loss_vfl_aux_4: 0.5732 (0.5887)  loss_bbox_aux_4: 0.1236 (0.1500)  loss_giou_aux_4: 0.9958 (1.0159)  loss_fgl_aux_4: 0.9662 (0.9403)  loss_ddf_aux_4: 0.0001 (0.0001)  loss_vfl_pre: 0.5620 (0.5738)  loss_bbox_pre: 0.1630 (0.1682)  loss_giou_pre: 1.0323 (1.0557)  loss_vfl_enc_0: 0.5625 (0.5574)  loss_bbox_enc_0: 0.2419 (0.2256)  loss_giou_enc_0: 1.1766 (1.1798)  loss_vfl_dn_0: 0.4204 (0.4228)  loss_bbox_dn_0: 0.1603 (0.1583)  loss_giou_dn_0: 1.0140 (1.0452)  loss_fgl_dn_0: 1.0144 (1.0130)  loss_ddf_dn_0: 0.5002 (0.5605)  loss_vfl_dn_1: 0.4512 (0.4525)  loss_bbox_dn_1: 0.1293 (0.1313)  loss_giou_dn_1: 0.8983 (0.9418)  loss_fgl_dn_1: 1.0211 (1.0283)  loss_ddf_dn_1: 0.1000 (0.0937)  loss_vfl_dn_2: 0.4470 (0.4513)  loss_bbox_dn_2: 0.1183 (0.1231)  loss_giou_dn_2: 0.8690 (0.9200)  loss_fgl_dn_2: 1.0279 (1.0308)  loss_ddf_dn_2: 0.0156 (0.0143)  loss_vfl_dn_3: 0.4480 (0.4499)  loss_bbox_dn_3: 0.1137 (0.1213)  loss_giou_dn_3: 0.8615 (0.9155)  loss_fgl_dn_3: 1.0262 (1.0325)  loss_ddf_dn_3: 0.0020 (0.0021)  loss_vfl_dn_4: 0.4478 (0.4506)  loss_bbox_dn_4: 0.1138 (0.1208)  loss_giou_dn_4: 0.8655 (0.9142)  loss_fgl_dn_4: 1.0271 (1.0332)  loss_ddf_dn_4: 0.0002 (0.0002)  loss_vfl_dn_5: 0.4478 (0.4524)  loss_bbox_dn_5: 0.1137 (0.1208)  loss_giou_dn_5: 0.8673 (0.9140)  loss_fgl_dn_5: 1.0273 (1.0333)  loss_vfl_dn_pre: 0.4241 (0.4223)  loss_bbox_dn_pre: 0.1588 (0.1583)  loss_giou_dn_pre: 0.9885 (1.0461)  time: 0.6700  data: 0.0077  max mem: 3024\n",
            "Epoch: [5]  [200/500]  eta: 0:03:21  lr: 0.000013  loss: 37.3754 (38.1108)  loss_vfl: 0.5713 (0.5998)  loss_bbox: 0.1446 (0.1663)  loss_giou: 0.9835 (1.0291)  loss_fgl: 0.9540 (0.9339)  loss_vfl_aux_0: 0.5503 (0.5884)  loss_bbox_aux_0: 0.1562 (0.1818)  loss_giou_aux_0: 1.0104 (1.0701)  loss_fgl_aux_0: 0.9544 (0.9359)  loss_ddf_aux_0: 0.2308 (0.1819)  loss_vfl_aux_1: 0.5698 (0.6028)  loss_bbox_aux_1: 0.1488 (0.1688)  loss_giou_aux_1: 0.9781 (1.0353)  loss_fgl_aux_1: 0.9555 (0.9344)  loss_ddf_aux_1: 0.0360 (0.0308)  loss_vfl_aux_2: 0.5835 (0.6010)  loss_bbox_aux_2: 0.1453 (0.1665)  loss_giou_aux_2: 0.9814 (1.0298)  loss_fgl_aux_2: 0.9529 (0.9335)  loss_ddf_aux_2: 0.0042 (0.0039)  loss_vfl_aux_3: 0.5801 (0.6007)  loss_bbox_aux_3: 0.1448 (0.1664)  loss_giou_aux_3: 0.9821 (1.0293)  loss_fgl_aux_3: 0.9527 (0.9337)  loss_ddf_aux_3: 0.0006 (0.0006)  loss_vfl_aux_4: 0.5513 (0.5983)  loss_bbox_aux_4: 0.1446 (0.1664)  loss_giou_aux_4: 0.9832 (1.0291)  loss_fgl_aux_4: 0.9534 (0.9339)  loss_ddf_aux_4: 0.0001 (0.0001)  loss_vfl_pre: 0.5605 (0.5850)  loss_bbox_pre: 0.1562 (0.1827)  loss_giou_pre: 1.0045 (1.0687)  loss_vfl_enc_0: 0.5225 (0.5672)  loss_bbox_enc_0: 0.1897 (0.2371)  loss_giou_enc_0: 1.1470 (1.1889)  loss_vfl_dn_0: 0.4312 (0.4224)  loss_bbox_dn_0: 0.1256 (0.1629)  loss_giou_dn_0: 0.9984 (1.0433)  loss_fgl_dn_0: 1.0283 (1.0142)  loss_ddf_dn_0: 0.6138 (0.5775)  loss_vfl_dn_1: 0.4624 (0.4531)  loss_bbox_dn_1: 0.1010 (0.1355)  loss_giou_dn_1: 0.8898 (0.9390)  loss_fgl_dn_1: 1.0417 (1.0305)  loss_ddf_dn_1: 0.0958 (0.0939)  loss_vfl_dn_2: 0.4622 (0.4517)  loss_bbox_dn_2: 0.0950 (0.1279)  loss_giou_dn_2: 0.8796 (0.9191)  loss_fgl_dn_2: 1.0403 (1.0337)  loss_ddf_dn_2: 0.0148 (0.0145)  loss_vfl_dn_3: 0.4561 (0.4501)  loss_bbox_dn_3: 0.0939 (0.1262)  loss_giou_dn_3: 0.8785 (0.9148)  loss_fgl_dn_3: 1.0467 (1.0352)  loss_ddf_dn_3: 0.0019 (0.0022)  loss_vfl_dn_4: 0.4583 (0.4499)  loss_bbox_dn_4: 0.0933 (0.1258)  loss_giou_dn_4: 0.8787 (0.9136)  loss_fgl_dn_4: 1.0471 (1.0357)  loss_ddf_dn_4: 0.0003 (0.0002)  loss_vfl_dn_5: 0.4561 (0.4515)  loss_bbox_dn_5: 0.0932 (0.1257)  loss_giou_dn_5: 0.8783 (0.9134)  loss_fgl_dn_5: 1.0477 (1.0359)  loss_vfl_dn_pre: 0.4209 (0.4220)  loss_bbox_dn_pre: 0.1302 (0.1630)  loss_giou_dn_pre: 0.9932 (1.0444)  time: 0.6681  data: 0.0084  max mem: 3024\n",
            "Epoch: [5]  [300/500]  eta: 0:02:12  lr: 0.000013  loss: 37.5478 (38.0397)  loss_vfl: 0.5186 (0.6056)  loss_bbox: 0.1579 (0.1625)  loss_giou: 1.0157 (1.0134)  loss_fgl: 0.9315 (0.9443)  loss_vfl_aux_0: 0.5190 (0.5964)  loss_bbox_aux_0: 0.1682 (0.1785)  loss_giou_aux_0: 1.0658 (1.0533)  loss_fgl_aux_0: 0.9217 (0.9478)  loss_ddf_aux_0: 0.1876 (0.1763)  loss_vfl_aux_1: 0.5264 (0.6086)  loss_bbox_aux_1: 0.1590 (0.1651)  loss_giou_aux_1: 1.0258 (1.0198)  loss_fgl_aux_1: 0.9290 (0.9446)  loss_ddf_aux_1: 0.0345 (0.0296)  loss_vfl_aux_2: 0.5249 (0.6060)  loss_bbox_aux_2: 0.1552 (0.1628)  loss_giou_aux_2: 1.0197 (1.0143)  loss_fgl_aux_2: 0.9308 (0.9437)  loss_ddf_aux_2: 0.0036 (0.0039)  loss_vfl_aux_3: 0.5376 (0.6042)  loss_bbox_aux_3: 0.1554 (0.1626)  loss_giou_aux_3: 1.0174 (1.0136)  loss_fgl_aux_3: 0.9308 (0.9440)  loss_ddf_aux_3: 0.0009 (0.0007)  loss_vfl_aux_4: 0.5195 (0.6028)  loss_bbox_aux_4: 0.1575 (0.1625)  loss_giou_aux_4: 1.0161 (1.0134)  loss_fgl_aux_4: 0.9308 (0.9442)  loss_ddf_aux_4: 0.0001 (0.0001)  loss_vfl_pre: 0.5039 (0.5928)  loss_bbox_pre: 0.1676 (0.1793)  loss_giou_pre: 1.0836 (1.0517)  loss_vfl_enc_0: 0.5229 (0.5746)  loss_bbox_enc_0: 0.2017 (0.2339)  loss_giou_enc_0: 1.1993 (1.1721)  loss_vfl_dn_0: 0.4155 (0.4235)  loss_bbox_dn_0: 0.1301 (0.1628)  loss_giou_dn_0: 1.0417 (1.0390)  loss_fgl_dn_0: 0.9954 (1.0162)  loss_ddf_dn_0: 0.5515 (0.5663)  loss_vfl_dn_1: 0.4495 (0.4543)  loss_bbox_dn_1: 0.1055 (0.1352)  loss_giou_dn_1: 0.9327 (0.9357)  loss_fgl_dn_1: 1.0102 (1.0321)  loss_ddf_dn_1: 0.0922 (0.0915)  loss_vfl_dn_2: 0.4460 (0.4529)  loss_bbox_dn_2: 0.1011 (0.1279)  loss_giou_dn_2: 0.9033 (0.9157)  loss_fgl_dn_2: 1.0097 (1.0353)  loss_ddf_dn_2: 0.0135 (0.0144)  loss_vfl_dn_3: 0.4473 (0.4524)  loss_bbox_dn_3: 0.0969 (0.1262)  loss_giou_dn_3: 0.9085 (0.9111)  loss_fgl_dn_3: 1.0112 (1.0369)  loss_ddf_dn_3: 0.0025 (0.0023)  loss_vfl_dn_4: 0.4441 (0.4525)  loss_bbox_dn_4: 0.0941 (0.1257)  loss_giou_dn_4: 0.9047 (0.9099)  loss_fgl_dn_4: 1.0116 (1.0375)  loss_ddf_dn_4: 0.0004 (0.0002)  loss_vfl_dn_5: 0.4465 (0.4539)  loss_bbox_dn_5: 0.0936 (0.1255)  loss_giou_dn_5: 0.9036 (0.9097)  loss_fgl_dn_5: 1.0123 (1.0377)  loss_vfl_dn_pre: 0.4155 (0.4230)  loss_bbox_dn_pre: 0.1282 (0.1632)  loss_giou_dn_pre: 1.0529 (1.0398)  time: 0.6716  data: 0.0089  max mem: 3024\n",
            "Epoch: [5]  [400/500]  eta: 0:01:05  lr: 0.000013  loss: 37.1914 (37.9819)  loss_vfl: 0.6665 (0.6059)  loss_bbox: 0.1205 (0.1593)  loss_giou: 0.9218 (1.0089)  loss_fgl: 0.9861 (0.9461)  loss_vfl_aux_0: 0.6177 (0.5962)  loss_bbox_aux_0: 0.1437 (0.1754)  loss_giou_aux_0: 0.9568 (1.0486)  loss_fgl_aux_0: 1.0062 (0.9510)  loss_ddf_aux_0: 0.1660 (0.1830)  loss_vfl_aux_1: 0.6348 (0.6060)  loss_bbox_aux_1: 0.1219 (0.1619)  loss_giou_aux_1: 0.9210 (1.0153)  loss_fgl_aux_1: 0.9952 (0.9468)  loss_ddf_aux_1: 0.0229 (0.0300)  loss_vfl_aux_2: 0.6396 (0.6053)  loss_bbox_aux_2: 0.1178 (0.1596)  loss_giou_aux_2: 0.9235 (1.0099)  loss_fgl_aux_2: 0.9889 (0.9456)  loss_ddf_aux_2: 0.0024 (0.0040)  loss_vfl_aux_3: 0.6465 (0.6036)  loss_bbox_aux_3: 0.1179 (0.1594)  loss_giou_aux_3: 0.9227 (1.0092)  loss_fgl_aux_3: 0.9872 (0.9458)  loss_ddf_aux_3: 0.0004 (0.0007)  loss_vfl_aux_4: 0.6719 (0.6034)  loss_bbox_aux_4: 0.1205 (0.1593)  loss_giou_aux_4: 0.9218 (1.0089)  loss_fgl_aux_4: 0.9865 (0.9460)  loss_ddf_aux_4: 0.0001 (0.0001)  loss_vfl_pre: 0.6328 (0.5926)  loss_bbox_pre: 0.1447 (0.1761)  loss_giou_pre: 0.9614 (1.0470)  loss_vfl_enc_0: 0.6191 (0.5774)  loss_bbox_enc_0: 0.2143 (0.2295)  loss_giou_enc_0: 1.1190 (1.1653)  loss_vfl_dn_0: 0.4189 (0.4243)  loss_bbox_dn_0: 0.1179 (0.1626)  loss_giou_dn_0: 1.0058 (1.0360)  loss_fgl_dn_0: 1.0239 (1.0184)  loss_ddf_dn_0: 0.4080 (0.5671)  loss_vfl_dn_1: 0.4451 (0.4538)  loss_bbox_dn_1: 0.0983 (0.1346)  loss_giou_dn_1: 0.9196 (0.9338)  loss_fgl_dn_1: 1.0317 (1.0331)  loss_ddf_dn_1: 0.0580 (0.0913)  loss_vfl_dn_2: 0.4490 (0.4528)  loss_bbox_dn_2: 0.0956 (0.1274)  loss_giou_dn_2: 0.8990 (0.9138)  loss_fgl_dn_2: 1.0307 (1.0360)  loss_ddf_dn_2: 0.0073 (0.0146)  loss_vfl_dn_3: 0.4441 (0.4522)  loss_bbox_dn_3: 0.0945 (0.1257)  loss_giou_dn_3: 0.8936 (0.9093)  loss_fgl_dn_3: 1.0290 (1.0375)  loss_ddf_dn_3: 0.0010 (0.0024)  loss_vfl_dn_4: 0.4446 (0.4521)  loss_bbox_dn_4: 0.0944 (0.1252)  loss_giou_dn_4: 0.8929 (0.9082)  loss_fgl_dn_4: 1.0292 (1.0380)  loss_ddf_dn_4: 0.0001 (0.0003)  loss_vfl_dn_5: 0.4480 (0.4535)  loss_bbox_dn_5: 0.0945 (0.1251)  loss_giou_dn_5: 0.8933 (0.9080)  loss_fgl_dn_5: 1.0298 (1.0383)  loss_vfl_dn_pre: 0.4204 (0.4241)  loss_bbox_dn_pre: 0.1184 (0.1627)  loss_giou_dn_pre: 1.0173 (1.0366)  time: 0.6258  data: 0.0083  max mem: 3024\n",
            "Epoch: [5]  [499/500]  eta: 0:00:00  lr: 0.000013  loss: 37.5767 (37.9407)  loss_vfl: 0.5981 (0.6082)  loss_bbox: 0.1376 (0.1569)  loss_giou: 0.8856 (1.0057)  loss_fgl: 0.9782 (0.9468)  loss_vfl_aux_0: 0.6265 (0.5967)  loss_bbox_aux_0: 0.1491 (0.1730)  loss_giou_aux_0: 0.8700 (1.0461)  loss_fgl_aux_0: 0.9796 (0.9521)  loss_ddf_aux_0: 0.2007 (0.1868)  loss_vfl_aux_1: 0.5796 (0.6074)  loss_bbox_aux_1: 0.1402 (0.1595)  loss_giou_aux_1: 0.8941 (1.0124)  loss_fgl_aux_1: 0.9798 (0.9476)  loss_ddf_aux_1: 0.0298 (0.0303)  loss_vfl_aux_2: 0.6206 (0.6077)  loss_bbox_aux_2: 0.1400 (0.1573)  loss_giou_aux_2: 0.8891 (1.0067)  loss_fgl_aux_2: 0.9774 (0.9465)  loss_ddf_aux_2: 0.0037 (0.0040)  loss_vfl_aux_3: 0.5981 (0.6051)  loss_bbox_aux_3: 0.1382 (0.1570)  loss_giou_aux_3: 0.8870 (1.0059)  loss_fgl_aux_3: 0.9774 (0.9466)  loss_ddf_aux_3: 0.0007 (0.0007)  loss_vfl_aux_4: 0.5947 (0.6050)  loss_bbox_aux_4: 0.1377 (0.1569)  loss_giou_aux_4: 0.8857 (1.0057)  loss_fgl_aux_4: 0.9780 (0.9467)  loss_ddf_aux_4: 0.0001 (0.0001)  loss_vfl_pre: 0.6245 (0.5930)  loss_bbox_pre: 0.1537 (0.1739)  loss_giou_pre: 0.8604 (1.0447)  loss_vfl_enc_0: 0.6338 (0.5765)  loss_bbox_enc_0: 0.1887 (0.2264)  loss_giou_enc_0: 1.0498 (1.1632)  loss_vfl_dn_0: 0.4246 (0.4257)  loss_bbox_dn_0: 0.1360 (0.1605)  loss_giou_dn_0: 0.9557 (1.0312)  loss_fgl_dn_0: 1.0638 (1.0213)  loss_ddf_dn_0: 0.5985 (0.5718)  loss_vfl_dn_1: 0.4595 (0.4552)  loss_bbox_dn_1: 0.1081 (0.1329)  loss_giou_dn_1: 0.8338 (0.9290)  loss_fgl_dn_1: 1.0721 (1.0353)  loss_ddf_dn_1: 0.0813 (0.0915)  loss_vfl_dn_2: 0.4658 (0.4541)  loss_bbox_dn_2: 0.0983 (0.1260)  loss_giou_dn_2: 0.8214 (0.9094)  loss_fgl_dn_2: 1.0670 (1.0378)  loss_ddf_dn_2: 0.0123 (0.0143)  loss_vfl_dn_3: 0.4624 (0.4536)  loss_bbox_dn_3: 0.0972 (0.1244)  loss_giou_dn_3: 0.8166 (0.9050)  loss_fgl_dn_3: 1.0671 (1.0393)  loss_ddf_dn_3: 0.0019 (0.0023)  loss_vfl_dn_4: 0.4646 (0.4535)  loss_bbox_dn_4: 0.0975 (0.1238)  loss_giou_dn_4: 0.8146 (0.9038)  loss_fgl_dn_4: 1.0683 (1.0398)  loss_ddf_dn_4: 0.0003 (0.0003)  loss_vfl_dn_5: 0.4651 (0.4549)  loss_bbox_dn_5: 0.0974 (0.1237)  loss_giou_dn_5: 0.8149 (0.9037)  loss_fgl_dn_5: 1.0682 (1.0400)  loss_vfl_dn_pre: 0.4238 (0.4255)  loss_bbox_dn_pre: 0.1368 (0.1608)  loss_giou_dn_pre: 0.9587 (1.0316)  time: 0.5416  data: 0.0076  max mem: 3024\n",
            "Epoch: [5] Total time: 0:05:24 (0.6489 s / it)\n",
            "Averaged stats: lr: 0.000013  loss: 37.5767 (37.9407)  loss_vfl: 0.5981 (0.6082)  loss_bbox: 0.1376 (0.1569)  loss_giou: 0.8856 (1.0057)  loss_fgl: 0.9782 (0.9468)  loss_vfl_aux_0: 0.6265 (0.5967)  loss_bbox_aux_0: 0.1491 (0.1730)  loss_giou_aux_0: 0.8700 (1.0461)  loss_fgl_aux_0: 0.9796 (0.9521)  loss_ddf_aux_0: 0.2007 (0.1868)  loss_vfl_aux_1: 0.5796 (0.6074)  loss_bbox_aux_1: 0.1402 (0.1595)  loss_giou_aux_1: 0.8941 (1.0124)  loss_fgl_aux_1: 0.9798 (0.9476)  loss_ddf_aux_1: 0.0298 (0.0303)  loss_vfl_aux_2: 0.6206 (0.6077)  loss_bbox_aux_2: 0.1400 (0.1573)  loss_giou_aux_2: 0.8891 (1.0067)  loss_fgl_aux_2: 0.9774 (0.9465)  loss_ddf_aux_2: 0.0037 (0.0040)  loss_vfl_aux_3: 0.5981 (0.6051)  loss_bbox_aux_3: 0.1382 (0.1570)  loss_giou_aux_3: 0.8870 (1.0059)  loss_fgl_aux_3: 0.9774 (0.9466)  loss_ddf_aux_3: 0.0007 (0.0007)  loss_vfl_aux_4: 0.5947 (0.6050)  loss_bbox_aux_4: 0.1377 (0.1569)  loss_giou_aux_4: 0.8857 (1.0057)  loss_fgl_aux_4: 0.9780 (0.9467)  loss_ddf_aux_4: 0.0001 (0.0001)  loss_vfl_pre: 0.6245 (0.5930)  loss_bbox_pre: 0.1537 (0.1739)  loss_giou_pre: 0.8604 (1.0447)  loss_vfl_enc_0: 0.6338 (0.5765)  loss_bbox_enc_0: 0.1887 (0.2264)  loss_giou_enc_0: 1.0498 (1.1632)  loss_vfl_dn_0: 0.4246 (0.4257)  loss_bbox_dn_0: 0.1360 (0.1605)  loss_giou_dn_0: 0.9557 (1.0312)  loss_fgl_dn_0: 1.0638 (1.0213)  loss_ddf_dn_0: 0.5985 (0.5718)  loss_vfl_dn_1: 0.4595 (0.4552)  loss_bbox_dn_1: 0.1081 (0.1329)  loss_giou_dn_1: 0.8338 (0.9290)  loss_fgl_dn_1: 1.0721 (1.0353)  loss_ddf_dn_1: 0.0813 (0.0915)  loss_vfl_dn_2: 0.4658 (0.4541)  loss_bbox_dn_2: 0.0983 (0.1260)  loss_giou_dn_2: 0.8214 (0.9094)  loss_fgl_dn_2: 1.0670 (1.0378)  loss_ddf_dn_2: 0.0123 (0.0143)  loss_vfl_dn_3: 0.4624 (0.4536)  loss_bbox_dn_3: 0.0972 (0.1244)  loss_giou_dn_3: 0.8166 (0.9050)  loss_fgl_dn_3: 1.0671 (1.0393)  loss_ddf_dn_3: 0.0019 (0.0023)  loss_vfl_dn_4: 0.4646 (0.4535)  loss_bbox_dn_4: 0.0975 (0.1238)  loss_giou_dn_4: 0.8146 (0.9038)  loss_fgl_dn_4: 1.0683 (1.0398)  loss_ddf_dn_4: 0.0003 (0.0003)  loss_vfl_dn_5: 0.4651 (0.4549)  loss_bbox_dn_5: 0.0974 (0.1237)  loss_giou_dn_5: 0.8149 (0.9037)  loss_fgl_dn_5: 1.0682 (1.0400)  loss_vfl_dn_pre: 0.4238 (0.4255)  loss_bbox_dn_pre: 0.1368 (0.1608)  loss_giou_dn_pre: 0.9587 (1.0316)\n",
            "Test:  [  0/250]  eta: 0:07:53    time: 1.8943  data: 1.2612  max mem: 3024\n",
            "Test:  [ 10/250]  eta: 0:01:42    time: 0.4290  data: 0.1565  max mem: 3024\n",
            "Test:  [ 20/250]  eta: 0:01:20    time: 0.2713  data: 0.0426  max mem: 3024\n",
            "Test:  [ 30/250]  eta: 0:01:09    time: 0.2505  data: 0.0301  max mem: 3024\n",
            "Test:  [ 40/250]  eta: 0:00:59    time: 0.2195  data: 0.0207  max mem: 3024\n",
            "Test:  [ 50/250]  eta: 0:00:53    time: 0.1991  data: 0.0215  max mem: 3024\n",
            "Test:  [ 60/250]  eta: 0:00:48    time: 0.2010  data: 0.0225  max mem: 3024\n",
            "Test:  [ 70/250]  eta: 0:00:45    time: 0.2132  data: 0.0233  max mem: 3024\n",
            "Test:  [ 80/250]  eta: 0:00:43    time: 0.2423  data: 0.0334  max mem: 3024\n",
            "Test:  [ 90/250]  eta: 0:00:41    time: 0.2706  data: 0.0441  max mem: 3024\n",
            "Test:  [100/250]  eta: 0:00:38    time: 0.2708  data: 0.0377  max mem: 3024\n",
            "Test:  [110/250]  eta: 0:00:35    time: 0.2293  data: 0.0249  max mem: 3024\n",
            "Test:  [120/250]  eta: 0:00:32    time: 0.1987  data: 0.0207  max mem: 3024\n",
            "Test:  [130/250]  eta: 0:00:29    time: 0.1989  data: 0.0212  max mem: 3024\n",
            "Test:  [140/250]  eta: 0:00:26    time: 0.1984  data: 0.0205  max mem: 3024\n",
            "Test:  [150/250]  eta: 0:00:24    time: 0.2191  data: 0.0259  max mem: 3024\n",
            "Test:  [160/250]  eta: 0:00:21    time: 0.2460  data: 0.0330  max mem: 3024\n",
            "Test:  [170/250]  eta: 0:00:19    time: 0.2811  data: 0.0404  max mem: 3024\n",
            "Test:  [180/250]  eta: 0:00:17    time: 0.2576  data: 0.0353  max mem: 3024\n",
            "Test:  [190/250]  eta: 0:00:14    time: 0.2011  data: 0.0222  max mem: 3024\n",
            "Test:  [200/250]  eta: 0:00:11    time: 0.1990  data: 0.0207  max mem: 3024\n",
            "Test:  [210/250]  eta: 0:00:09    time: 0.1991  data: 0.0209  max mem: 3024\n",
            "Test:  [220/250]  eta: 0:00:07    time: 0.1971  data: 0.0201  max mem: 3024\n",
            "Test:  [230/250]  eta: 0:00:04    time: 0.2480  data: 0.0285  max mem: 3024\n",
            "Test:  [240/250]  eta: 0:00:02    time: 0.2848  data: 0.0395  max mem: 3024\n",
            "Test:  [249/250]  eta: 0:00:00    time: 0.2453  data: 0.0337  max mem: 3024\n",
            "Test: Total time: 0:00:59 (0.2392 s / it)\n",
            "Averaged stats: \n",
            "Accumulating evaluation results...\n",
            "COCOeval_opt.accumulate() finished...\n",
            "DONE (t=1.30s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.085\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.168\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.075\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.039\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.107\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.209\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.103\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.226\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.284\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.145\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.347\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.530\n",
            " Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.544\n",
            " Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.256\n",
            "best_stat: {'epoch': 5, 'coco_eval_bbox': 0.08480202244585941}\n",
            "Epoch: [6]  [  0/500]  eta: 0:20:09  lr: 0.000013  loss: 37.2884 (37.2884)  loss_vfl: 0.5063 (0.5063)  loss_bbox: 0.1155 (0.1155)  loss_giou: 1.0897 (1.0897)  loss_fgl: 0.9181 (0.9181)  loss_vfl_aux_0: 0.5166 (0.5166)  loss_bbox_aux_0: 0.1226 (0.1226)  loss_giou_aux_0: 1.0966 (1.0966)  loss_fgl_aux_0: 0.9102 (0.9102)  loss_ddf_aux_0: 0.1996 (0.1996)  loss_vfl_aux_1: 0.5479 (0.5479)  loss_bbox_aux_1: 0.1172 (0.1172)  loss_giou_aux_1: 1.0918 (1.0918)  loss_fgl_aux_1: 0.9175 (0.9175)  loss_ddf_aux_1: 0.0272 (0.0272)  loss_vfl_aux_2: 0.4893 (0.4893)  loss_bbox_aux_2: 0.1160 (0.1160)  loss_giou_aux_2: 1.0909 (1.0909)  loss_fgl_aux_2: 0.9181 (0.9181)  loss_ddf_aux_2: 0.0040 (0.0040)  loss_vfl_aux_3: 0.5088 (0.5088)  loss_bbox_aux_3: 0.1154 (0.1154)  loss_giou_aux_3: 1.0904 (1.0904)  loss_fgl_aux_3: 0.9177 (0.9177)  loss_ddf_aux_3: 0.0006 (0.0006)  loss_vfl_aux_4: 0.4880 (0.4880)  loss_bbox_aux_4: 0.1155 (0.1155)  loss_giou_aux_4: 1.0898 (1.0898)  loss_fgl_aux_4: 0.9182 (0.9182)  loss_ddf_aux_4: -0.0000 (-0.0000)  loss_vfl_pre: 0.5054 (0.5054)  loss_bbox_pre: 0.1255 (0.1255)  loss_giou_pre: 1.0976 (1.0976)  loss_vfl_enc_0: 0.4878 (0.4878)  loss_bbox_enc_0: 0.1952 (0.1952)  loss_giou_enc_0: 1.2583 (1.2583)  loss_vfl_dn_0: 0.4177 (0.4177)  loss_bbox_dn_0: 0.1016 (0.1016)  loss_giou_dn_0: 1.0784 (1.0784)  loss_fgl_dn_0: 1.0286 (1.0286)  loss_ddf_dn_0: 0.5396 (0.5396)  loss_vfl_dn_1: 0.4456 (0.4456)  loss_bbox_dn_1: 0.0878 (0.0878)  loss_giou_dn_1: 0.9727 (0.9727)  loss_fgl_dn_1: 1.0508 (1.0508)  loss_ddf_dn_1: 0.0874 (0.0874)  loss_vfl_dn_2: 0.4500 (0.4500)  loss_bbox_dn_2: 0.0828 (0.0828)  loss_giou_dn_2: 0.9606 (0.9606)  loss_fgl_dn_2: 1.0520 (1.0520)  loss_ddf_dn_2: 0.0119 (0.0119)  loss_vfl_dn_3: 0.4512 (0.4512)  loss_bbox_dn_3: 0.0822 (0.0822)  loss_giou_dn_3: 0.9555 (0.9555)  loss_fgl_dn_3: 1.0531 (1.0531)  loss_ddf_dn_3: 0.0016 (0.0016)  loss_vfl_dn_4: 0.4500 (0.4500)  loss_bbox_dn_4: 0.0819 (0.0819)  loss_giou_dn_4: 0.9526 (0.9526)  loss_fgl_dn_4: 1.0555 (1.0555)  loss_ddf_dn_4: 0.0002 (0.0002)  loss_vfl_dn_5: 0.4504 (0.4504)  loss_bbox_dn_5: 0.0819 (0.0819)  loss_giou_dn_5: 0.9520 (0.9520)  loss_fgl_dn_5: 1.0555 (1.0555)  loss_vfl_dn_pre: 0.4221 (0.4221)  loss_bbox_dn_pre: 0.0995 (0.0995)  loss_giou_dn_pre: 1.0666 (1.0666)  time: 2.4187  data: 1.1754  max mem: 3024\n",
            "Epoch: [6]  [100/500]  eta: 0:04:23  lr: 0.000013  loss: 37.6188 (37.8389)  loss_vfl: 0.5640 (0.6276)  loss_bbox: 0.1576 (0.1491)  loss_giou: 0.9424 (0.9656)  loss_fgl: 0.9776 (0.9737)  loss_vfl_aux_0: 0.5889 (0.6252)  loss_bbox_aux_0: 0.1718 (0.1625)  loss_giou_aux_0: 0.9899 (1.0057)  loss_fgl_aux_0: 1.0012 (0.9787)  loss_ddf_aux_0: 0.1707 (0.2014)  loss_vfl_aux_1: 0.5654 (0.6405)  loss_bbox_aux_1: 0.1614 (0.1508)  loss_giou_aux_1: 0.9379 (0.9722)  loss_fgl_aux_1: 0.9896 (0.9746)  loss_ddf_aux_1: 0.0283 (0.0310)  loss_vfl_aux_2: 0.5894 (0.6350)  loss_bbox_aux_2: 0.1588 (0.1492)  loss_giou_aux_2: 0.9393 (0.9662)  loss_fgl_aux_2: 0.9784 (0.9736)  loss_ddf_aux_2: 0.0040 (0.0040)  loss_vfl_aux_3: 0.5757 (0.6317)  loss_bbox_aux_3: 0.1582 (0.1492)  loss_giou_aux_3: 0.9434 (0.9658)  loss_fgl_aux_3: 0.9783 (0.9737)  loss_ddf_aux_3: 0.0006 (0.0006)  loss_vfl_aux_4: 0.5708 (0.6284)  loss_bbox_aux_4: 0.1577 (0.1491)  loss_giou_aux_4: 0.9430 (0.9656)  loss_fgl_aux_4: 0.9775 (0.9737)  loss_ddf_aux_4: 0.0001 (0.0001)  loss_vfl_pre: 0.5762 (0.6222)  loss_bbox_pre: 0.1698 (0.1634)  loss_giou_pre: 0.9790 (1.0037)  loss_vfl_enc_0: 0.5659 (0.6052)  loss_bbox_enc_0: 0.2062 (0.2131)  loss_giou_enc_0: 1.1369 (1.1271)  loss_vfl_dn_0: 0.4287 (0.4334)  loss_bbox_dn_0: 0.1576 (0.1540)  loss_giou_dn_0: 0.9982 (1.0061)  loss_fgl_dn_0: 1.0309 (1.0356)  loss_ddf_dn_0: 0.5892 (0.5968)  loss_vfl_dn_1: 0.4553 (0.4609)  loss_bbox_dn_1: 0.1172 (0.1264)  loss_giou_dn_1: 0.8745 (0.9021)  loss_fgl_dn_1: 1.0404 (1.0453)  loss_ddf_dn_1: 0.0968 (0.0931)  loss_vfl_dn_2: 0.4546 (0.4589)  loss_bbox_dn_2: 0.1148 (0.1198)  loss_giou_dn_2: 0.8644 (0.8822)  loss_fgl_dn_2: 1.0413 (1.0473)  loss_ddf_dn_2: 0.0152 (0.0141)  loss_vfl_dn_3: 0.4514 (0.4572)  loss_bbox_dn_3: 0.1144 (0.1184)  loss_giou_dn_3: 0.8517 (0.8782)  loss_fgl_dn_3: 1.0453 (1.0486)  loss_ddf_dn_3: 0.0024 (0.0021)  loss_vfl_dn_4: 0.4548 (0.4571)  loss_bbox_dn_4: 0.1145 (0.1180)  loss_giou_dn_4: 0.8516 (0.8773)  loss_fgl_dn_4: 1.0452 (1.0490)  loss_ddf_dn_4: 0.0002 (0.0002)  loss_vfl_dn_5: 0.4531 (0.4586)  loss_bbox_dn_5: 0.1147 (0.1179)  loss_giou_dn_5: 0.8511 (0.8771)  loss_fgl_dn_5: 1.0455 (1.0492)  loss_vfl_dn_pre: 0.4280 (0.4329)  loss_bbox_dn_pre: 0.1609 (0.1541)  loss_giou_dn_pre: 0.9991 (1.0081)  time: 0.6656  data: 0.0077  max mem: 3024\n",
            "Epoch: [6]  [200/500]  eta: 0:03:14  lr: 0.000013  loss: 37.9752 (38.0463)  loss_vfl: 0.5625 (0.6127)  loss_bbox: 0.1344 (0.1660)  loss_giou: 1.0254 (0.9819)  loss_fgl: 0.9189 (0.9642)  loss_vfl_aux_0: 0.5400 (0.6102)  loss_bbox_aux_0: 0.1601 (0.1804)  loss_giou_aux_0: 1.0914 (1.0214)  loss_fgl_aux_0: 0.9077 (0.9685)  loss_ddf_aux_0: 0.1648 (0.1916)  loss_vfl_aux_1: 0.5723 (0.6237)  loss_bbox_aux_1: 0.1455 (0.1684)  loss_giou_aux_1: 1.0247 (0.9892)  loss_fgl_aux_1: 0.9215 (0.9650)  loss_ddf_aux_1: 0.0225 (0.0316)  loss_vfl_aux_2: 0.5884 (0.6212)  loss_bbox_aux_2: 0.1341 (0.1661)  loss_giou_aux_2: 1.0223 (0.9828)  loss_fgl_aux_2: 0.9113 (0.9639)  loss_ddf_aux_2: 0.0032 (0.0043)  loss_vfl_aux_3: 0.5537 (0.6144)  loss_bbox_aux_3: 0.1344 (0.1660)  loss_giou_aux_3: 1.0233 (0.9822)  loss_fgl_aux_3: 0.9135 (0.9639)  loss_ddf_aux_3: 0.0006 (0.0007)  loss_vfl_aux_4: 0.5474 (0.6113)  loss_bbox_aux_4: 0.1345 (0.1660)  loss_giou_aux_4: 1.0249 (0.9819)  loss_fgl_aux_4: 0.9173 (0.9641)  loss_ddf_aux_4: 0.0001 (0.0001)  loss_vfl_pre: 0.5498 (0.6062)  loss_bbox_pre: 0.1590 (0.1822)  loss_giou_pre: 1.0920 (1.0196)  loss_vfl_enc_0: 0.5469 (0.5948)  loss_bbox_enc_0: 0.1668 (0.2322)  loss_giou_enc_0: 1.1981 (1.1352)  loss_vfl_dn_0: 0.4175 (0.4316)  loss_bbox_dn_0: 0.1235 (0.1635)  loss_giou_dn_0: 1.0542 (1.0125)  loss_fgl_dn_0: 1.0072 (1.0328)  loss_ddf_dn_0: 0.6326 (0.6238)  loss_vfl_dn_1: 0.4717 (0.4612)  loss_bbox_dn_1: 0.0995 (0.1363)  loss_giou_dn_1: 0.9273 (0.9067)  loss_fgl_dn_1: 1.0096 (1.0436)  loss_ddf_dn_1: 0.1069 (0.0997)  loss_vfl_dn_2: 0.4622 (0.4598)  loss_bbox_dn_2: 0.0894 (0.1301)  loss_giou_dn_2: 0.9124 (0.8869)  loss_fgl_dn_2: 1.0148 (1.0457)  loss_ddf_dn_2: 0.0130 (0.0159)  loss_vfl_dn_3: 0.4553 (0.4577)  loss_bbox_dn_3: 0.0907 (0.1291)  loss_giou_dn_3: 0.9074 (0.8830)  loss_fgl_dn_3: 1.0153 (1.0470)  loss_ddf_dn_3: 0.0019 (0.0026)  loss_vfl_dn_4: 0.4604 (0.4576)  loss_bbox_dn_4: 0.0905 (0.1287)  loss_giou_dn_4: 0.9083 (0.8822)  loss_fgl_dn_4: 1.0134 (1.0476)  loss_ddf_dn_4: 0.0002 (0.0003)  loss_vfl_dn_5: 0.4592 (0.4592)  loss_bbox_dn_5: 0.0905 (0.1286)  loss_giou_dn_5: 0.9087 (0.8820)  loss_fgl_dn_5: 1.0127 (1.0478)  loss_vfl_dn_pre: 0.4180 (0.4312)  loss_bbox_dn_pre: 0.1269 (0.1633)  loss_giou_dn_pre: 1.0589 (1.0145)  time: 0.6508  data: 0.0100  max mem: 3024\n",
            "Epoch: [6]  [300/500]  eta: 0:02:08  lr: 0.000013  loss: 36.6216 (37.9197)  loss_vfl: 0.5820 (0.6093)  loss_bbox: 0.1271 (0.1610)  loss_giou: 0.9397 (0.9803)  loss_fgl: 1.0015 (0.9676)  loss_vfl_aux_0: 0.5801 (0.6038)  loss_bbox_aux_0: 0.1379 (0.1753)  loss_giou_aux_0: 0.9915 (1.0186)  loss_fgl_aux_0: 0.9923 (0.9722)  loss_ddf_aux_0: 0.1426 (0.1786)  loss_vfl_aux_1: 0.5898 (0.6160)  loss_bbox_aux_1: 0.1340 (0.1633)  loss_giou_aux_1: 0.9487 (0.9876)  loss_fgl_aux_1: 1.0012 (0.9681)  loss_ddf_aux_1: 0.0264 (0.0299)  loss_vfl_aux_2: 0.5811 (0.6168)  loss_bbox_aux_2: 0.1280 (0.1611)  loss_giou_aux_2: 0.9405 (0.9812)  loss_fgl_aux_2: 1.0009 (0.9672)  loss_ddf_aux_2: 0.0027 (0.0041)  loss_vfl_aux_3: 0.5801 (0.6104)  loss_bbox_aux_3: 0.1275 (0.1610)  loss_giou_aux_3: 0.9400 (0.9805)  loss_fgl_aux_3: 1.0016 (0.9673)  loss_ddf_aux_3: 0.0003 (0.0007)  loss_vfl_aux_4: 0.5840 (0.6081)  loss_bbox_aux_4: 0.1272 (0.1610)  loss_giou_aux_4: 0.9400 (0.9802)  loss_fgl_aux_4: 1.0014 (0.9675)  loss_ddf_aux_4: 0.0001 (0.0001)  loss_vfl_pre: 0.5684 (0.6004)  loss_bbox_pre: 0.1375 (0.1767)  loss_giou_pre: 0.9799 (1.0169)  loss_vfl_enc_0: 0.5439 (0.5903)  loss_bbox_enc_0: 0.1767 (0.2232)  loss_giou_enc_0: 1.0572 (1.1299)  loss_vfl_dn_0: 0.4229 (0.4316)  loss_bbox_dn_0: 0.1460 (0.1604)  loss_giou_dn_0: 1.0082 (1.0113)  loss_fgl_dn_0: 1.0350 (1.0329)  loss_ddf_dn_0: 0.4307 (0.6184)  loss_vfl_dn_1: 0.4568 (0.4607)  loss_bbox_dn_1: 0.1109 (0.1335)  loss_giou_dn_1: 0.9116 (0.9061)  loss_fgl_dn_1: 1.0486 (1.0442)  loss_ddf_dn_1: 0.0602 (0.0972)  loss_vfl_dn_2: 0.4468 (0.4589)  loss_bbox_dn_2: 0.1016 (0.1277)  loss_giou_dn_2: 0.8945 (0.8870)  loss_fgl_dn_2: 1.0484 (1.0463)  loss_ddf_dn_2: 0.0084 (0.0152)  loss_vfl_dn_3: 0.4460 (0.4571)  loss_bbox_dn_3: 0.1020 (0.1267)  loss_giou_dn_3: 0.8971 (0.8831)  loss_fgl_dn_3: 1.0492 (1.0478)  loss_ddf_dn_3: 0.0009 (0.0023)  loss_vfl_dn_4: 0.4492 (0.4570)  loss_bbox_dn_4: 0.1028 (0.1263)  loss_giou_dn_4: 0.8958 (0.8824)  loss_fgl_dn_4: 1.0488 (1.0484)  loss_ddf_dn_4: 0.0001 (0.0002)  loss_vfl_dn_5: 0.4536 (0.4585)  loss_bbox_dn_5: 0.1027 (0.1262)  loss_giou_dn_5: 0.8954 (0.8822)  loss_fgl_dn_5: 1.0486 (1.0486)  loss_vfl_dn_pre: 0.4214 (0.4311)  loss_bbox_dn_pre: 0.1445 (0.1605)  loss_giou_dn_pre: 1.0138 (1.0138)  time: 0.5943  data: 0.0072  max mem: 3024\n",
            "Epoch: [6]  [400/500]  eta: 0:01:03  lr: 0.000013  loss: 37.5929 (37.8735)  loss_vfl: 0.6382 (0.6083)  loss_bbox: 0.1365 (0.1574)  loss_giou: 0.8949 (0.9816)  loss_fgl: 1.0193 (0.9659)  loss_vfl_aux_0: 0.6465 (0.6025)  loss_bbox_aux_0: 0.1487 (0.1722)  loss_giou_aux_0: 0.9112 (1.0201)  loss_fgl_aux_0: 1.0315 (0.9708)  loss_ddf_aux_0: 0.1721 (0.1803)  loss_vfl_aux_1: 0.6367 (0.6147)  loss_bbox_aux_1: 0.1377 (0.1598)  loss_giou_aux_1: 0.8943 (0.9886)  loss_fgl_aux_1: 1.0207 (0.9665)  loss_ddf_aux_1: 0.0246 (0.0298)  loss_vfl_aux_2: 0.6240 (0.6151)  loss_bbox_aux_2: 0.1365 (0.1577)  loss_giou_aux_2: 0.8932 (0.9825)  loss_fgl_aux_2: 1.0168 (0.9657)  loss_ddf_aux_2: 0.0038 (0.0040)  loss_vfl_aux_3: 0.6011 (0.6073)  loss_bbox_aux_3: 0.1365 (0.1575)  loss_giou_aux_3: 0.8953 (0.9818)  loss_fgl_aux_3: 1.0193 (0.9657)  loss_ddf_aux_3: 0.0007 (0.0007)  loss_vfl_aux_4: 0.6113 (0.6056)  loss_bbox_aux_4: 0.1365 (0.1574)  loss_giou_aux_4: 0.8950 (0.9816)  loss_fgl_aux_4: 1.0196 (0.9659)  loss_ddf_aux_4: 0.0001 (0.0001)  loss_vfl_pre: 0.6406 (0.5989)  loss_bbox_pre: 0.1502 (0.1732)  loss_giou_pre: 0.9161 (1.0184)  loss_vfl_enc_0: 0.6016 (0.5905)  loss_bbox_enc_0: 0.1905 (0.2206)  loss_giou_enc_0: 1.0242 (1.1313)  loss_vfl_dn_0: 0.4509 (0.4315)  loss_bbox_dn_0: 0.1533 (0.1586)  loss_giou_dn_0: 0.9503 (1.0081)  loss_fgl_dn_0: 1.0762 (1.0354)  loss_ddf_dn_0: 0.7216 (0.6350)  loss_vfl_dn_1: 0.4724 (0.4606)  loss_bbox_dn_1: 0.1292 (0.1315)  loss_giou_dn_1: 0.8545 (0.9031)  loss_fgl_dn_1: 1.0765 (1.0459)  loss_ddf_dn_1: 0.1137 (0.0972)  loss_vfl_dn_2: 0.4741 (0.4588)  loss_bbox_dn_2: 0.1230 (0.1258)  loss_giou_dn_2: 0.8389 (0.8843)  loss_fgl_dn_2: 1.0736 (1.0482)  loss_ddf_dn_2: 0.0201 (0.0148)  loss_vfl_dn_3: 0.4717 (0.4564)  loss_bbox_dn_3: 0.1235 (0.1246)  loss_giou_dn_3: 0.8329 (0.8804)  loss_fgl_dn_3: 1.0725 (1.0496)  loss_ddf_dn_3: 0.0025 (0.0022)  loss_vfl_dn_4: 0.4673 (0.4562)  loss_bbox_dn_4: 0.1228 (0.1242)  loss_giou_dn_4: 0.8339 (0.8797)  loss_fgl_dn_4: 1.0733 (1.0501)  loss_ddf_dn_4: 0.0003 (0.0002)  loss_vfl_dn_5: 0.4641 (0.4574)  loss_bbox_dn_5: 0.1225 (0.1242)  loss_giou_dn_5: 0.8347 (0.8796)  loss_fgl_dn_5: 1.0736 (1.0503)  loss_vfl_dn_pre: 0.4478 (0.4312)  loss_bbox_dn_pre: 0.1537 (0.1587)  loss_giou_dn_pre: 0.9385 (1.0098)  time: 0.6384  data: 0.0111  max mem: 3024\n",
            "Epoch: [6]  [499/500]  eta: 0:00:00  lr: 0.000013  loss: 36.8943 (37.7746)  loss_vfl: 0.5518 (0.6036)  loss_bbox: 0.1276 (0.1549)  loss_giou: 0.9466 (0.9834)  loss_fgl: 0.9482 (0.9625)  loss_vfl_aux_0: 0.5410 (0.5995)  loss_bbox_aux_0: 0.1376 (0.1693)  loss_giou_aux_0: 0.9704 (1.0217)  loss_fgl_aux_0: 0.9385 (0.9675)  loss_ddf_aux_0: 0.1432 (0.1805)  loss_vfl_aux_1: 0.5391 (0.6090)  loss_bbox_aux_1: 0.1295 (0.1574)  loss_giou_aux_1: 0.9396 (0.9904)  loss_fgl_aux_1: 0.9421 (0.9632)  loss_ddf_aux_1: 0.0241 (0.0302)  loss_vfl_aux_2: 0.5547 (0.6116)  loss_bbox_aux_2: 0.1267 (0.1553)  loss_giou_aux_2: 0.9443 (0.9843)  loss_fgl_aux_2: 0.9478 (0.9622)  loss_ddf_aux_2: 0.0037 (0.0041)  loss_vfl_aux_3: 0.5444 (0.6027)  loss_bbox_aux_3: 0.1275 (0.1550)  loss_giou_aux_3: 0.9459 (0.9836)  loss_fgl_aux_3: 0.9477 (0.9622)  loss_ddf_aux_3: 0.0006 (0.0006)  loss_vfl_aux_4: 0.5469 (0.6013)  loss_bbox_aux_4: 0.1276 (0.1550)  loss_giou_aux_4: 0.9466 (0.9834)  loss_fgl_aux_4: 0.9482 (0.9624)  loss_ddf_aux_4: 0.0001 (0.0001)  loss_vfl_pre: 0.5396 (0.5957)  loss_bbox_pre: 0.1385 (0.1703)  loss_giou_pre: 0.9737 (1.0199)  loss_vfl_enc_0: 0.5288 (0.5859)  loss_bbox_enc_0: 0.1680 (0.2165)  loss_giou_enc_0: 1.0749 (1.1310)  loss_vfl_dn_0: 0.4360 (0.4315)  loss_bbox_dn_0: 0.1245 (0.1557)  loss_giou_dn_0: 0.9412 (1.0066)  loss_fgl_dn_0: 1.0632 (1.0361)  loss_ddf_dn_0: 0.5980 (0.6299)  loss_vfl_dn_1: 0.4644 (0.4598)  loss_bbox_dn_1: 0.0978 (0.1287)  loss_giou_dn_1: 0.8635 (0.9023)  loss_fgl_dn_1: 1.0593 (1.0461)  loss_ddf_dn_1: 0.0840 (0.0970)  loss_vfl_dn_2: 0.4561 (0.4580)  loss_bbox_dn_2: 0.0983 (0.1230)  loss_giou_dn_2: 0.8474 (0.8834)  loss_fgl_dn_2: 1.0602 (1.0483)  loss_ddf_dn_2: 0.0129 (0.0148)  loss_vfl_dn_3: 0.4546 (0.4559)  loss_bbox_dn_3: 0.0985 (0.1219)  loss_giou_dn_3: 0.8358 (0.8794)  loss_fgl_dn_3: 1.0598 (1.0497)  loss_ddf_dn_3: 0.0016 (0.0021)  loss_vfl_dn_4: 0.4575 (0.4554)  loss_bbox_dn_4: 0.0984 (0.1215)  loss_giou_dn_4: 0.8309 (0.8787)  loss_fgl_dn_4: 1.0602 (1.0503)  loss_ddf_dn_4: 0.0002 (0.0002)  loss_vfl_dn_5: 0.4553 (0.4565)  loss_bbox_dn_5: 0.0985 (0.1214)  loss_giou_dn_5: 0.8298 (0.8785)  loss_fgl_dn_5: 1.0606 (1.0504)  loss_vfl_dn_pre: 0.4377 (0.4312)  loss_bbox_dn_pre: 0.1270 (0.1558)  loss_giou_dn_pre: 0.9375 (1.0082)  time: 0.6039  data: 0.0079  max mem: 3024\n",
            "Epoch: [6] Total time: 0:05:21 (0.6438 s / it)\n",
            "Averaged stats: lr: 0.000013  loss: 36.8943 (37.7746)  loss_vfl: 0.5518 (0.6036)  loss_bbox: 0.1276 (0.1549)  loss_giou: 0.9466 (0.9834)  loss_fgl: 0.9482 (0.9625)  loss_vfl_aux_0: 0.5410 (0.5995)  loss_bbox_aux_0: 0.1376 (0.1693)  loss_giou_aux_0: 0.9704 (1.0217)  loss_fgl_aux_0: 0.9385 (0.9675)  loss_ddf_aux_0: 0.1432 (0.1805)  loss_vfl_aux_1: 0.5391 (0.6090)  loss_bbox_aux_1: 0.1295 (0.1574)  loss_giou_aux_1: 0.9396 (0.9904)  loss_fgl_aux_1: 0.9421 (0.9632)  loss_ddf_aux_1: 0.0241 (0.0302)  loss_vfl_aux_2: 0.5547 (0.6116)  loss_bbox_aux_2: 0.1267 (0.1553)  loss_giou_aux_2: 0.9443 (0.9843)  loss_fgl_aux_2: 0.9478 (0.9622)  loss_ddf_aux_2: 0.0037 (0.0041)  loss_vfl_aux_3: 0.5444 (0.6027)  loss_bbox_aux_3: 0.1275 (0.1550)  loss_giou_aux_3: 0.9459 (0.9836)  loss_fgl_aux_3: 0.9477 (0.9622)  loss_ddf_aux_3: 0.0006 (0.0006)  loss_vfl_aux_4: 0.5469 (0.6013)  loss_bbox_aux_4: 0.1276 (0.1550)  loss_giou_aux_4: 0.9466 (0.9834)  loss_fgl_aux_4: 0.9482 (0.9624)  loss_ddf_aux_4: 0.0001 (0.0001)  loss_vfl_pre: 0.5396 (0.5957)  loss_bbox_pre: 0.1385 (0.1703)  loss_giou_pre: 0.9737 (1.0199)  loss_vfl_enc_0: 0.5288 (0.5859)  loss_bbox_enc_0: 0.1680 (0.2165)  loss_giou_enc_0: 1.0749 (1.1310)  loss_vfl_dn_0: 0.4360 (0.4315)  loss_bbox_dn_0: 0.1245 (0.1557)  loss_giou_dn_0: 0.9412 (1.0066)  loss_fgl_dn_0: 1.0632 (1.0361)  loss_ddf_dn_0: 0.5980 (0.6299)  loss_vfl_dn_1: 0.4644 (0.4598)  loss_bbox_dn_1: 0.0978 (0.1287)  loss_giou_dn_1: 0.8635 (0.9023)  loss_fgl_dn_1: 1.0593 (1.0461)  loss_ddf_dn_1: 0.0840 (0.0970)  loss_vfl_dn_2: 0.4561 (0.4580)  loss_bbox_dn_2: 0.0983 (0.1230)  loss_giou_dn_2: 0.8474 (0.8834)  loss_fgl_dn_2: 1.0602 (1.0483)  loss_ddf_dn_2: 0.0129 (0.0148)  loss_vfl_dn_3: 0.4546 (0.4559)  loss_bbox_dn_3: 0.0985 (0.1219)  loss_giou_dn_3: 0.8358 (0.8794)  loss_fgl_dn_3: 1.0598 (1.0497)  loss_ddf_dn_3: 0.0016 (0.0021)  loss_vfl_dn_4: 0.4575 (0.4554)  loss_bbox_dn_4: 0.0984 (0.1215)  loss_giou_dn_4: 0.8309 (0.8787)  loss_fgl_dn_4: 1.0602 (1.0503)  loss_ddf_dn_4: 0.0002 (0.0002)  loss_vfl_dn_5: 0.4553 (0.4565)  loss_bbox_dn_5: 0.0985 (0.1214)  loss_giou_dn_5: 0.8298 (0.8785)  loss_fgl_dn_5: 1.0606 (1.0504)  loss_vfl_dn_pre: 0.4377 (0.4312)  loss_bbox_dn_pre: 0.1270 (0.1558)  loss_giou_dn_pre: 0.9375 (1.0082)\n",
            "Test:  [  0/250]  eta: 0:05:04    time: 1.2163  data: 0.8872  max mem: 3024\n",
            "Test:  [ 10/250]  eta: 0:01:09    time: 0.2894  data: 0.1000  max mem: 3024\n",
            "Test:  [ 20/250]  eta: 0:01:00    time: 0.2136  data: 0.0255  max mem: 3024\n",
            "Test:  [ 30/250]  eta: 0:00:57    time: 0.2467  data: 0.0376  max mem: 3024\n",
            "Test:  [ 40/250]  eta: 0:00:55    time: 0.2692  data: 0.0459  max mem: 3024\n",
            "Test:  [ 50/250]  eta: 0:00:52    time: 0.2573  data: 0.0351  max mem: 3024\n",
            "Test:  [ 60/250]  eta: 0:00:47    time: 0.2159  data: 0.0220  max mem: 3024\n",
            "Test:  [ 70/250]  eta: 0:00:43    time: 0.1940  data: 0.0207  max mem: 3024\n",
            "Test:  [ 80/250]  eta: 0:00:40    time: 0.1949  data: 0.0218  max mem: 3024\n",
            "Test:  [ 90/250]  eta: 0:00:37    time: 0.1996  data: 0.0242  max mem: 3024\n",
            "Test:  [100/250]  eta: 0:00:35    time: 0.2308  data: 0.0336  max mem: 3024\n",
            "Test:  [110/250]  eta: 0:00:33    time: 0.2609  data: 0.0410  max mem: 3024\n",
            "Test:  [120/250]  eta: 0:00:31    time: 0.2713  data: 0.0378  max mem: 3024\n",
            "Test:  [130/250]  eta: 0:00:28    time: 0.2368  data: 0.0284  max mem: 3024\n",
            "Test:  [140/250]  eta: 0:00:25    time: 0.1960  data: 0.0223  max mem: 3024\n",
            "Test:  [150/250]  eta: 0:00:23    time: 0.1950  data: 0.0215  max mem: 3024\n",
            "Test:  [160/250]  eta: 0:00:20    time: 0.1948  data: 0.0209  max mem: 3024\n",
            "Test:  [170/250]  eta: 0:00:18    time: 0.2030  data: 0.0246  max mem: 3024\n",
            "Test:  [180/250]  eta: 0:00:16    time: 0.2374  data: 0.0350  max mem: 3024\n",
            "Test:  [190/250]  eta: 0:00:14    time: 0.2897  data: 0.0421  max mem: 3024\n",
            "Test:  [200/250]  eta: 0:00:11    time: 0.2588  data: 0.0343  max mem: 3024\n",
            "Test:  [210/250]  eta: 0:00:09    time: 0.1988  data: 0.0236  max mem: 3024\n",
            "Test:  [220/250]  eta: 0:00:06    time: 0.1960  data: 0.0204  max mem: 3024\n",
            "Test:  [230/250]  eta: 0:00:04    time: 0.1965  data: 0.0209  max mem: 3024\n",
            "Test:  [240/250]  eta: 0:00:02    time: 0.1969  data: 0.0219  max mem: 3024\n",
            "Test:  [249/250]  eta: 0:00:00    time: 0.2182  data: 0.0215  max mem: 3024\n",
            "Test: Total time: 0:00:57 (0.2288 s / it)\n",
            "Averaged stats: \n",
            "Accumulating evaluation results...\n",
            "COCOeval_opt.accumulate() finished...\n",
            "DONE (t=1.31s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.098\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.192\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.086\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.044\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.123\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.249\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.104\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.224\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.281\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.146\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.335\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.557\n",
            " Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.516\n",
            " Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.265\n",
            "best_stat: {'epoch': 6, 'coco_eval_bbox': 0.09774409631623465}\n",
            "Epoch: [7]  [  0/500]  eta: 0:20:13  lr: 0.000013  loss: 39.9223 (39.9223)  loss_vfl: 0.6802 (0.6802)  loss_bbox: 0.0927 (0.0927)  loss_giou: 1.5008 (1.5008)  loss_fgl: 0.7550 (0.7550)  loss_vfl_aux_0: 0.5684 (0.5684)  loss_bbox_aux_0: 0.0968 (0.0968)  loss_giou_aux_0: 1.5011 (1.5011)  loss_fgl_aux_0: 0.7288 (0.7288)  loss_ddf_aux_0: 0.1307 (0.1307)  loss_vfl_aux_1: 0.4927 (0.4927)  loss_bbox_aux_1: 0.0937 (0.0937)  loss_giou_aux_1: 1.5032 (1.5032)  loss_fgl_aux_1: 0.7450 (0.7450)  loss_ddf_aux_1: 0.0181 (0.0181)  loss_vfl_aux_2: 0.5605 (0.5605)  loss_bbox_aux_2: 0.0928 (0.0928)  loss_giou_aux_2: 1.5016 (1.5016)  loss_fgl_aux_2: 0.7517 (0.7517)  loss_ddf_aux_2: 0.0018 (0.0018)  loss_vfl_aux_3: 0.6367 (0.6367)  loss_bbox_aux_3: 0.0929 (0.0929)  loss_giou_aux_3: 1.5019 (1.5019)  loss_fgl_aux_3: 0.7547 (0.7547)  loss_ddf_aux_3: 0.0003 (0.0003)  loss_vfl_aux_4: 0.6895 (0.6895)  loss_bbox_aux_4: 0.0927 (0.0927)  loss_giou_aux_4: 1.5009 (1.5009)  loss_fgl_aux_4: 0.7549 (0.7549)  loss_ddf_aux_4: 0.0000 (0.0000)  loss_vfl_pre: 0.5649 (0.5649)  loss_bbox_pre: 0.0979 (0.0979)  loss_giou_pre: 1.4906 (1.4906)  loss_vfl_enc_0: 0.5044 (0.5044)  loss_bbox_enc_0: 0.1279 (0.1279)  loss_giou_enc_0: 1.4824 (1.4824)  loss_vfl_dn_0: 0.3716 (0.3716)  loss_bbox_dn_0: 0.0642 (0.0642)  loss_giou_dn_0: 1.2448 (1.2448)  loss_fgl_dn_0: 0.9346 (0.9346)  loss_ddf_dn_0: 0.2933 (0.2933)  loss_vfl_dn_1: 0.3923 (0.3923)  loss_bbox_dn_1: 0.0613 (0.0613)  loss_giou_dn_1: 1.1895 (1.1895)  loss_fgl_dn_1: 0.9808 (0.9808)  loss_ddf_dn_1: 0.0404 (0.0404)  loss_vfl_dn_2: 0.3923 (0.3923)  loss_bbox_dn_2: 0.0617 (0.0617)  loss_giou_dn_2: 1.1906 (1.1906)  loss_fgl_dn_2: 0.9825 (0.9825)  loss_ddf_dn_2: 0.0067 (0.0067)  loss_vfl_dn_3: 0.3838 (0.3838)  loss_bbox_dn_3: 0.0637 (0.0637)  loss_giou_dn_3: 1.1946 (1.1946)  loss_fgl_dn_3: 0.9908 (0.9908)  loss_ddf_dn_3: 0.0010 (0.0010)  loss_vfl_dn_4: 0.3923 (0.3923)  loss_bbox_dn_4: 0.0648 (0.0648)  loss_giou_dn_4: 1.1964 (1.1964)  loss_fgl_dn_4: 0.9942 (0.9942)  loss_ddf_dn_4: 0.0000 (0.0000)  loss_vfl_dn_5: 0.4094 (0.4094)  loss_bbox_dn_5: 0.0649 (0.0649)  loss_giou_dn_5: 1.1969 (1.1969)  loss_fgl_dn_5: 0.9941 (0.9941)  loss_vfl_dn_pre: 0.3809 (0.3809)  loss_bbox_dn_pre: 0.0623 (0.0623)  loss_giou_dn_pre: 1.2174 (1.2174)  time: 2.4268  data: 1.0522  max mem: 3024\n",
            "Epoch: [7]  [100/500]  eta: 0:04:19  lr: 0.000013  loss: 37.4553 (37.7051)  loss_vfl: 0.5654 (0.5872)  loss_bbox: 0.1397 (0.1586)  loss_giou: 0.9126 (1.0177)  loss_fgl: 0.9764 (0.9376)  loss_vfl_aux_0: 0.6064 (0.5814)  loss_bbox_aux_0: 0.1439 (0.1720)  loss_giou_aux_0: 0.9662 (1.0523)  loss_fgl_aux_0: 0.9747 (0.9401)  loss_ddf_aux_0: 0.1624 (0.1698)  loss_vfl_aux_1: 0.5679 (0.5976)  loss_bbox_aux_1: 0.1400 (0.1616)  loss_giou_aux_1: 0.9195 (1.0221)  loss_fgl_aux_1: 0.9706 (0.9392)  loss_ddf_aux_1: 0.0182 (0.0247)  loss_vfl_aux_2: 0.5835 (0.6011)  loss_bbox_aux_2: 0.1398 (0.1593)  loss_giou_aux_2: 0.9127 (1.0184)  loss_fgl_aux_2: 0.9727 (0.9373)  loss_ddf_aux_2: 0.0018 (0.0038)  loss_vfl_aux_3: 0.5850 (0.5927)  loss_bbox_aux_3: 0.1399 (0.1588)  loss_giou_aux_3: 0.9122 (1.0179)  loss_fgl_aux_3: 0.9748 (0.9372)  loss_ddf_aux_3: 0.0003 (0.0006)  loss_vfl_aux_4: 0.5713 (0.5855)  loss_bbox_aux_4: 0.1398 (0.1587)  loss_giou_aux_4: 0.9125 (1.0176)  loss_fgl_aux_4: 0.9765 (0.9376)  loss_ddf_aux_4: 0.0000 (0.0001)  loss_vfl_pre: 0.6021 (0.5777)  loss_bbox_pre: 0.1424 (0.1717)  loss_giou_pre: 0.9599 (1.0507)  loss_vfl_enc_0: 0.6055 (0.5690)  loss_bbox_enc_0: 0.1938 (0.2160)  loss_giou_enc_0: 1.0867 (1.1585)  loss_vfl_dn_0: 0.4236 (0.4290)  loss_bbox_dn_0: 0.1201 (0.1561)  loss_giou_dn_0: 0.9826 (1.0112)  loss_fgl_dn_0: 1.0296 (1.0280)  loss_ddf_dn_0: 0.5172 (0.5720)  loss_vfl_dn_1: 0.4521 (0.4548)  loss_bbox_dn_1: 0.1098 (0.1312)  loss_giou_dn_1: 0.8714 (0.9087)  loss_fgl_dn_1: 1.0502 (1.0402)  loss_ddf_dn_1: 0.0705 (0.0821)  loss_vfl_dn_2: 0.4529 (0.4531)  loss_bbox_dn_2: 0.1108 (0.1275)  loss_giou_dn_2: 0.8617 (0.8936)  loss_fgl_dn_2: 1.0544 (1.0407)  loss_ddf_dn_2: 0.0069 (0.0122)  loss_vfl_dn_3: 0.4497 (0.4509)  loss_bbox_dn_3: 0.1104 (0.1271)  loss_giou_dn_3: 0.8562 (0.8911)  loss_fgl_dn_3: 1.0548 (1.0415)  loss_ddf_dn_3: 0.0008 (0.0015)  loss_vfl_dn_4: 0.4517 (0.4513)  loss_bbox_dn_4: 0.1099 (0.1270)  loss_giou_dn_4: 0.8559 (0.8907)  loss_fgl_dn_4: 1.0548 (1.0420)  loss_ddf_dn_4: 0.0001 (0.0002)  loss_vfl_dn_5: 0.4497 (0.4519)  loss_bbox_dn_5: 0.1100 (0.1270)  loss_giou_dn_5: 0.8560 (0.8907)  loss_fgl_dn_5: 1.0549 (1.0421)  loss_vfl_dn_pre: 0.4292 (0.4287)  loss_bbox_dn_pre: 0.1176 (0.1561)  loss_giou_dn_pre: 0.9884 (1.0124)  time: 0.6438  data: 0.0095  max mem: 3024\n",
            "Epoch: [7]  [200/500]  eta: 0:03:12  lr: 0.000013  loss: 37.2487 (37.6799)  loss_vfl: 0.5781 (0.6000)  loss_bbox: 0.1191 (0.1513)  loss_giou: 0.9197 (1.0044)  loss_fgl: 0.9794 (0.9475)  loss_vfl_aux_0: 0.5728 (0.5881)  loss_bbox_aux_0: 0.1253 (0.1651)  loss_giou_aux_0: 0.9739 (1.0418)  loss_fgl_aux_0: 1.0055 (0.9509)  loss_ddf_aux_0: 0.1901 (0.1697)  loss_vfl_aux_1: 0.5884 (0.6060)  loss_bbox_aux_1: 0.1255 (0.1542)  loss_giou_aux_1: 0.9232 (1.0110)  loss_fgl_aux_1: 0.9799 (0.9485)  loss_ddf_aux_1: 0.0410 (0.0277)  loss_vfl_aux_2: 0.5913 (0.6086)  loss_bbox_aux_2: 0.1208 (0.1521)  loss_giou_aux_2: 0.9192 (1.0057)  loss_fgl_aux_2: 0.9772 (0.9470)  loss_ddf_aux_2: 0.0082 (0.0044)  loss_vfl_aux_3: 0.5884 (0.6003)  loss_bbox_aux_3: 0.1195 (0.1515)  loss_giou_aux_3: 0.9199 (1.0047)  loss_fgl_aux_3: 0.9786 (0.9471)  loss_ddf_aux_3: 0.0012 (0.0007)  loss_vfl_aux_4: 0.5835 (0.5956)  loss_bbox_aux_4: 0.1192 (0.1513)  loss_giou_aux_4: 0.9197 (1.0044)  loss_fgl_aux_4: 0.9795 (0.9474)  loss_ddf_aux_4: 0.0001 (0.0001)  loss_vfl_pre: 0.5688 (0.5857)  loss_bbox_pre: 0.1273 (0.1651)  loss_giou_pre: 0.9737 (1.0403)  loss_vfl_enc_0: 0.5464 (0.5721)  loss_bbox_enc_0: 0.1412 (0.2113)  loss_giou_enc_0: 1.0809 (1.1506)  loss_vfl_dn_0: 0.4463 (0.4317)  loss_bbox_dn_0: 0.1077 (0.1528)  loss_giou_dn_0: 0.9903 (1.0057)  loss_fgl_dn_0: 1.0321 (1.0334)  loss_ddf_dn_0: 0.6025 (0.5748)  loss_vfl_dn_1: 0.4775 (0.4587)  loss_bbox_dn_1: 0.0873 (0.1278)  loss_giou_dn_1: 0.8739 (0.9044)  loss_fgl_dn_1: 1.0495 (1.0446)  loss_ddf_dn_1: 0.0981 (0.0839)  loss_vfl_dn_2: 0.4592 (0.4567)  loss_bbox_dn_2: 0.0797 (0.1237)  loss_giou_dn_2: 0.8572 (0.8883)  loss_fgl_dn_2: 1.0483 (1.0459)  loss_ddf_dn_2: 0.0158 (0.0123)  loss_vfl_dn_3: 0.4727 (0.4550)  loss_bbox_dn_3: 0.0772 (0.1230)  loss_giou_dn_3: 0.8444 (0.8853)  loss_fgl_dn_3: 1.0484 (1.0469)  loss_ddf_dn_3: 0.0019 (0.0016)  loss_vfl_dn_4: 0.4692 (0.4544)  loss_bbox_dn_4: 0.0764 (0.1227)  loss_giou_dn_4: 0.8452 (0.8848)  loss_fgl_dn_4: 1.0481 (1.0474)  loss_ddf_dn_4: 0.0002 (0.0002)  loss_vfl_dn_5: 0.4692 (0.4552)  loss_bbox_dn_5: 0.0758 (0.1226)  loss_giou_dn_5: 0.8455 (0.8847)  loss_fgl_dn_5: 1.0480 (1.0475)  loss_vfl_dn_pre: 0.4397 (0.4312)  loss_bbox_dn_pre: 0.1074 (0.1529)  loss_giou_dn_pre: 1.0073 (1.0075)  time: 0.6916  data: 0.0086  max mem: 3024\n",
            "Epoch: [7]  [300/500]  eta: 0:02:05  lr: 0.000013  loss: 36.9233 (37.5393)  loss_vfl: 0.6069 (0.6012)  loss_bbox: 0.1144 (0.1469)  loss_giou: 0.9219 (0.9861)  loss_fgl: 0.9860 (0.9583)  loss_vfl_aux_0: 0.5879 (0.5873)  loss_bbox_aux_0: 0.1200 (0.1612)  loss_giou_aux_0: 0.9782 (1.0242)  loss_fgl_aux_0: 1.0104 (0.9631)  loss_ddf_aux_0: 0.1767 (0.1755)  loss_vfl_aux_1: 0.5820 (0.6038)  loss_bbox_aux_1: 0.1163 (0.1499)  loss_giou_aux_1: 0.9450 (0.9926)  loss_fgl_aux_1: 0.9912 (0.9597)  loss_ddf_aux_1: 0.0367 (0.0297)  loss_vfl_aux_2: 0.5933 (0.6080)  loss_bbox_aux_2: 0.1163 (0.1474)  loss_giou_aux_2: 0.9255 (0.9872)  loss_fgl_aux_2: 0.9866 (0.9580)  loss_ddf_aux_2: 0.0047 (0.0046)  loss_vfl_aux_3: 0.5991 (0.6013)  loss_bbox_aux_3: 0.1156 (0.1470)  loss_giou_aux_3: 0.9233 (0.9863)  loss_fgl_aux_3: 0.9862 (0.9580)  loss_ddf_aux_3: 0.0006 (0.0007)  loss_vfl_aux_4: 0.6035 (0.5978)  loss_bbox_aux_4: 0.1146 (0.1469)  loss_giou_aux_4: 0.9219 (0.9861)  loss_fgl_aux_4: 0.9863 (0.9583)  loss_ddf_aux_4: 0.0001 (0.0001)  loss_vfl_pre: 0.5874 (0.5844)  loss_bbox_pre: 0.1205 (0.1615)  loss_giou_pre: 0.9835 (1.0228)  loss_vfl_enc_0: 0.5830 (0.5768)  loss_bbox_enc_0: 0.1507 (0.2096)  loss_giou_enc_0: 1.0792 (1.1353)  loss_vfl_dn_0: 0.4270 (0.4324)  loss_bbox_dn_0: 0.1317 (0.1517)  loss_giou_dn_0: 0.9759 (0.9982)  loss_fgl_dn_0: 1.0336 (1.0387)  loss_ddf_dn_0: 0.5061 (0.5879)  loss_vfl_dn_1: 0.4512 (0.4595)  loss_bbox_dn_1: 0.1068 (0.1256)  loss_giou_dn_1: 0.8263 (0.8942)  loss_fgl_dn_1: 1.0478 (1.0480)  loss_ddf_dn_1: 0.0802 (0.0864)  loss_vfl_dn_2: 0.4470 (0.4580)  loss_bbox_dn_2: 0.1059 (0.1205)  loss_giou_dn_2: 0.7971 (0.8772)  loss_fgl_dn_2: 1.0479 (1.0489)  loss_ddf_dn_2: 0.0091 (0.0130)  loss_vfl_dn_3: 0.4397 (0.4555)  loss_bbox_dn_3: 0.1055 (0.1197)  loss_giou_dn_3: 0.7913 (0.8741)  loss_fgl_dn_3: 1.0484 (1.0500)  loss_ddf_dn_3: 0.0013 (0.0017)  loss_vfl_dn_4: 0.4355 (0.4547)  loss_bbox_dn_4: 0.1046 (0.1194)  loss_giou_dn_4: 0.7886 (0.8735)  loss_fgl_dn_4: 1.0491 (1.0505)  loss_ddf_dn_4: 0.0001 (0.0002)  loss_vfl_dn_5: 0.4368 (0.4556)  loss_bbox_dn_5: 0.1043 (0.1194)  loss_giou_dn_5: 0.7881 (0.8734)  loss_fgl_dn_5: 1.0491 (1.0506)  loss_vfl_dn_pre: 0.4302 (0.4319)  loss_bbox_dn_pre: 0.1355 (0.1519)  loss_giou_dn_pre: 0.9834 (0.9995)  time: 0.5297  data: 0.0072  max mem: 3024\n",
            "Epoch: [7]  [400/500]  eta: 0:01:02  lr: 0.000013  loss: 37.5717 (37.5486)  loss_vfl: 0.5469 (0.5980)  loss_bbox: 0.1409 (0.1472)  loss_giou: 0.9338 (0.9843)  loss_fgl: 0.9702 (0.9589)  loss_vfl_aux_0: 0.5557 (0.5919)  loss_bbox_aux_0: 0.1527 (0.1613)  loss_giou_aux_0: 0.9597 (1.0221)  loss_fgl_aux_0: 0.9842 (0.9639)  loss_ddf_aux_0: 0.1940 (0.1767)  loss_vfl_aux_1: 0.5806 (0.6055)  loss_bbox_aux_1: 0.1453 (0.1498)  loss_giou_aux_1: 0.9415 (0.9902)  loss_fgl_aux_1: 0.9781 (0.9601)  loss_ddf_aux_1: 0.0266 (0.0295)  loss_vfl_aux_2: 0.5942 (0.6094)  loss_bbox_aux_2: 0.1408 (0.1476)  loss_giou_aux_2: 0.9359 (0.9853)  loss_fgl_aux_2: 0.9713 (0.9584)  loss_ddf_aux_2: 0.0034 (0.0045)  loss_vfl_aux_3: 0.5581 (0.6007)  loss_bbox_aux_3: 0.1411 (0.1473)  loss_giou_aux_3: 0.9345 (0.9845)  loss_fgl_aux_3: 0.9705 (0.9586)  loss_ddf_aux_3: 0.0006 (0.0007)  loss_vfl_aux_4: 0.5620 (0.5963)  loss_bbox_aux_4: 0.1410 (0.1472)  loss_giou_aux_4: 0.9338 (0.9843)  loss_fgl_aux_4: 0.9703 (0.9588)  loss_ddf_aux_4: 0.0001 (0.0001)  loss_vfl_pre: 0.5552 (0.5891)  loss_bbox_pre: 0.1536 (0.1618)  loss_giou_pre: 0.9612 (1.0209)  loss_vfl_enc_0: 0.6133 (0.5801)  loss_bbox_enc_0: 0.1839 (0.2092)  loss_giou_enc_0: 1.0765 (1.1336)  loss_vfl_dn_0: 0.4182 (0.4327)  loss_bbox_dn_0: 0.1542 (0.1540)  loss_giou_dn_0: 1.0142 (0.9970)  loss_fgl_dn_0: 1.0452 (1.0399)  loss_ddf_dn_0: 0.6077 (0.5964)  loss_vfl_dn_1: 0.4509 (0.4600)  loss_bbox_dn_1: 0.1207 (0.1275)  loss_giou_dn_1: 0.8833 (0.8915)  loss_fgl_dn_1: 1.0450 (1.0485)  loss_ddf_dn_1: 0.0821 (0.0870)  loss_vfl_dn_2: 0.4504 (0.4581)  loss_bbox_dn_2: 0.1103 (0.1225)  loss_giou_dn_2: 0.8687 (0.8745)  loss_fgl_dn_2: 1.0500 (1.0492)  loss_ddf_dn_2: 0.0114 (0.0129)  loss_vfl_dn_3: 0.4414 (0.4549)  loss_bbox_dn_3: 0.1101 (0.1216)  loss_giou_dn_3: 0.8676 (0.8712)  loss_fgl_dn_3: 1.0559 (1.0503)  loss_ddf_dn_3: 0.0014 (0.0017)  loss_vfl_dn_4: 0.4412 (0.4543)  loss_bbox_dn_4: 0.1100 (0.1213)  loss_giou_dn_4: 0.8687 (0.8706)  loss_fgl_dn_4: 1.0569 (1.0508)  loss_ddf_dn_4: 0.0002 (0.0002)  loss_vfl_dn_5: 0.4478 (0.4549)  loss_bbox_dn_5: 0.1098 (0.1212)  loss_giou_dn_5: 0.8684 (0.8705)  loss_fgl_dn_5: 1.0570 (1.0510)  loss_vfl_dn_pre: 0.4207 (0.4322)  loss_bbox_dn_pre: 0.1503 (0.1542)  loss_giou_dn_pre: 1.0248 (0.9984)  time: 0.5732  data: 0.0072  max mem: 3024\n",
            "Epoch: [7]  [499/500]  eta: 0:00:00  lr: 0.000013  loss: 36.6484 (37.5132)  loss_vfl: 0.6128 (0.5962)  loss_bbox: 0.1139 (0.1470)  loss_giou: 0.9241 (0.9859)  loss_fgl: 0.9823 (0.9570)  loss_vfl_aux_0: 0.6167 (0.5910)  loss_bbox_aux_0: 0.1276 (0.1611)  loss_giou_aux_0: 0.9273 (1.0241)  loss_fgl_aux_0: 0.9817 (0.9620)  loss_ddf_aux_0: 0.1597 (0.1711)  loss_vfl_aux_1: 0.6245 (0.6054)  loss_bbox_aux_1: 0.1136 (0.1496)  loss_giou_aux_1: 0.9273 (0.9919)  loss_fgl_aux_1: 0.9674 (0.9586)  loss_ddf_aux_1: 0.0242 (0.0288)  loss_vfl_aux_2: 0.6357 (0.6074)  loss_bbox_aux_2: 0.1137 (0.1475)  loss_giou_aux_2: 0.9248 (0.9868)  loss_fgl_aux_2: 0.9747 (0.9567)  loss_ddf_aux_2: 0.0031 (0.0044)  loss_vfl_aux_3: 0.6289 (0.5989)  loss_bbox_aux_3: 0.1139 (0.1471)  loss_giou_aux_3: 0.9235 (0.9861)  loss_fgl_aux_3: 0.9801 (0.9568)  loss_ddf_aux_3: 0.0005 (0.0007)  loss_vfl_aux_4: 0.6152 (0.5943)  loss_bbox_aux_4: 0.1139 (0.1470)  loss_giou_aux_4: 0.9242 (0.9858)  loss_fgl_aux_4: 0.9823 (0.9570)  loss_ddf_aux_4: 0.0001 (0.0001)  loss_vfl_pre: 0.6167 (0.5877)  loss_bbox_pre: 0.1279 (0.1616)  loss_giou_pre: 0.9326 (1.0228)  loss_vfl_enc_0: 0.6138 (0.5782)  loss_bbox_enc_0: 0.1742 (0.2080)  loss_giou_enc_0: 1.0378 (1.1356)  loss_vfl_dn_0: 0.4424 (0.4323)  loss_bbox_dn_0: 0.1513 (0.1538)  loss_giou_dn_0: 0.9381 (0.9990)  loss_fgl_dn_0: 1.0506 (1.0382)  loss_ddf_dn_0: 0.5130 (0.5867)  loss_vfl_dn_1: 0.4639 (0.4597)  loss_bbox_dn_1: 0.1149 (0.1268)  loss_giou_dn_1: 0.8270 (0.8931)  loss_fgl_dn_1: 1.0383 (1.0466)  loss_ddf_dn_1: 0.0687 (0.0865)  loss_vfl_dn_2: 0.4653 (0.4579)  loss_bbox_dn_2: 0.1045 (0.1216)  loss_giou_dn_2: 0.8129 (0.8757)  loss_fgl_dn_2: 1.0448 (1.0472)  loss_ddf_dn_2: 0.0084 (0.0125)  loss_vfl_dn_3: 0.4683 (0.4549)  loss_bbox_dn_3: 0.1026 (0.1207)  loss_giou_dn_3: 0.8107 (0.8725)  loss_fgl_dn_3: 1.0448 (1.0483)  loss_ddf_dn_3: 0.0013 (0.0017)  loss_vfl_dn_4: 0.4634 (0.4541)  loss_bbox_dn_4: 0.1018 (0.1203)  loss_giou_dn_4: 0.8083 (0.8719)  loss_fgl_dn_4: 1.0448 (1.0487)  loss_ddf_dn_4: 0.0001 (0.0002)  loss_vfl_dn_5: 0.4631 (0.4549)  loss_bbox_dn_5: 0.1017 (0.1203)  loss_giou_dn_5: 0.8069 (0.8718)  loss_fgl_dn_5: 1.0448 (1.0489)  loss_vfl_dn_pre: 0.4373 (0.4318)  loss_bbox_dn_pre: 0.1545 (0.1541)  loss_giou_dn_pre: 0.9571 (1.0005)  time: 0.6359  data: 0.0098  max mem: 3024\n",
            "Epoch: [7] Total time: 0:05:15 (0.6306 s / it)\n",
            "Averaged stats: lr: 0.000013  loss: 36.6484 (37.5132)  loss_vfl: 0.6128 (0.5962)  loss_bbox: 0.1139 (0.1470)  loss_giou: 0.9241 (0.9859)  loss_fgl: 0.9823 (0.9570)  loss_vfl_aux_0: 0.6167 (0.5910)  loss_bbox_aux_0: 0.1276 (0.1611)  loss_giou_aux_0: 0.9273 (1.0241)  loss_fgl_aux_0: 0.9817 (0.9620)  loss_ddf_aux_0: 0.1597 (0.1711)  loss_vfl_aux_1: 0.6245 (0.6054)  loss_bbox_aux_1: 0.1136 (0.1496)  loss_giou_aux_1: 0.9273 (0.9919)  loss_fgl_aux_1: 0.9674 (0.9586)  loss_ddf_aux_1: 0.0242 (0.0288)  loss_vfl_aux_2: 0.6357 (0.6074)  loss_bbox_aux_2: 0.1137 (0.1475)  loss_giou_aux_2: 0.9248 (0.9868)  loss_fgl_aux_2: 0.9747 (0.9567)  loss_ddf_aux_2: 0.0031 (0.0044)  loss_vfl_aux_3: 0.6289 (0.5989)  loss_bbox_aux_3: 0.1139 (0.1471)  loss_giou_aux_3: 0.9235 (0.9861)  loss_fgl_aux_3: 0.9801 (0.9568)  loss_ddf_aux_3: 0.0005 (0.0007)  loss_vfl_aux_4: 0.6152 (0.5943)  loss_bbox_aux_4: 0.1139 (0.1470)  loss_giou_aux_4: 0.9242 (0.9858)  loss_fgl_aux_4: 0.9823 (0.9570)  loss_ddf_aux_4: 0.0001 (0.0001)  loss_vfl_pre: 0.6167 (0.5877)  loss_bbox_pre: 0.1279 (0.1616)  loss_giou_pre: 0.9326 (1.0228)  loss_vfl_enc_0: 0.6138 (0.5782)  loss_bbox_enc_0: 0.1742 (0.2080)  loss_giou_enc_0: 1.0378 (1.1356)  loss_vfl_dn_0: 0.4424 (0.4323)  loss_bbox_dn_0: 0.1513 (0.1538)  loss_giou_dn_0: 0.9381 (0.9990)  loss_fgl_dn_0: 1.0506 (1.0382)  loss_ddf_dn_0: 0.5130 (0.5867)  loss_vfl_dn_1: 0.4639 (0.4597)  loss_bbox_dn_1: 0.1149 (0.1268)  loss_giou_dn_1: 0.8270 (0.8931)  loss_fgl_dn_1: 1.0383 (1.0466)  loss_ddf_dn_1: 0.0687 (0.0865)  loss_vfl_dn_2: 0.4653 (0.4579)  loss_bbox_dn_2: 0.1045 (0.1216)  loss_giou_dn_2: 0.8129 (0.8757)  loss_fgl_dn_2: 1.0448 (1.0472)  loss_ddf_dn_2: 0.0084 (0.0125)  loss_vfl_dn_3: 0.4683 (0.4549)  loss_bbox_dn_3: 0.1026 (0.1207)  loss_giou_dn_3: 0.8107 (0.8725)  loss_fgl_dn_3: 1.0448 (1.0483)  loss_ddf_dn_3: 0.0013 (0.0017)  loss_vfl_dn_4: 0.4634 (0.4541)  loss_bbox_dn_4: 0.1018 (0.1203)  loss_giou_dn_4: 0.8083 (0.8719)  loss_fgl_dn_4: 1.0448 (1.0487)  loss_ddf_dn_4: 0.0001 (0.0002)  loss_vfl_dn_5: 0.4631 (0.4549)  loss_bbox_dn_5: 0.1017 (0.1203)  loss_giou_dn_5: 0.8069 (0.8718)  loss_fgl_dn_5: 1.0448 (1.0489)  loss_vfl_dn_pre: 0.4373 (0.4318)  loss_bbox_dn_pre: 0.1545 (0.1541)  loss_giou_dn_pre: 0.9571 (1.0005)\n",
            "Test:  [  0/250]  eta: 0:04:26    time: 1.0652  data: 0.7393  max mem: 3024\n",
            "Test:  [ 10/250]  eta: 0:01:08    time: 0.2868  data: 0.0913  max mem: 3024\n",
            "Test:  [ 20/250]  eta: 0:01:02    time: 0.2317  data: 0.0297  max mem: 3024\n",
            "Test:  [ 30/250]  eta: 0:00:59    time: 0.2592  data: 0.0392  max mem: 3024\n",
            "Test:  [ 40/250]  eta: 0:00:58    time: 0.2836  data: 0.0392  max mem: 3024\n",
            "Test:  [ 50/250]  eta: 0:00:52    time: 0.2503  data: 0.0266  max mem: 3024\n",
            "Test:  [ 60/250]  eta: 0:00:47    time: 0.1975  data: 0.0202  max mem: 3024\n",
            "Test:  [ 70/250]  eta: 0:00:43    time: 0.1976  data: 0.0203  max mem: 3024\n",
            "Test:  [ 80/250]  eta: 0:00:40    time: 0.1973  data: 0.0205  max mem: 3024\n",
            "Test:  [ 90/250]  eta: 0:00:37    time: 0.2035  data: 0.0213  max mem: 3024\n",
            "Test:  [100/250]  eta: 0:00:36    time: 0.2546  data: 0.0329  max mem: 3024\n",
            "Test:  [110/250]  eta: 0:00:34    time: 0.2881  data: 0.0431  max mem: 3024\n",
            "Test:  [120/250]  eta: 0:00:31    time: 0.2539  data: 0.0383  max mem: 3024\n",
            "Test:  [130/250]  eta: 0:00:28    time: 0.2143  data: 0.0272  max mem: 3024\n",
            "Test:  [140/250]  eta: 0:00:26    time: 0.1983  data: 0.0204  max mem: 3024\n",
            "Test:  [150/250]  eta: 0:00:23    time: 0.2158  data: 0.0217  max mem: 3024\n",
            "Test:  [160/250]  eta: 0:00:21    time: 0.2149  data: 0.0214  max mem: 3024\n",
            "Test:  [170/250]  eta: 0:00:18    time: 0.2183  data: 0.0278  max mem: 3024\n",
            "Test:  [180/250]  eta: 0:00:16    time: 0.2458  data: 0.0376  max mem: 3024\n",
            "Test:  [190/250]  eta: 0:00:14    time: 0.2627  data: 0.0415  max mem: 3024\n",
            "Test:  [200/250]  eta: 0:00:11    time: 0.2417  data: 0.0357  max mem: 3024\n",
            "Test:  [210/250]  eta: 0:00:09    time: 0.2179  data: 0.0240  max mem: 3024\n",
            "Test:  [220/250]  eta: 0:00:07    time: 0.2111  data: 0.0202  max mem: 3024\n",
            "Test:  [230/250]  eta: 0:00:04    time: 0.1966  data: 0.0203  max mem: 3024\n",
            "Test:  [240/250]  eta: 0:00:02    time: 0.1973  data: 0.0206  max mem: 3024\n",
            "Test:  [249/250]  eta: 0:00:00    time: 0.2019  data: 0.0215  max mem: 3024\n",
            "Test: Total time: 0:00:57 (0.2311 s / it)\n",
            "Averaged stats: \n",
            "Accumulating evaluation results...\n",
            "COCOeval_opt.accumulate() finished...\n",
            "DONE (t=1.32s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.112\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.221\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.100\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.046\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.137\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.277\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.119\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.261\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.318\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.160\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.370\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.578\n",
            " Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.576\n",
            " Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.302\n",
            "best_stat: {'epoch': 7, 'coco_eval_bbox': 0.11249756687089463}\n",
            "Epoch: [8]  [  0/500]  eta: 0:20:22  lr: 0.000013  loss: 36.4027 (36.4027)  loss_vfl: 0.4817 (0.4817)  loss_bbox: 0.1627 (0.1627)  loss_giou: 0.9950 (0.9950)  loss_fgl: 0.8237 (0.8237)  loss_vfl_aux_0: 0.4919 (0.4919)  loss_bbox_aux_0: 0.1995 (0.1995)  loss_giou_aux_0: 1.0628 (1.0628)  loss_fgl_aux_0: 0.8594 (0.8594)  loss_ddf_aux_0: 0.2297 (0.2297)  loss_vfl_aux_1: 0.5283 (0.5283)  loss_bbox_aux_1: 0.1726 (0.1726)  loss_giou_aux_1: 1.0236 (1.0236)  loss_fgl_aux_1: 0.8340 (0.8340)  loss_ddf_aux_1: 0.0418 (0.0418)  loss_vfl_aux_2: 0.5278 (0.5278)  loss_bbox_aux_2: 0.1613 (0.1613)  loss_giou_aux_2: 1.0014 (1.0014)  loss_fgl_aux_2: 0.8211 (0.8211)  loss_ddf_aux_2: 0.0037 (0.0037)  loss_vfl_aux_3: 0.4705 (0.4705)  loss_bbox_aux_3: 0.1619 (0.1619)  loss_giou_aux_3: 0.9962 (0.9962)  loss_fgl_aux_3: 0.8228 (0.8228)  loss_ddf_aux_3: 0.0006 (0.0006)  loss_vfl_aux_4: 0.4905 (0.4905)  loss_bbox_aux_4: 0.1629 (0.1629)  loss_giou_aux_4: 0.9953 (0.9953)  loss_fgl_aux_4: 0.8232 (0.8232)  loss_ddf_aux_4: 0.0001 (0.0001)  loss_vfl_pre: 0.4871 (0.4871)  loss_bbox_pre: 0.2011 (0.2011)  loss_giou_pre: 1.0636 (1.0636)  loss_vfl_enc_0: 0.5112 (0.5112)  loss_bbox_enc_0: 0.2903 (0.2903)  loss_giou_enc_0: 1.1695 (1.1695)  loss_vfl_dn_0: 0.4326 (0.4326)  loss_bbox_dn_0: 0.2339 (0.2339)  loss_giou_dn_0: 1.0341 (1.0341)  loss_fgl_dn_0: 0.9978 (0.9978)  loss_ddf_dn_0: 0.5247 (0.5247)  loss_vfl_dn_1: 0.4492 (0.4492)  loss_bbox_dn_1: 0.1768 (0.1768)  loss_giou_dn_1: 0.9010 (0.9010)  loss_fgl_dn_1: 0.9994 (0.9994)  loss_ddf_dn_1: 0.0823 (0.0823)  loss_vfl_dn_2: 0.4553 (0.4553)  loss_bbox_dn_2: 0.1553 (0.1553)  loss_giou_dn_2: 0.8635 (0.8635)  loss_fgl_dn_2: 0.9977 (0.9977)  loss_ddf_dn_2: 0.0120 (0.0120)  loss_vfl_dn_3: 0.4404 (0.4404)  loss_bbox_dn_3: 0.1540 (0.1540)  loss_giou_dn_3: 0.8588 (0.8588)  loss_fgl_dn_3: 0.9975 (0.9975)  loss_ddf_dn_3: 0.0030 (0.0030)  loss_vfl_dn_4: 0.4363 (0.4363)  loss_bbox_dn_4: 0.1488 (0.1488)  loss_giou_dn_4: 0.8560 (0.8560)  loss_fgl_dn_4: 0.9986 (0.9986)  loss_ddf_dn_4: 0.0001 (0.0001)  loss_vfl_dn_5: 0.4275 (0.4275)  loss_bbox_dn_5: 0.1487 (0.1487)  loss_giou_dn_5: 0.8561 (0.8561)  loss_fgl_dn_5: 0.9988 (0.9988)  loss_vfl_dn_pre: 0.4346 (0.4346)  loss_bbox_dn_pre: 0.2292 (0.2292)  loss_giou_dn_pre: 1.0297 (1.0297)  time: 2.4453  data: 1.0380  max mem: 3024\n",
            "Epoch: [8]  [100/500]  eta: 0:04:25  lr: 0.000013  loss: 36.3604 (37.3346)  loss_vfl: 0.5728 (0.6110)  loss_bbox: 0.0926 (0.1418)  loss_giou: 0.8437 (0.9327)  loss_fgl: 0.9904 (0.9910)  loss_vfl_aux_0: 0.6182 (0.6151)  loss_bbox_aux_0: 0.0990 (0.1595)  loss_giou_aux_0: 0.9059 (0.9735)  loss_fgl_aux_0: 0.9877 (0.9993)  loss_ddf_aux_0: 0.1568 (0.1692)  loss_vfl_aux_1: 0.5903 (0.6260)  loss_bbox_aux_1: 0.0928 (0.1444)  loss_giou_aux_1: 0.8551 (0.9381)  loss_fgl_aux_1: 0.9880 (0.9935)  loss_ddf_aux_1: 0.0219 (0.0270)  loss_vfl_aux_2: 0.5640 (0.6225)  loss_bbox_aux_2: 0.0926 (0.1420)  loss_giou_aux_2: 0.8465 (0.9337)  loss_fgl_aux_2: 0.9890 (0.9908)  loss_ddf_aux_2: 0.0027 (0.0038)  loss_vfl_aux_3: 0.5649 (0.6134)  loss_bbox_aux_3: 0.0925 (0.1418)  loss_giou_aux_3: 0.8437 (0.9328)  loss_fgl_aux_3: 0.9905 (0.9909)  loss_ddf_aux_3: 0.0004 (0.0005)  loss_vfl_aux_4: 0.5737 (0.6119)  loss_bbox_aux_4: 0.0926 (0.1418)  loss_giou_aux_4: 0.8438 (0.9326)  loss_fgl_aux_4: 0.9905 (0.9909)  loss_ddf_aux_4: 0.0001 (0.0001)  loss_vfl_pre: 0.6084 (0.6106)  loss_bbox_pre: 0.1013 (0.1589)  loss_giou_pre: 0.9038 (0.9731)  loss_vfl_enc_0: 0.6006 (0.5910)  loss_bbox_enc_0: 0.1228 (0.2089)  loss_giou_enc_0: 0.9948 (1.0910)  loss_vfl_dn_0: 0.4399 (0.4368)  loss_bbox_dn_0: 0.1006 (0.1515)  loss_giou_dn_0: 0.9261 (0.9754)  loss_fgl_dn_0: 1.0813 (1.0538)  loss_ddf_dn_0: 0.7437 (0.6319)  loss_vfl_dn_1: 0.4617 (0.4644)  loss_bbox_dn_1: 0.0836 (0.1238)  loss_giou_dn_1: 0.8303 (0.8610)  loss_fgl_dn_1: 1.0780 (1.0588)  loss_ddf_dn_1: 0.1059 (0.0905)  loss_vfl_dn_2: 0.4526 (0.4622)  loss_bbox_dn_2: 0.0834 (0.1177)  loss_giou_dn_2: 0.8103 (0.8426)  loss_fgl_dn_2: 1.0739 (1.0585)  loss_ddf_dn_2: 0.0182 (0.0132)  loss_vfl_dn_3: 0.4543 (0.4591)  loss_bbox_dn_3: 0.0823 (0.1161)  loss_giou_dn_3: 0.8134 (0.8386)  loss_fgl_dn_3: 1.0786 (1.0596)  loss_ddf_dn_3: 0.0016 (0.0017)  loss_vfl_dn_4: 0.4500 (0.4592)  loss_bbox_dn_4: 0.0822 (0.1155)  loss_giou_dn_4: 0.8143 (0.8379)  loss_fgl_dn_4: 1.0781 (1.0601)  loss_ddf_dn_4: 0.0002 (0.0002)  loss_vfl_dn_5: 0.4536 (0.4604)  loss_bbox_dn_5: 0.0825 (0.1156)  loss_giou_dn_5: 0.8147 (0.8378)  loss_fgl_dn_5: 1.0781 (1.0603)  loss_vfl_dn_pre: 0.4431 (0.4363)  loss_bbox_dn_pre: 0.1018 (0.1528)  loss_giou_dn_pre: 0.9108 (0.9763)  time: 0.6911  data: 0.0110  max mem: 3024\n",
            "Epoch: [8]  [200/500]  eta: 0:03:12  lr: 0.000013  loss: 36.1005 (37.3048)  loss_vfl: 0.5449 (0.6036)  loss_bbox: 0.1207 (0.1417)  loss_giou: 0.9388 (0.9296)  loss_fgl: 0.9186 (0.9880)  loss_vfl_aux_0: 0.5630 (0.6116)  loss_bbox_aux_0: 0.1281 (0.1600)  loss_giou_aux_0: 0.9626 (0.9707)  loss_fgl_aux_0: 0.9456 (0.9965)  loss_ddf_aux_0: 0.1693 (0.1678)  loss_vfl_aux_1: 0.5601 (0.6216)  loss_bbox_aux_1: 0.1220 (0.1444)  loss_giou_aux_1: 0.9433 (0.9362)  loss_fgl_aux_1: 0.9249 (0.9903)  loss_ddf_aux_1: 0.0364 (0.0295)  loss_vfl_aux_2: 0.5684 (0.6177)  loss_bbox_aux_2: 0.1215 (0.1418)  loss_giou_aux_2: 0.9385 (0.9306)  loss_fgl_aux_2: 0.9184 (0.9878)  loss_ddf_aux_2: 0.0045 (0.0043)  loss_vfl_aux_3: 0.5400 (0.6080)  loss_bbox_aux_3: 0.1209 (0.1417)  loss_giou_aux_3: 0.9381 (0.9297)  loss_fgl_aux_3: 0.9201 (0.9880)  loss_ddf_aux_3: 0.0004 (0.0005)  loss_vfl_aux_4: 0.5493 (0.6038)  loss_bbox_aux_4: 0.1207 (0.1417)  loss_giou_aux_4: 0.9383 (0.9296)  loss_fgl_aux_4: 0.9188 (0.9880)  loss_ddf_aux_4: 0.0001 (0.0001)  loss_vfl_pre: 0.5586 (0.6085)  loss_bbox_pre: 0.1287 (0.1600)  loss_giou_pre: 0.9666 (0.9697)  loss_vfl_enc_0: 0.5190 (0.5944)  loss_bbox_enc_0: 0.1601 (0.2113)  loss_giou_enc_0: 1.1304 (1.0897)  loss_vfl_dn_0: 0.4341 (0.4369)  loss_bbox_dn_0: 0.1190 (0.1579)  loss_giou_dn_0: 0.9607 (0.9749)  loss_fgl_dn_0: 1.0509 (1.0556)  loss_ddf_dn_0: 0.5267 (0.6335)  loss_vfl_dn_1: 0.4587 (0.4645)  loss_bbox_dn_1: 0.0892 (0.1274)  loss_giou_dn_1: 0.8568 (0.8599)  loss_fgl_dn_1: 1.0455 (1.0603)  loss_ddf_dn_1: 0.0800 (0.0941)  loss_vfl_dn_2: 0.4553 (0.4623)  loss_bbox_dn_2: 0.0894 (0.1211)  loss_giou_dn_2: 0.8470 (0.8415)  loss_fgl_dn_2: 1.0365 (1.0598)  loss_ddf_dn_2: 0.0114 (0.0141)  loss_vfl_dn_3: 0.4500 (0.4594)  loss_bbox_dn_3: 0.0873 (0.1195)  loss_giou_dn_3: 0.8465 (0.8379)  loss_fgl_dn_3: 1.0400 (1.0607)  loss_ddf_dn_3: 0.0014 (0.0018)  loss_vfl_dn_4: 0.4465 (0.4583)  loss_bbox_dn_4: 0.0861 (0.1189)  loss_giou_dn_4: 0.8462 (0.8373)  loss_fgl_dn_4: 1.0389 (1.0611)  loss_ddf_dn_4: 0.0001 (0.0002)  loss_vfl_dn_5: 0.4441 (0.4591)  loss_bbox_dn_5: 0.0859 (0.1189)  loss_giou_dn_5: 0.8462 (0.8372)  loss_fgl_dn_5: 1.0386 (1.0613)  loss_vfl_dn_pre: 0.4316 (0.4366)  loss_bbox_dn_pre: 0.1197 (0.1588)  loss_giou_dn_pre: 0.9593 (0.9753)  time: 0.5832  data: 0.0076  max mem: 3024\n",
            "Epoch: [8]  [300/500]  eta: 0:02:09  lr: 0.000013  loss: 36.3580 (37.2681)  loss_vfl: 0.5679 (0.6011)  loss_bbox: 0.0985 (0.1395)  loss_giou: 0.9627 (0.9494)  loss_fgl: 0.9652 (0.9754)  loss_vfl_aux_0: 0.5581 (0.6035)  loss_bbox_aux_0: 0.1180 (0.1558)  loss_giou_aux_0: 0.9967 (0.9890)  loss_fgl_aux_0: 0.9831 (0.9831)  loss_ddf_aux_0: 0.1386 (0.1644)  loss_vfl_aux_1: 0.5781 (0.6154)  loss_bbox_aux_1: 0.0988 (0.1422)  loss_giou_aux_1: 0.9593 (0.9558)  loss_fgl_aux_1: 0.9761 (0.9776)  loss_ddf_aux_1: 0.0299 (0.0298)  loss_vfl_aux_2: 0.5786 (0.6152)  loss_bbox_aux_2: 0.0985 (0.1396)  loss_giou_aux_2: 0.9657 (0.9503)  loss_fgl_aux_2: 0.9673 (0.9752)  loss_ddf_aux_2: 0.0035 (0.0042)  loss_vfl_aux_3: 0.5664 (0.6052)  loss_bbox_aux_3: 0.0986 (0.1395)  loss_giou_aux_3: 0.9627 (0.9495)  loss_fgl_aux_3: 0.9654 (0.9753)  loss_ddf_aux_3: 0.0003 (0.0005)  loss_vfl_aux_4: 0.5747 (0.6003)  loss_bbox_aux_4: 0.0985 (0.1394)  loss_giou_aux_4: 0.9626 (0.9494)  loss_fgl_aux_4: 0.9651 (0.9754)  loss_ddf_aux_4: 0.0001 (0.0001)  loss_vfl_pre: 0.5449 (0.6008)  loss_bbox_pre: 0.1190 (0.1561)  loss_giou_pre: 0.9915 (0.9878)  loss_vfl_enc_0: 0.5259 (0.5865)  loss_bbox_enc_0: 0.1348 (0.2042)  loss_giou_enc_0: 1.1079 (1.1061)  loss_vfl_dn_0: 0.4363 (0.4358)  loss_bbox_dn_0: 0.0982 (0.1512)  loss_giou_dn_0: 0.9907 (0.9817)  loss_fgl_dn_0: 1.0307 (1.0504)  loss_ddf_dn_0: 0.4982 (0.6153)  loss_vfl_dn_1: 0.4553 (0.4638)  loss_bbox_dn_1: 0.0819 (0.1220)  loss_giou_dn_1: 0.8686 (0.8679)  loss_fgl_dn_1: 1.0354 (1.0551)  loss_ddf_dn_1: 0.0682 (0.0912)  loss_vfl_dn_2: 0.4568 (0.4617)  loss_bbox_dn_2: 0.0792 (0.1161)  loss_giou_dn_2: 0.8545 (0.8501)  loss_fgl_dn_2: 1.0382 (1.0547)  loss_ddf_dn_2: 0.0080 (0.0135)  loss_vfl_dn_3: 0.4460 (0.4587)  loss_bbox_dn_3: 0.0771 (0.1145)  loss_giou_dn_3: 0.8521 (0.8467)  loss_fgl_dn_3: 1.0386 (1.0556)  loss_ddf_dn_3: 0.0007 (0.0018)  loss_vfl_dn_4: 0.4478 (0.4572)  loss_bbox_dn_4: 0.0764 (0.1140)  loss_giou_dn_4: 0.8506 (0.8461)  loss_fgl_dn_4: 1.0387 (1.0560)  loss_ddf_dn_4: 0.0001 (0.0002)  loss_vfl_dn_5: 0.4480 (0.4580)  loss_bbox_dn_5: 0.0763 (0.1140)  loss_giou_dn_5: 0.8498 (0.8460)  loss_fgl_dn_5: 1.0387 (1.0562)  loss_vfl_dn_pre: 0.4333 (0.4356)  loss_bbox_dn_pre: 0.0986 (0.1518)  loss_giou_dn_pre: 0.9939 (0.9821)  time: 0.5554  data: 0.0076  max mem: 3024\n",
            "Epoch: [8]  [400/500]  eta: 0:01:04  lr: 0.000013  loss: 36.7351 (37.3400)  loss_vfl: 0.5908 (0.5994)  loss_bbox: 0.1305 (0.1434)  loss_giou: 0.8619 (0.9564)  loss_fgl: 1.0218 (0.9730)  loss_vfl_aux_0: 0.5737 (0.5999)  loss_bbox_aux_0: 0.1257 (0.1607)  loss_giou_aux_0: 0.9032 (0.9953)  loss_fgl_aux_0: 1.0196 (0.9804)  loss_ddf_aux_0: 0.1506 (0.1608)  loss_vfl_aux_1: 0.6118 (0.6141)  loss_bbox_aux_1: 0.1234 (0.1462)  loss_giou_aux_1: 0.8656 (0.9625)  loss_fgl_aux_1: 1.0279 (0.9751)  loss_ddf_aux_1: 0.0239 (0.0291)  loss_vfl_aux_2: 0.6177 (0.6128)  loss_bbox_aux_2: 0.1301 (0.1436)  loss_giou_aux_2: 0.8609 (0.9571)  loss_fgl_aux_2: 1.0250 (0.9728)  loss_ddf_aux_2: 0.0027 (0.0040)  loss_vfl_aux_3: 0.6045 (0.6039)  loss_bbox_aux_3: 0.1304 (0.1434)  loss_giou_aux_3: 0.8616 (0.9564)  loss_fgl_aux_3: 1.0221 (0.9728)  loss_ddf_aux_3: 0.0004 (0.0005)  loss_vfl_aux_4: 0.5938 (0.5984)  loss_bbox_aux_4: 0.1307 (0.1434)  loss_giou_aux_4: 0.8619 (0.9563)  loss_fgl_aux_4: 1.0220 (0.9729)  loss_ddf_aux_4: 0.0001 (0.0001)  loss_vfl_pre: 0.5776 (0.5974)  loss_bbox_pre: 0.1264 (0.1604)  loss_giou_pre: 0.9014 (0.9939)  loss_vfl_enc_0: 0.5332 (0.5848)  loss_bbox_enc_0: 0.1704 (0.2064)  loss_giou_enc_0: 1.0201 (1.1071)  loss_vfl_dn_0: 0.4331 (0.4357)  loss_bbox_dn_0: 0.1270 (0.1541)  loss_giou_dn_0: 0.9673 (0.9832)  loss_fgl_dn_0: 1.0657 (1.0492)  loss_ddf_dn_0: 0.5284 (0.5994)  loss_vfl_dn_1: 0.4634 (0.4626)  loss_bbox_dn_1: 0.1114 (0.1262)  loss_giou_dn_1: 0.8299 (0.8721)  loss_fgl_dn_1: 1.0586 (1.0552)  loss_ddf_dn_1: 0.0640 (0.0883)  loss_vfl_dn_2: 0.4534 (0.4612)  loss_bbox_dn_2: 0.1046 (0.1203)  loss_giou_dn_2: 0.8140 (0.8546)  loss_fgl_dn_2: 1.0592 (1.0551)  loss_ddf_dn_2: 0.0094 (0.0128)  loss_vfl_dn_3: 0.4468 (0.4582)  loss_bbox_dn_3: 0.1030 (0.1189)  loss_giou_dn_3: 0.8151 (0.8515)  loss_fgl_dn_3: 1.0604 (1.0559)  loss_ddf_dn_3: 0.0016 (0.0017)  loss_vfl_dn_4: 0.4521 (0.4570)  loss_bbox_dn_4: 0.1028 (0.1184)  loss_giou_dn_4: 0.8157 (0.8509)  loss_fgl_dn_4: 1.0602 (1.0563)  loss_ddf_dn_4: 0.0002 (0.0002)  loss_vfl_dn_5: 0.4512 (0.4577)  loss_bbox_dn_5: 0.1030 (0.1183)  loss_giou_dn_5: 0.8155 (0.8508)  loss_fgl_dn_5: 1.0600 (1.0564)  loss_vfl_dn_pre: 0.4324 (0.4356)  loss_bbox_dn_pre: 0.1257 (0.1538)  loss_giou_dn_pre: 0.9437 (0.9834)  time: 0.6256  data: 0.0092  max mem: 3024\n",
            "Epoch: [8]  [499/500]  eta: 0:00:00  lr: 0.000013  loss: 36.8794 (37.3681)  loss_vfl: 0.6470 (0.6013)  loss_bbox: 0.1276 (0.1455)  loss_giou: 0.8564 (0.9526)  loss_fgl: 1.0361 (0.9737)  loss_vfl_aux_0: 0.6270 (0.6070)  loss_bbox_aux_0: 0.1417 (0.1624)  loss_giou_aux_0: 0.8743 (0.9917)  loss_fgl_aux_0: 1.0697 (0.9821)  loss_ddf_aux_0: 0.1750 (0.1645)  loss_vfl_aux_1: 0.6475 (0.6173)  loss_bbox_aux_1: 0.1296 (0.1483)  loss_giou_aux_1: 0.8389 (0.9587)  loss_fgl_aux_1: 1.0493 (0.9762)  loss_ddf_aux_1: 0.0317 (0.0300)  loss_vfl_aux_2: 0.6655 (0.6176)  loss_bbox_aux_2: 0.1273 (0.1456)  loss_giou_aux_2: 0.8558 (0.9533)  loss_fgl_aux_2: 1.0405 (0.9736)  loss_ddf_aux_2: 0.0028 (0.0040)  loss_vfl_aux_3: 0.6387 (0.6067)  loss_bbox_aux_3: 0.1275 (0.1455)  loss_giou_aux_3: 0.8551 (0.9527)  loss_fgl_aux_3: 1.0373 (0.9736)  loss_ddf_aux_3: 0.0004 (0.0005)  loss_vfl_aux_4: 0.6255 (0.6003)  loss_bbox_aux_4: 0.1276 (0.1454)  loss_giou_aux_4: 0.8561 (0.9526)  loss_fgl_aux_4: 1.0359 (0.9737)  loss_ddf_aux_4: 0.0001 (0.0001)  loss_vfl_pre: 0.6201 (0.6040)  loss_bbox_pre: 0.1410 (0.1624)  loss_giou_pre: 0.8718 (0.9906)  loss_vfl_enc_0: 0.6538 (0.5894)  loss_bbox_enc_0: 0.1849 (0.2108)  loss_giou_enc_0: 0.9876 (1.1042)  loss_vfl_dn_0: 0.4490 (0.4360)  loss_bbox_dn_0: 0.1375 (0.1560)  loss_giou_dn_0: 0.9350 (0.9820)  loss_fgl_dn_0: 1.0641 (1.0494)  loss_ddf_dn_0: 0.5301 (0.5945)  loss_vfl_dn_1: 0.4814 (0.4626)  loss_bbox_dn_1: 0.1083 (0.1274)  loss_giou_dn_1: 0.8185 (0.8710)  loss_fgl_dn_1: 1.0807 (1.0551)  loss_ddf_dn_1: 0.0768 (0.0879)  loss_vfl_dn_2: 0.4800 (0.4610)  loss_bbox_dn_2: 0.1015 (0.1214)  loss_giou_dn_2: 0.8022 (0.8534)  loss_fgl_dn_2: 1.0791 (1.0548)  loss_ddf_dn_2: 0.0080 (0.0126)  loss_vfl_dn_3: 0.4717 (0.4581)  loss_bbox_dn_3: 0.0997 (0.1199)  loss_giou_dn_3: 0.8026 (0.8502)  loss_fgl_dn_3: 1.0745 (1.0557)  loss_ddf_dn_3: 0.0009 (0.0016)  loss_vfl_dn_4: 0.4673 (0.4569)  loss_bbox_dn_4: 0.0999 (0.1195)  loss_giou_dn_4: 0.8036 (0.8497)  loss_fgl_dn_4: 1.0739 (1.0560)  loss_ddf_dn_4: 0.0002 (0.0002)  loss_vfl_dn_5: 0.4670 (0.4576)  loss_bbox_dn_5: 0.1000 (0.1194)  loss_giou_dn_5: 0.8041 (0.8496)  loss_fgl_dn_5: 1.0738 (1.0562)  loss_vfl_dn_pre: 0.4517 (0.4356)  loss_bbox_dn_pre: 0.1381 (0.1559)  loss_giou_dn_pre: 0.9436 (0.9831)  time: 0.6289  data: 0.0086  max mem: 3024\n",
            "Epoch: [8] Total time: 0:05:20 (0.6410 s / it)\n",
            "Averaged stats: lr: 0.000013  loss: 36.8794 (37.3681)  loss_vfl: 0.6470 (0.6013)  loss_bbox: 0.1276 (0.1455)  loss_giou: 0.8564 (0.9526)  loss_fgl: 1.0361 (0.9737)  loss_vfl_aux_0: 0.6270 (0.6070)  loss_bbox_aux_0: 0.1417 (0.1624)  loss_giou_aux_0: 0.8743 (0.9917)  loss_fgl_aux_0: 1.0697 (0.9821)  loss_ddf_aux_0: 0.1750 (0.1645)  loss_vfl_aux_1: 0.6475 (0.6173)  loss_bbox_aux_1: 0.1296 (0.1483)  loss_giou_aux_1: 0.8389 (0.9587)  loss_fgl_aux_1: 1.0493 (0.9762)  loss_ddf_aux_1: 0.0317 (0.0300)  loss_vfl_aux_2: 0.6655 (0.6176)  loss_bbox_aux_2: 0.1273 (0.1456)  loss_giou_aux_2: 0.8558 (0.9533)  loss_fgl_aux_2: 1.0405 (0.9736)  loss_ddf_aux_2: 0.0028 (0.0040)  loss_vfl_aux_3: 0.6387 (0.6067)  loss_bbox_aux_3: 0.1275 (0.1455)  loss_giou_aux_3: 0.8551 (0.9527)  loss_fgl_aux_3: 1.0373 (0.9736)  loss_ddf_aux_3: 0.0004 (0.0005)  loss_vfl_aux_4: 0.6255 (0.6003)  loss_bbox_aux_4: 0.1276 (0.1454)  loss_giou_aux_4: 0.8561 (0.9526)  loss_fgl_aux_4: 1.0359 (0.9737)  loss_ddf_aux_4: 0.0001 (0.0001)  loss_vfl_pre: 0.6201 (0.6040)  loss_bbox_pre: 0.1410 (0.1624)  loss_giou_pre: 0.8718 (0.9906)  loss_vfl_enc_0: 0.6538 (0.5894)  loss_bbox_enc_0: 0.1849 (0.2108)  loss_giou_enc_0: 0.9876 (1.1042)  loss_vfl_dn_0: 0.4490 (0.4360)  loss_bbox_dn_0: 0.1375 (0.1560)  loss_giou_dn_0: 0.9350 (0.9820)  loss_fgl_dn_0: 1.0641 (1.0494)  loss_ddf_dn_0: 0.5301 (0.5945)  loss_vfl_dn_1: 0.4814 (0.4626)  loss_bbox_dn_1: 0.1083 (0.1274)  loss_giou_dn_1: 0.8185 (0.8710)  loss_fgl_dn_1: 1.0807 (1.0551)  loss_ddf_dn_1: 0.0768 (0.0879)  loss_vfl_dn_2: 0.4800 (0.4610)  loss_bbox_dn_2: 0.1015 (0.1214)  loss_giou_dn_2: 0.8022 (0.8534)  loss_fgl_dn_2: 1.0791 (1.0548)  loss_ddf_dn_2: 0.0080 (0.0126)  loss_vfl_dn_3: 0.4717 (0.4581)  loss_bbox_dn_3: 0.0997 (0.1199)  loss_giou_dn_3: 0.8026 (0.8502)  loss_fgl_dn_3: 1.0745 (1.0557)  loss_ddf_dn_3: 0.0009 (0.0016)  loss_vfl_dn_4: 0.4673 (0.4569)  loss_bbox_dn_4: 0.0999 (0.1195)  loss_giou_dn_4: 0.8036 (0.8497)  loss_fgl_dn_4: 1.0739 (1.0560)  loss_ddf_dn_4: 0.0002 (0.0002)  loss_vfl_dn_5: 0.4670 (0.4576)  loss_bbox_dn_5: 0.1000 (0.1194)  loss_giou_dn_5: 0.8041 (0.8496)  loss_fgl_dn_5: 1.0738 (1.0562)  loss_vfl_dn_pre: 0.4517 (0.4356)  loss_bbox_dn_pre: 0.1381 (0.1559)  loss_giou_dn_pre: 0.9436 (0.9831)\n",
            "Test:  [  0/250]  eta: 0:05:00    time: 1.2036  data: 0.8848  max mem: 3024\n",
            "Test:  [ 10/250]  eta: 0:01:09    time: 0.2883  data: 0.0963  max mem: 3024\n",
            "Test:  [ 20/250]  eta: 0:00:59    time: 0.2130  data: 0.0230  max mem: 3024\n",
            "Test:  [ 30/250]  eta: 0:00:57    time: 0.2442  data: 0.0361  max mem: 3024\n",
            "Test:  [ 40/250]  eta: 0:00:55    time: 0.2653  data: 0.0457  max mem: 3024\n",
            "Test:  [ 50/250]  eta: 0:00:51    time: 0.2595  data: 0.0366  max mem: 3024\n",
            "Test:  [ 60/250]  eta: 0:00:47    time: 0.2240  data: 0.0230  max mem: 3024\n",
            "Test:  [ 70/250]  eta: 0:00:43    time: 0.2000  data: 0.0209  max mem: 3024\n",
            "Test:  [ 80/250]  eta: 0:00:40    time: 0.1987  data: 0.0207  max mem: 3024\n",
            "Test:  [ 90/250]  eta: 0:00:37    time: 0.1987  data: 0.0205  max mem: 3024\n",
            "Test:  [100/250]  eta: 0:00:35    time: 0.2249  data: 0.0274  max mem: 3024\n",
            "Test:  [110/250]  eta: 0:00:33    time: 0.2807  data: 0.0385  max mem: 3024\n",
            "Test:  [120/250]  eta: 0:00:31    time: 0.2726  data: 0.0384  max mem: 3024\n",
            "Test:  [130/250]  eta: 0:00:28    time: 0.2173  data: 0.0271  max mem: 3024\n",
            "Test:  [140/250]  eta: 0:00:25    time: 0.1991  data: 0.0207  max mem: 3024\n",
            "Test:  [150/250]  eta: 0:00:23    time: 0.1984  data: 0.0210  max mem: 3024\n",
            "Test:  [160/250]  eta: 0:00:20    time: 0.2152  data: 0.0214  max mem: 3024\n",
            "Test:  [170/250]  eta: 0:00:18    time: 0.2311  data: 0.0268  max mem: 3024\n",
            "Test:  [180/250]  eta: 0:00:16    time: 0.2423  data: 0.0337  max mem: 3024\n",
            "Test:  [190/250]  eta: 0:00:14    time: 0.2621  data: 0.0405  max mem: 3024\n",
            "Test:  [200/250]  eta: 0:00:11    time: 0.2535  data: 0.0387  max mem: 3024\n",
            "Test:  [210/250]  eta: 0:00:09    time: 0.2315  data: 0.0261  max mem: 3024\n",
            "Test:  [220/250]  eta: 0:00:07    time: 0.2117  data: 0.0203  max mem: 3024\n",
            "Test:  [230/250]  eta: 0:00:04    time: 0.1971  data: 0.0206  max mem: 3024\n",
            "Test:  [240/250]  eta: 0:00:02    time: 0.1964  data: 0.0202  max mem: 3024\n",
            "Test:  [249/250]  eta: 0:00:00    time: 0.1988  data: 0.0200  max mem: 3024\n",
            "Test: Total time: 0:00:57 (0.2306 s / it)\n",
            "Averaged stats: \n",
            "Accumulating evaluation results...\n",
            "COCOeval_opt.accumulate() finished...\n",
            "DONE (t=1.32s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.118\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.237\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.106\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.049\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.154\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.286\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.117\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.259\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.174\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.374\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.569\n",
            " Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.587\n",
            " Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.302\n",
            "best_stat: {'epoch': 8, 'coco_eval_bbox': 0.11835711916976474}\n",
            "Epoch: [9]  [  0/500]  eta: 0:21:25  lr: 0.000013  loss: 37.0793 (37.0793)  loss_vfl: 0.6841 (0.6841)  loss_bbox: 0.1114 (0.1114)  loss_giou: 0.8540 (0.8540)  loss_fgl: 1.0505 (1.0505)  loss_vfl_aux_0: 0.7783 (0.7783)  loss_bbox_aux_0: 0.1157 (0.1157)  loss_giou_aux_0: 0.8694 (0.8694)  loss_fgl_aux_0: 1.0589 (1.0589)  loss_ddf_aux_0: 0.1165 (0.1165)  loss_vfl_aux_1: 0.7563 (0.7563)  loss_bbox_aux_1: 0.1168 (0.1168)  loss_giou_aux_1: 0.8648 (0.8648)  loss_fgl_aux_1: 1.0548 (1.0548)  loss_ddf_aux_1: 0.0278 (0.0278)  loss_vfl_aux_2: 0.6997 (0.6997)  loss_bbox_aux_2: 0.1107 (0.1107)  loss_giou_aux_2: 0.8548 (0.8548)  loss_fgl_aux_2: 1.0482 (1.0482)  loss_ddf_aux_2: 0.0038 (0.0038)  loss_vfl_aux_3: 0.7046 (0.7046)  loss_bbox_aux_3: 0.1106 (0.1106)  loss_giou_aux_3: 0.8534 (0.8534)  loss_fgl_aux_3: 1.0494 (1.0494)  loss_ddf_aux_3: 0.0007 (0.0007)  loss_vfl_aux_4: 0.6958 (0.6958)  loss_bbox_aux_4: 0.1112 (0.1112)  loss_giou_aux_4: 0.8542 (0.8542)  loss_fgl_aux_4: 1.0495 (1.0495)  loss_ddf_aux_4: 0.0002 (0.0002)  loss_vfl_pre: 0.7861 (0.7861)  loss_bbox_pre: 0.1144 (0.1144)  loss_giou_pre: 0.8674 (0.8674)  loss_vfl_enc_0: 0.7651 (0.7651)  loss_bbox_enc_0: 0.1720 (0.1720)  loss_giou_enc_0: 0.9779 (0.9779)  loss_vfl_dn_0: 0.4556 (0.4556)  loss_bbox_dn_0: 0.1285 (0.1285)  loss_giou_dn_0: 0.9711 (0.9711)  loss_fgl_dn_0: 1.0520 (1.0520)  loss_ddf_dn_0: 0.3151 (0.3151)  loss_vfl_dn_1: 0.4719 (0.4719)  loss_bbox_dn_1: 0.1070 (0.1070)  loss_giou_dn_1: 0.8535 (0.8535)  loss_fgl_dn_1: 1.0548 (1.0548)  loss_ddf_dn_1: 0.0355 (0.0355)  loss_vfl_dn_2: 0.4661 (0.4661)  loss_bbox_dn_2: 0.0964 (0.0964)  loss_giou_dn_2: 0.8390 (0.8390)  loss_fgl_dn_2: 1.0467 (1.0467)  loss_ddf_dn_2: 0.0043 (0.0043)  loss_vfl_dn_3: 0.4705 (0.4705)  loss_bbox_dn_3: 0.0964 (0.0964)  loss_giou_dn_3: 0.8357 (0.8357)  loss_fgl_dn_3: 1.0472 (1.0472)  loss_ddf_dn_3: 0.0007 (0.0007)  loss_vfl_dn_4: 0.4595 (0.4595)  loss_bbox_dn_4: 0.0970 (0.0970)  loss_giou_dn_4: 0.8363 (0.8363)  loss_fgl_dn_4: 1.0470 (1.0470)  loss_ddf_dn_4: 0.0002 (0.0002)  loss_vfl_dn_5: 0.4595 (0.4595)  loss_bbox_dn_5: 0.0973 (0.0973)  loss_giou_dn_5: 0.8368 (0.8368)  loss_fgl_dn_5: 1.0472 (1.0472)  loss_vfl_dn_pre: 0.4529 (0.4529)  loss_bbox_dn_pre: 0.1315 (0.1315)  loss_giou_dn_pre: 0.9772 (0.9772)  time: 2.5714  data: 1.0646  max mem: 3024\n",
            "Epoch: [9]  [100/500]  eta: 0:04:36  lr: 0.000013  loss: 36.5515 (36.9429)  loss_vfl: 0.5908 (0.5992)  loss_bbox: 0.1149 (0.1284)  loss_giou: 0.9133 (0.9398)  loss_fgl: 0.9838 (0.9824)  loss_vfl_aux_0: 0.5767 (0.6008)  loss_bbox_aux_0: 0.1212 (0.1447)  loss_giou_aux_0: 0.9832 (0.9745)  loss_fgl_aux_0: 0.9804 (0.9934)  loss_ddf_aux_0: 0.1784 (0.1721)  loss_vfl_aux_1: 0.6421 (0.6194)  loss_bbox_aux_1: 0.1172 (0.1318)  loss_giou_aux_1: 0.9253 (0.9455)  loss_fgl_aux_1: 0.9828 (0.9857)  loss_ddf_aux_1: 0.0316 (0.0305)  loss_vfl_aux_2: 0.6211 (0.6087)  loss_bbox_aux_2: 0.1150 (0.1285)  loss_giou_aux_2: 0.9154 (0.9406)  loss_fgl_aux_2: 0.9800 (0.9821)  loss_ddf_aux_2: 0.0026 (0.0031)  loss_vfl_aux_3: 0.6108 (0.5991)  loss_bbox_aux_3: 0.1149 (0.1284)  loss_giou_aux_3: 0.9140 (0.9400)  loss_fgl_aux_3: 0.9826 (0.9822)  loss_ddf_aux_3: 0.0002 (0.0004)  loss_vfl_aux_4: 0.5864 (0.5970)  loss_bbox_aux_4: 0.1148 (0.1284)  loss_giou_aux_4: 0.9135 (0.9399)  loss_fgl_aux_4: 0.9841 (0.9823)  loss_ddf_aux_4: 0.0001 (0.0001)  loss_vfl_pre: 0.5918 (0.5986)  loss_bbox_pre: 0.1227 (0.1447)  loss_giou_pre: 0.9731 (0.9728)  loss_vfl_enc_0: 0.6133 (0.5975)  loss_bbox_enc_0: 0.1487 (0.1884)  loss_giou_enc_0: 1.0722 (1.0759)  loss_vfl_dn_0: 0.4265 (0.4368)  loss_bbox_dn_0: 0.0969 (0.1399)  loss_giou_dn_0: 0.9921 (0.9702)  loss_fgl_dn_0: 1.0257 (1.0530)  loss_ddf_dn_0: 0.5459 (0.5878)  loss_vfl_dn_1: 0.4529 (0.4607)  loss_bbox_dn_1: 0.0713 (0.1120)  loss_giou_dn_1: 0.8683 (0.8620)  loss_fgl_dn_1: 1.0257 (1.0549)  loss_ddf_dn_1: 0.0663 (0.0844)  loss_vfl_dn_2: 0.4482 (0.4584)  loss_bbox_dn_2: 0.0674 (0.1067)  loss_giou_dn_2: 0.8605 (0.8454)  loss_fgl_dn_2: 1.0281 (1.0534)  loss_ddf_dn_2: 0.0079 (0.0112)  loss_vfl_dn_3: 0.4475 (0.4561)  loss_bbox_dn_3: 0.0676 (0.1056)  loss_giou_dn_3: 0.8561 (0.8423)  loss_fgl_dn_3: 1.0281 (1.0542)  loss_ddf_dn_3: 0.0009 (0.0013)  loss_vfl_dn_4: 0.4456 (0.4537)  loss_bbox_dn_4: 0.0683 (0.1053)  loss_giou_dn_4: 0.8562 (0.8416)  loss_fgl_dn_4: 1.0281 (1.0545)  loss_ddf_dn_4: 0.0001 (0.0001)  loss_vfl_dn_5: 0.4500 (0.4544)  loss_bbox_dn_5: 0.0683 (0.1052)  loss_giou_dn_5: 0.8568 (0.8416)  loss_fgl_dn_5: 1.0281 (1.0546)  loss_vfl_dn_pre: 0.4233 (0.4360)  loss_bbox_dn_pre: 0.0961 (0.1406)  loss_giou_dn_pre: 0.9849 (0.9720)  time: 0.6821  data: 0.0083  max mem: 3024\n",
            "Epoch: [9]  [200/500]  eta: 0:03:19  lr: 0.000013  loss: 36.2903 (36.9239)  loss_vfl: 0.5303 (0.5898)  loss_bbox: 0.1016 (0.1286)  loss_giou: 0.9465 (0.9391)  loss_fgl: 0.9484 (0.9729)  loss_vfl_aux_0: 0.5303 (0.5953)  loss_bbox_aux_0: 0.1246 (0.1475)  loss_giou_aux_0: 0.9997 (0.9811)  loss_fgl_aux_0: 0.9643 (0.9843)  loss_ddf_aux_0: 0.2169 (0.1868)  loss_vfl_aux_1: 0.5972 (0.6127)  loss_bbox_aux_1: 0.1007 (0.1325)  loss_giou_aux_1: 0.9501 (0.9457)  loss_fgl_aux_1: 0.9475 (0.9759)  loss_ddf_aux_1: 0.0327 (0.0335)  loss_vfl_aux_2: 0.6318 (0.6063)  loss_bbox_aux_2: 0.1011 (0.1290)  loss_giou_aux_2: 0.9475 (0.9400)  loss_fgl_aux_2: 0.9521 (0.9725)  loss_ddf_aux_2: 0.0030 (0.0037)  loss_vfl_aux_3: 0.5879 (0.5941)  loss_bbox_aux_3: 0.1015 (0.1287)  loss_giou_aux_3: 0.9472 (0.9393)  loss_fgl_aux_3: 0.9489 (0.9726)  loss_ddf_aux_3: 0.0005 (0.0005)  loss_vfl_aux_4: 0.5439 (0.5887)  loss_bbox_aux_4: 0.1016 (0.1286)  loss_giou_aux_4: 0.9464 (0.9392)  loss_fgl_aux_4: 0.9483 (0.9728)  loss_ddf_aux_4: 0.0001 (0.0001)  loss_vfl_pre: 0.5303 (0.5931)  loss_bbox_pre: 0.1260 (0.1476)  loss_giou_pre: 1.0070 (0.9803)  loss_vfl_enc_0: 0.5840 (0.5854)  loss_bbox_enc_0: 0.1714 (0.1969)  loss_giou_enc_0: 1.1587 (1.0949)  loss_vfl_dn_0: 0.4177 (0.4368)  loss_bbox_dn_0: 0.1362 (0.1455)  loss_giou_dn_0: 0.9654 (0.9716)  loss_fgl_dn_0: 1.0381 (1.0527)  loss_ddf_dn_0: 0.6342 (0.5992)  loss_vfl_dn_1: 0.4507 (0.4615)  loss_bbox_dn_1: 0.1058 (0.1162)  loss_giou_dn_1: 0.8659 (0.8609)  loss_fgl_dn_1: 1.0388 (1.0528)  loss_ddf_dn_1: 0.0836 (0.0880)  loss_vfl_dn_2: 0.4563 (0.4582)  loss_bbox_dn_2: 0.1002 (0.1107)  loss_giou_dn_2: 0.8564 (0.8442)  loss_fgl_dn_2: 1.0384 (1.0516)  loss_ddf_dn_2: 0.0113 (0.0125)  loss_vfl_dn_3: 0.4509 (0.4542)  loss_bbox_dn_3: 0.0979 (0.1093)  loss_giou_dn_3: 0.8562 (0.8410)  loss_fgl_dn_3: 1.0390 (1.0524)  loss_ddf_dn_3: 0.0016 (0.0015)  loss_vfl_dn_4: 0.4434 (0.4518)  loss_bbox_dn_4: 0.0965 (0.1089)  loss_giou_dn_4: 0.8560 (0.8403)  loss_fgl_dn_4: 1.0390 (1.0527)  loss_ddf_dn_4: 0.0002 (0.0002)  loss_vfl_dn_5: 0.4424 (0.4523)  loss_bbox_dn_5: 0.0965 (0.1088)  loss_giou_dn_5: 0.8564 (0.8403)  loss_fgl_dn_5: 1.0390 (1.0528)  loss_vfl_dn_pre: 0.4185 (0.4362)  loss_bbox_dn_pre: 0.1317 (0.1460)  loss_giou_dn_pre: 0.9587 (0.9729)  time: 0.6645  data: 0.0093  max mem: 3024\n",
            "Epoch: [9]  [300/500]  eta: 0:02:11  lr: 0.000013  loss: 36.7209 (36.9091)  loss_vfl: 0.6206 (0.5855)  loss_bbox: 0.1217 (0.1293)  loss_giou: 0.8550 (0.9497)  loss_fgl: 0.9837 (0.9681)  loss_vfl_aux_0: 0.5894 (0.5913)  loss_bbox_aux_0: 0.1259 (0.1456)  loss_giou_aux_0: 0.9027 (0.9903)  loss_fgl_aux_0: 0.9997 (0.9774)  loss_ddf_aux_0: 0.1327 (0.1781)  loss_vfl_aux_1: 0.6006 (0.6046)  loss_bbox_aux_1: 0.1225 (0.1324)  loss_giou_aux_1: 0.8551 (0.9559)  loss_fgl_aux_1: 0.9883 (0.9706)  loss_ddf_aux_1: 0.0219 (0.0322)  loss_vfl_aux_2: 0.5859 (0.6014)  loss_bbox_aux_2: 0.1214 (0.1295)  loss_giou_aux_2: 0.8520 (0.9505)  loss_fgl_aux_2: 0.9839 (0.9678)  loss_ddf_aux_2: 0.0023 (0.0037)  loss_vfl_aux_3: 0.5991 (0.5886)  loss_bbox_aux_3: 0.1216 (0.1293)  loss_giou_aux_3: 0.8549 (0.9499)  loss_fgl_aux_3: 0.9832 (0.9680)  loss_ddf_aux_3: 0.0003 (0.0005)  loss_vfl_aux_4: 0.6099 (0.5840)  loss_bbox_aux_4: 0.1217 (0.1293)  loss_giou_aux_4: 0.8551 (0.9497)  loss_fgl_aux_4: 0.9837 (0.9681)  loss_ddf_aux_4: 0.0001 (0.0001)  loss_vfl_pre: 0.5894 (0.5893)  loss_bbox_pre: 0.1265 (0.1458)  loss_giou_pre: 0.8971 (0.9895)  loss_vfl_enc_0: 0.5425 (0.5750)  loss_bbox_enc_0: 0.1590 (0.1925)  loss_giou_enc_0: 1.0717 (1.1049)  loss_vfl_dn_0: 0.4524 (0.4368)  loss_bbox_dn_0: 0.1055 (0.1426)  loss_giou_dn_0: 0.8845 (0.9714)  loss_fgl_dn_0: 1.0688 (1.0516)  loss_ddf_dn_0: 0.6948 (0.6135)  loss_vfl_dn_1: 0.4636 (0.4610)  loss_bbox_dn_1: 0.0894 (0.1136)  loss_giou_dn_1: 0.7623 (0.8621)  loss_fgl_dn_1: 1.0624 (1.0518)  loss_ddf_dn_1: 0.0701 (0.0898)  loss_vfl_dn_2: 0.4575 (0.4577)  loss_bbox_dn_2: 0.0865 (0.1084)  loss_giou_dn_2: 0.7530 (0.8452)  loss_fgl_dn_2: 1.0617 (1.0506)  loss_ddf_dn_2: 0.0068 (0.0125)  loss_vfl_dn_3: 0.4539 (0.4536)  loss_bbox_dn_3: 0.0869 (0.1072)  loss_giou_dn_3: 0.7527 (0.8420)  loss_fgl_dn_3: 1.0638 (1.0514)  loss_ddf_dn_3: 0.0009 (0.0016)  loss_vfl_dn_4: 0.4504 (0.4517)  loss_bbox_dn_4: 0.0867 (0.1068)  loss_giou_dn_4: 0.7510 (0.8414)  loss_fgl_dn_4: 1.0637 (1.0517)  loss_ddf_dn_4: 0.0001 (0.0002)  loss_vfl_dn_5: 0.4495 (0.4522)  loss_bbox_dn_5: 0.0866 (0.1067)  loss_giou_dn_5: 0.7507 (0.8414)  loss_fgl_dn_5: 1.0636 (1.0518)  loss_vfl_dn_pre: 0.4543 (0.4361)  loss_bbox_dn_pre: 0.1059 (0.1430)  loss_giou_dn_pre: 0.8756 (0.9733)  time: 0.6244  data: 0.0090  max mem: 3024\n",
            "Epoch: [9]  [400/500]  eta: 0:01:04  lr: 0.000013  loss: 37.0151 (36.8595)  loss_vfl: 0.5923 (0.5864)  loss_bbox: 0.1192 (0.1279)  loss_giou: 0.8704 (0.9467)  loss_fgl: 1.0498 (0.9702)  loss_vfl_aux_0: 0.6016 (0.5942)  loss_bbox_aux_0: 0.1315 (0.1433)  loss_giou_aux_0: 0.9120 (0.9864)  loss_fgl_aux_0: 1.0687 (0.9803)  loss_ddf_aux_0: 0.1373 (0.1724)  loss_vfl_aux_1: 0.5986 (0.6074)  loss_bbox_aux_1: 0.1221 (0.1306)  loss_giou_aux_1: 0.8770 (0.9526)  loss_fgl_aux_1: 1.0485 (0.9728)  loss_ddf_aux_1: 0.0198 (0.0306)  loss_vfl_aux_2: 0.6123 (0.6048)  loss_bbox_aux_2: 0.1232 (0.1280)  loss_giou_aux_2: 0.8678 (0.9473)  loss_fgl_aux_2: 1.0497 (0.9699)  loss_ddf_aux_2: 0.0020 (0.0034)  loss_vfl_aux_3: 0.5889 (0.5905)  loss_bbox_aux_3: 0.1207 (0.1279)  loss_giou_aux_3: 0.8701 (0.9468)  loss_fgl_aux_3: 1.0497 (0.9700)  loss_ddf_aux_3: 0.0002 (0.0005)  loss_vfl_aux_4: 0.5962 (0.5857)  loss_bbox_aux_4: 0.1192 (0.1279)  loss_giou_aux_4: 0.8703 (0.9467)  loss_fgl_aux_4: 1.0499 (0.9701)  loss_ddf_aux_4: 0.0001 (0.0001)  loss_vfl_pre: 0.6006 (0.5918)  loss_bbox_pre: 0.1302 (0.1436)  loss_giou_pre: 0.9151 (0.9860)  loss_vfl_enc_0: 0.5864 (0.5768)  loss_bbox_enc_0: 0.1599 (0.1892)  loss_giou_enc_0: 1.1200 (1.1024)  loss_vfl_dn_0: 0.4387 (0.4384)  loss_bbox_dn_0: 0.1265 (0.1403)  loss_giou_dn_0: 0.9444 (0.9658)  loss_fgl_dn_0: 1.0684 (1.0547)  loss_ddf_dn_0: 0.5788 (0.6101)  loss_vfl_dn_1: 0.4595 (0.4616)  loss_bbox_dn_1: 0.1075 (0.1118)  loss_giou_dn_1: 0.8552 (0.8574)  loss_fgl_dn_1: 1.0621 (1.0544)  loss_ddf_dn_1: 0.0716 (0.0870)  loss_vfl_dn_2: 0.4551 (0.4586)  loss_bbox_dn_2: 0.1044 (0.1066)  loss_giou_dn_2: 0.8439 (0.8404)  loss_fgl_dn_2: 1.0435 (1.0532)  loss_ddf_dn_2: 0.0072 (0.0117)  loss_vfl_dn_3: 0.4534 (0.4541)  loss_bbox_dn_3: 0.1042 (0.1055)  loss_giou_dn_3: 0.8413 (0.8374)  loss_fgl_dn_3: 1.0427 (1.0540)  loss_ddf_dn_3: 0.0008 (0.0015)  loss_vfl_dn_4: 0.4563 (0.4526)  loss_bbox_dn_4: 0.1043 (0.1051)  loss_giou_dn_4: 0.8407 (0.8368)  loss_fgl_dn_4: 1.0449 (1.0543)  loss_ddf_dn_4: 0.0001 (0.0002)  loss_vfl_dn_5: 0.4543 (0.4527)  loss_bbox_dn_5: 0.1042 (0.1051)  loss_giou_dn_5: 0.8403 (0.8368)  loss_fgl_dn_5: 1.0454 (1.0544)  loss_vfl_dn_pre: 0.4412 (0.4377)  loss_bbox_dn_pre: 0.1245 (0.1407)  loss_giou_dn_pre: 0.9420 (0.9678)  time: 0.6084  data: 0.0076  max mem: 3024\n",
            "Epoch: [9]  [499/500]  eta: 0:00:00  lr: 0.000013  loss: 36.5530 (36.8925)  loss_vfl: 0.5581 (0.5866)  loss_bbox: 0.1152 (0.1295)  loss_giou: 0.9511 (0.9478)  loss_fgl: 0.9751 (0.9689)  loss_vfl_aux_0: 0.5635 (0.5954)  loss_bbox_aux_0: 0.1350 (0.1446)  loss_giou_aux_0: 0.9704 (0.9885)  loss_fgl_aux_0: 0.9593 (0.9785)  loss_ddf_aux_0: 0.1761 (0.1759)  loss_vfl_aux_1: 0.6030 (0.6086)  loss_bbox_aux_1: 0.1212 (0.1320)  loss_giou_aux_1: 0.9584 (0.9538)  loss_fgl_aux_1: 0.9801 (0.9712)  loss_ddf_aux_1: 0.0201 (0.0300)  loss_vfl_aux_2: 0.6328 (0.6070)  loss_bbox_aux_2: 0.1152 (0.1297)  loss_giou_aux_2: 0.9467 (0.9485)  loss_fgl_aux_2: 0.9792 (0.9687)  loss_ddf_aux_2: 0.0020 (0.0033)  loss_vfl_aux_3: 0.5957 (0.5913)  loss_bbox_aux_3: 0.1153 (0.1295)  loss_giou_aux_3: 0.9482 (0.9479)  loss_fgl_aux_3: 0.9755 (0.9687)  loss_ddf_aux_3: 0.0003 (0.0005)  loss_vfl_aux_4: 0.5674 (0.5860)  loss_bbox_aux_4: 0.1153 (0.1295)  loss_giou_aux_4: 0.9501 (0.9478)  loss_fgl_aux_4: 0.9750 (0.9688)  loss_ddf_aux_4: 0.0001 (0.0001)  loss_vfl_pre: 0.5674 (0.5926)  loss_bbox_pre: 0.1381 (0.1450)  loss_giou_pre: 0.9756 (0.9883)  loss_vfl_enc_0: 0.5239 (0.5737)  loss_bbox_enc_0: 0.1570 (0.1914)  loss_giou_enc_0: 1.1458 (1.1075)  loss_vfl_dn_0: 0.4192 (0.4381)  loss_bbox_dn_0: 0.1049 (0.1412)  loss_giou_dn_0: 0.9926 (0.9663)  loss_fgl_dn_0: 1.0456 (1.0547)  loss_ddf_dn_0: 0.5384 (0.6161)  loss_vfl_dn_1: 0.4573 (0.4622)  loss_bbox_dn_1: 0.0869 (0.1122)  loss_giou_dn_1: 0.8821 (0.8562)  loss_fgl_dn_1: 1.0648 (1.0548)  loss_ddf_dn_1: 0.0640 (0.0867)  loss_vfl_dn_2: 0.4565 (0.4588)  loss_bbox_dn_2: 0.0836 (0.1070)  loss_giou_dn_2: 0.8627 (0.8393)  loss_fgl_dn_2: 1.0586 (1.0534)  loss_ddf_dn_2: 0.0069 (0.0115)  loss_vfl_dn_3: 0.4465 (0.4542)  loss_bbox_dn_3: 0.0822 (0.1060)  loss_giou_dn_3: 0.8611 (0.8363)  loss_fgl_dn_3: 1.0578 (1.0542)  loss_ddf_dn_3: 0.0008 (0.0015)  loss_vfl_dn_4: 0.4514 (0.4527)  loss_bbox_dn_4: 0.0819 (0.1057)  loss_giou_dn_4: 0.8611 (0.8357)  loss_fgl_dn_4: 1.0577 (1.0545)  loss_ddf_dn_4: 0.0001 (0.0002)  loss_vfl_dn_5: 0.4512 (0.4527)  loss_bbox_dn_5: 0.0818 (0.1056)  loss_giou_dn_5: 0.8606 (0.8357)  loss_fgl_dn_5: 1.0578 (1.0546)  loss_vfl_dn_pre: 0.4233 (0.4375)  loss_bbox_dn_pre: 0.1045 (0.1417)  loss_giou_dn_pre: 0.9723 (0.9680)  time: 0.6352  data: 0.0090  max mem: 3024\n",
            "Epoch: [9] Total time: 0:05:23 (0.6478 s / it)\n",
            "Averaged stats: lr: 0.000013  loss: 36.5530 (36.8925)  loss_vfl: 0.5581 (0.5866)  loss_bbox: 0.1152 (0.1295)  loss_giou: 0.9511 (0.9478)  loss_fgl: 0.9751 (0.9689)  loss_vfl_aux_0: 0.5635 (0.5954)  loss_bbox_aux_0: 0.1350 (0.1446)  loss_giou_aux_0: 0.9704 (0.9885)  loss_fgl_aux_0: 0.9593 (0.9785)  loss_ddf_aux_0: 0.1761 (0.1759)  loss_vfl_aux_1: 0.6030 (0.6086)  loss_bbox_aux_1: 0.1212 (0.1320)  loss_giou_aux_1: 0.9584 (0.9538)  loss_fgl_aux_1: 0.9801 (0.9712)  loss_ddf_aux_1: 0.0201 (0.0300)  loss_vfl_aux_2: 0.6328 (0.6070)  loss_bbox_aux_2: 0.1152 (0.1297)  loss_giou_aux_2: 0.9467 (0.9485)  loss_fgl_aux_2: 0.9792 (0.9687)  loss_ddf_aux_2: 0.0020 (0.0033)  loss_vfl_aux_3: 0.5957 (0.5913)  loss_bbox_aux_3: 0.1153 (0.1295)  loss_giou_aux_3: 0.9482 (0.9479)  loss_fgl_aux_3: 0.9755 (0.9687)  loss_ddf_aux_3: 0.0003 (0.0005)  loss_vfl_aux_4: 0.5674 (0.5860)  loss_bbox_aux_4: 0.1153 (0.1295)  loss_giou_aux_4: 0.9501 (0.9478)  loss_fgl_aux_4: 0.9750 (0.9688)  loss_ddf_aux_4: 0.0001 (0.0001)  loss_vfl_pre: 0.5674 (0.5926)  loss_bbox_pre: 0.1381 (0.1450)  loss_giou_pre: 0.9756 (0.9883)  loss_vfl_enc_0: 0.5239 (0.5737)  loss_bbox_enc_0: 0.1570 (0.1914)  loss_giou_enc_0: 1.1458 (1.1075)  loss_vfl_dn_0: 0.4192 (0.4381)  loss_bbox_dn_0: 0.1049 (0.1412)  loss_giou_dn_0: 0.9926 (0.9663)  loss_fgl_dn_0: 1.0456 (1.0547)  loss_ddf_dn_0: 0.5384 (0.6161)  loss_vfl_dn_1: 0.4573 (0.4622)  loss_bbox_dn_1: 0.0869 (0.1122)  loss_giou_dn_1: 0.8821 (0.8562)  loss_fgl_dn_1: 1.0648 (1.0548)  loss_ddf_dn_1: 0.0640 (0.0867)  loss_vfl_dn_2: 0.4565 (0.4588)  loss_bbox_dn_2: 0.0836 (0.1070)  loss_giou_dn_2: 0.8627 (0.8393)  loss_fgl_dn_2: 1.0586 (1.0534)  loss_ddf_dn_2: 0.0069 (0.0115)  loss_vfl_dn_3: 0.4465 (0.4542)  loss_bbox_dn_3: 0.0822 (0.1060)  loss_giou_dn_3: 0.8611 (0.8363)  loss_fgl_dn_3: 1.0578 (1.0542)  loss_ddf_dn_3: 0.0008 (0.0015)  loss_vfl_dn_4: 0.4514 (0.4527)  loss_bbox_dn_4: 0.0819 (0.1057)  loss_giou_dn_4: 0.8611 (0.8357)  loss_fgl_dn_4: 1.0577 (1.0545)  loss_ddf_dn_4: 0.0001 (0.0002)  loss_vfl_dn_5: 0.4512 (0.4527)  loss_bbox_dn_5: 0.0818 (0.1056)  loss_giou_dn_5: 0.8606 (0.8357)  loss_fgl_dn_5: 1.0578 (1.0546)  loss_vfl_dn_pre: 0.4233 (0.4375)  loss_bbox_dn_pre: 0.1045 (0.1417)  loss_giou_dn_pre: 0.9723 (0.9680)\n",
            "Test:  [  0/250]  eta: 0:06:46    time: 1.6276  data: 1.1892  max mem: 3024\n",
            "Test:  [ 10/250]  eta: 0:01:37    time: 0.4083  data: 0.1555  max mem: 3024\n",
            "Test:  [ 20/250]  eta: 0:01:18    time: 0.2787  data: 0.0480  max mem: 3024\n",
            "Test:  [ 30/250]  eta: 0:01:06    time: 0.2431  data: 0.0354  max mem: 3024\n",
            "Test:  [ 40/250]  eta: 0:00:58    time: 0.2064  data: 0.0237  max mem: 3024\n",
            "Test:  [ 50/250]  eta: 0:00:53    time: 0.2183  data: 0.0203  max mem: 3024\n",
            "Test:  [ 60/250]  eta: 0:00:48    time: 0.2189  data: 0.0203  max mem: 3024\n",
            "Test:  [ 70/250]  eta: 0:00:44    time: 0.2003  data: 0.0206  max mem: 3024\n",
            "Test:  [ 80/250]  eta: 0:00:42    time: 0.2372  data: 0.0337  max mem: 3024\n",
            "Test:  [ 90/250]  eta: 0:00:41    time: 0.2893  data: 0.0550  max mem: 3024\n",
            "Test:  [100/250]  eta: 0:00:39    time: 0.3189  data: 0.0595  max mem: 3024\n",
            "Test:  [110/250]  eta: 0:00:37    time: 0.3059  data: 0.0490  max mem: 3024\n",
            "Test:  [120/250]  eta: 0:00:34    time: 0.2470  data: 0.0344  max mem: 3024\n",
            "Test:  [130/250]  eta: 0:00:30    time: 0.2062  data: 0.0234  max mem: 3024\n",
            "Test:  [140/250]  eta: 0:00:27    time: 0.1987  data: 0.0206  max mem: 3024\n",
            "Test:  [150/250]  eta: 0:00:24    time: 0.1988  data: 0.0202  max mem: 3024\n",
            "Test:  [160/250]  eta: 0:00:22    time: 0.2156  data: 0.0208  max mem: 3024\n",
            "Test:  [170/250]  eta: 0:00:19    time: 0.2453  data: 0.0302  max mem: 3024\n",
            "Test:  [180/250]  eta: 0:00:17    time: 0.2620  data: 0.0392  max mem: 3024\n",
            "Test:  [190/250]  eta: 0:00:15    time: 0.2704  data: 0.0424  max mem: 3024\n",
            "Test:  [200/250]  eta: 0:00:12    time: 0.2398  data: 0.0341  max mem: 3024\n",
            "Test:  [210/250]  eta: 0:00:09    time: 0.2156  data: 0.0218  max mem: 3024\n",
            "Test:  [220/250]  eta: 0:00:07    time: 0.2119  data: 0.0208  max mem: 3024\n",
            "Test:  [230/250]  eta: 0:00:04    time: 0.1975  data: 0.0207  max mem: 3024\n",
            "Test:  [240/250]  eta: 0:00:02    time: 0.1965  data: 0.0201  max mem: 3024\n",
            "Test:  [249/250]  eta: 0:00:00    time: 0.2001  data: 0.0206  max mem: 3024\n",
            "Test: Total time: 0:01:00 (0.2416 s / it)\n",
            "Averaged stats: \n",
            "Accumulating evaluation results...\n",
            "COCOeval_opt.accumulate() finished...\n",
            "DONE (t=1.33s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.128\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.250\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.116\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.051\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.159\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.325\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.124\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.263\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.318\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.165\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.372\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.599\n",
            " Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.592\n",
            " Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.298\n",
            "best_stat: {'epoch': 9, 'coco_eval_bbox': 0.12751077112519302}\n",
            "Training time 1:06:00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.__version__)\n"
      ],
      "metadata": {
        "id": "mai1XP7sFNxP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25164d41-d767-45d5-97c6-857ebcafc4b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5.1+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKZqCDKQNOus",
        "outputId": "80573a2a-d401-4de3-e8ab-afb8d4f4d24d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"RANK\"] = \"0\"\n",
        "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
        "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
        "os.environ[\"MASTER_PORT\"] = \"12345\"\n",
        "'''!CUDA_VISIBLE_DEVICES=0 python train.py -c /content/D-FINE/configs/dfine/custom/dfine_hgnetv2_l_custom.yml --use-amp --seed=0'''"
      ],
      "metadata": {
        "id": "e3CRoqpBNQyV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python train.py -c /content/D-FINE/configs/dfine/custom/dfine_hgnetv2_l_custom.yml --test-only -r /content/D-FINE/output/dfine_hgnetv2_l_custom/best_stg1.pth"
      ],
      "metadata": {
        "id": "cXaAm8V8Oi36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "593cd4ad-13f0-4dec-ff15-a4fac3639d18"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-21 09:39:31.550905: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-21 09:39:31.584453: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-21 09:39:31.594270: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-21 09:39:31.617726: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-21 09:39:33.180193: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Initialized distributed mode...\n",
            "cfg:  {'task': 'detection', '_model': None, '_postprocessor': None, '_criterion': None, '_optimizer': None, '_lr_scheduler': None, '_lr_warmup_scheduler': None, '_train_dataloader': None, '_val_dataloader': None, '_ema': None, '_scaler': None, '_train_dataset': None, '_val_dataset': None, '_collate_fn': None, '_evaluator': None, '_writer': None, 'num_workers': 0, 'batch_size': None, '_train_batch_size': None, '_val_batch_size': None, '_train_shuffle': None, '_val_shuffle': None, 'resume': '/content/D-FINE/output/dfine_hgnetv2_l_custom/best_stg1.pth', 'tuning': None, 'epoches': 10, 'last_epoch': -1, 'use_amp': False, 'use_ema': True, 'ema_decay': 0.9999, 'ema_warmups': 2000, 'sync_bn': True, 'clip_max_norm': 0.1, 'find_unused_parameters': False, 'seed': None, 'print_freq': 100, 'checkpoint_freq': 12, 'output_dir': './output/dfine_hgnetv2_l_custom', 'summary_dir': None, 'device': '', 'yaml_cfg': {'task': 'detection', 'evaluator': {'type': 'CocoEvaluator', 'iou_types': ['bbox']}, 'num_classes': 777, 'remap_mscoco_category': False, 'train_dataloader': {'type': 'DataLoader', 'dataset': {'type': 'CocoDetection', 'img_folder': '/content/drive/MyDrive/dataset/dataset/images/train', 'ann_file': '/content/drive/MyDrive/dataset/dataset/annotations/instances_train.json', 'return_masks': False, 'transforms': {'type': 'Compose', 'ops': [{'type': 'RandomPhotometricDistort', 'p': 0.5}, {'type': 'RandomZoomOut', 'fill': 0}, {'type': 'RandomIoUCrop', 'p': 0.8}, {'type': 'SanitizeBoundingBoxes', 'min_size': 1}, {'type': 'RandomHorizontalFlip'}, {'type': 'Resize', 'size': [640, 640]}, {'type': 'SanitizeBoundingBoxes', 'min_size': 1}, {'type': 'ConvertPILImage', 'dtype': 'float32', 'scale': True}, {'type': 'ConvertBoxes', 'fmt': 'cxcywh', 'normalize': True}], 'policy': {'name': 'stop_epoch', 'epoch': 72, 'ops': ['RandomPhotometricDistort', 'RandomZoomOut', 'RandomIoUCrop']}}}, 'shuffle': True, 'num_workers': 4, 'drop_last': True, 'collate_fn': {'type': 'BatchImageCollateFunction', 'base_size': 640, 'base_size_repeat': 4, 'stop_epoch': 72, 'ema_restart_decay': 0.9999}, 'total_batch_size': 2}, 'val_dataloader': {'type': 'DataLoader', 'dataset': {'type': 'CocoDetection', 'img_folder': '/content/drive/MyDrive/dataset/dataset/images/val', 'ann_file': '/content/drive/MyDrive/dataset/dataset/annotations/instances_val.json', 'return_masks': False, 'transforms': {'type': 'Compose', 'ops': [{'type': 'Resize', 'size': [640, 640]}, {'type': 'ConvertPILImage', 'dtype': 'float32', 'scale': True}]}}, 'shuffle': False, 'num_workers': 4, 'drop_last': False, 'collate_fn': {'type': 'BatchImageCollateFunction'}, 'total_batch_size': 4}, 'print_freq': 100, 'output_dir': './output/dfine_hgnetv2_l_custom', 'checkpoint_freq': 12, 'sync_bn': True, 'find_unused_parameters': False, 'use_amp': False, 'scaler': {'type': 'GradScaler', 'enabled': True}, 'use_ema': True, 'ema': {'type': 'ModelEMA', 'decay': 0.9999, 'warmups': 1000, 'start': 0}, 'epoches': 10, 'clip_max_norm': 0.1, 'optimizer': {'type': 'AdamW', 'params': [{'params': '^(?=.*backbone)(?!.*norm|bn).*$', 'lr': 1.25e-05}, {'params': '^(?=.*(?:encoder|decoder))(?=.*(?:norm|bn)).*$', 'weight_decay': 0.0}], 'lr': 0.00025, 'betas': [0.9, 0.999], 'weight_decay': 0.000125}, 'lr_scheduler': {'type': 'MultiStepLR', 'milestones': [500], 'gamma': 0.1}, 'lr_warmup_scheduler': {'type': 'LinearWarmup', 'warmup_duration': 500}, 'model': 'DFINE', 'criterion': 'DFINECriterion', 'postprocessor': 'DFINEPostProcessor', 'use_focal_loss': True, 'eval_spatial_size': [640, 640], 'DFINE': {'backbone': 'HGNetv2', 'encoder': 'HybridEncoder', 'decoder': 'DFINETransformer'}, 'HGNetv2': {'pretrained': False, 'local_model_dir': 'weight/hgnetv2/', 'name': 'B4', 'return_idx': [1, 2, 3], 'freeze_stem_only': True, 'freeze_at': 0, 'freeze_norm': True}, 'HybridEncoder': {'in_channels': [512, 1024, 2048], 'feat_strides': [8, 16, 32], 'hidden_dim': 256, 'use_encoder_idx': [2], 'num_encoder_layers': 1, 'nhead': 8, 'dim_feedforward': 1024, 'dropout': 0.0, 'enc_act': 'gelu', 'expansion': 1.0, 'depth_mult': 1, 'act': 'silu'}, 'DFINETransformer': {'feat_channels': [256, 256, 256], 'feat_strides': [8, 16, 32], 'hidden_dim': 256, 'num_levels': 3, 'num_layers': 6, 'eval_idx': -1, 'num_queries': 300, 'num_denoising': 100, 'label_noise_ratio': 0.5, 'box_noise_scale': 1.0, 'reg_max': 32, 'reg_scale': 4, 'layer_scale': 1, 'num_points': [3, 6, 3], 'cross_attn_method': 'default', 'query_select_method': 'default'}, 'DFINEPostProcessor': {'num_top_queries': 300}, 'DFINECriterion': {'weight_dict': {'loss_vfl': 1, 'loss_bbox': 5, 'loss_giou': 2, 'loss_fgl': 0.15, 'loss_ddf': 1.5}, 'losses': ['vfl', 'boxes', 'local'], 'alpha': 0.75, 'gamma': 2.0, 'reg_max': 32, 'matcher': {'type': 'HungarianMatcher', 'weight_dict': {'cost_class': 2, 'cost_bbox': 5, 'cost_giou': 2}, 'alpha': 0.25, 'gamma': 2.0}}, '__include__': ['../../dataset/custom_detection.yml', '../../runtime.yml', '../include/dataloader.yml', '../include/optimizer.yml', '../include/dfine_hgnetv2.yml'], 'config': '/content/D-FINE/configs/dfine/custom/dfine_hgnetv2_l_custom.yml', 'resume': '/content/D-FINE/output/dfine_hgnetv2_l_custom/best_stg1.pth', 'test_only': True, 'print_method': 'builtin', 'print_rank': 0}}\n",
            "building val_dataloader with batch_size=4...\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Resume checkpoint from /content/D-FINE/output/dfine_hgnetv2_l_custom/best_stg1.pth\n",
            "/content/D-FINE/src/solver/_solver.py:159: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(path, map_location='cpu')\n",
            "Load last_epoch\n",
            "Load model.state_dict\n",
            "Load criterion.state_dict\n",
            "Load postprocessor.state_dict\n",
            "Load ema.state_dict\n",
            "Test:  [  0/250]  eta: 0:07:29    time: 1.7994  data: 0.6514  max mem: 836\n",
            "Test:  [ 10/250]  eta: 0:01:20    time: 0.3339  data: 0.0772  max mem: 848\n",
            "Test:  [ 20/250]  eta: 0:01:13    time: 0.2450  data: 0.0326  max mem: 848\n",
            "Test:  [ 30/250]  eta: 0:01:06    time: 0.2878  data: 0.0435  max mem: 848\n",
            "Test:  [ 40/250]  eta: 0:01:00    time: 0.2554  data: 0.0402  max mem: 848\n",
            "Test:  [ 50/250]  eta: 0:00:53    time: 0.2107  data: 0.0294  max mem: 848\n",
            "Test:  [ 60/250]  eta: 0:00:48    time: 0.1890  data: 0.0210  max mem: 848\n",
            "Test:  [ 70/250]  eta: 0:00:44    time: 0.1928  data: 0.0231  max mem: 848\n",
            "Test:  [ 80/250]  eta: 0:00:40    time: 0.1873  data: 0.0228  max mem: 848\n",
            "Test:  [ 90/250]  eta: 0:00:37    time: 0.2026  data: 0.0226  max mem: 848\n",
            "Test:  [100/250]  eta: 0:00:35    time: 0.2320  data: 0.0354  max mem: 848\n",
            "Test:  [110/250]  eta: 0:00:33    time: 0.2552  data: 0.0457  max mem: 848\n",
            "Test:  [120/250]  eta: 0:00:31    time: 0.2668  data: 0.0424  max mem: 848\n",
            "Test:  [130/250]  eta: 0:00:28    time: 0.2249  data: 0.0301  max mem: 848\n",
            "Test:  [140/250]  eta: 0:00:25    time: 0.1840  data: 0.0196  max mem: 848\n",
            "Test:  [150/250]  eta: 0:00:23    time: 0.1851  data: 0.0203  max mem: 848\n",
            "Test:  [160/250]  eta: 0:00:20    time: 0.2015  data: 0.0207  max mem: 848\n",
            "Test:  [170/250]  eta: 0:00:18    time: 0.2020  data: 0.0204  max mem: 848\n",
            "Test:  [180/250]  eta: 0:00:15    time: 0.2110  data: 0.0301  max mem: 848\n",
            "Test:  [190/250]  eta: 0:00:13    time: 0.2519  data: 0.0407  max mem: 848\n",
            "Test:  [200/250]  eta: 0:00:11    time: 0.2725  data: 0.0440  max mem: 848\n",
            "Test:  [210/250]  eta: 0:00:09    time: 0.2321  data: 0.0341  max mem: 848\n",
            "Test:  [220/250]  eta: 0:00:06    time: 0.2026  data: 0.0207  max mem: 848\n",
            "Test:  [230/250]  eta: 0:00:04    time: 0.2028  data: 0.0202  max mem: 848\n",
            "Test:  [240/250]  eta: 0:00:02    time: 0.1890  data: 0.0206  max mem: 848\n",
            "Test:  [249/250]  eta: 0:00:00    time: 0.1867  data: 0.0191  max mem: 848\n",
            "Test: Total time: 0:00:56 (0.2253 s / it)\n",
            "Averaged stats: \n",
            "Accumulating evaluation results...\n",
            "COCOeval_opt.accumulate() finished...\n",
            "DONE (t=1.11s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.128\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.250\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.116\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.051\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.159\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.325\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.124\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.263\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.318\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.165\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.372\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.599\n",
            " Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.592\n",
            " Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.298\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}